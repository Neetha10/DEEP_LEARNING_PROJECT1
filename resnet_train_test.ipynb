{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0Nv9m0Kc7qLK",
        "outputId": "1b12999e-08ad-4578-c244-a7daa02e595c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Model: Hybrid-ResNet - Type: B\n",
            "Total parameters: 2,608,582\n",
            "\n",
            "Epoch: 1/100\n",
            "Learning rate: 0.100000\n",
            "Epoch: [0][0/391]\tTime 0.365 (0.365)\tData 0.221 (0.221)\tLoss 2.3004 (2.3004)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [0][50/391]\tTime 0.128 (0.102)\tData 0.060 (0.010)\tLoss 2.1123 (2.2970)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [0][100/391]\tTime 0.143 (0.108)\tData 0.058 (0.017)\tLoss 2.0988 (2.2531)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [0][150/391]\tTime 0.077 (0.107)\tData 0.000 (0.017)\tLoss 2.1942 (2.2147)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [0][200/391]\tTime 0.072 (0.105)\tData 0.000 (0.017)\tLoss 2.1662 (2.1872)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [0][250/391]\tTime 0.098 (0.107)\tData 0.000 (0.018)\tLoss 1.9738 (2.1693)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [0][300/391]\tTime 0.094 (0.106)\tData 0.000 (0.018)\tLoss 2.0783 (2.1490)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [0][350/391]\tTime 0.103 (0.107)\tData 0.000 (0.018)\tLoss 2.0934 (2.1376)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.158 (0.158)\tLoss 1.7692 (1.7692)\tPrec@1 41.406 (41.406)\n",
            "Test: [50/79]\tTime 0.020 (0.033)\tLoss 1.8485 (1.8074)\tPrec@1 34.375 (38.603)\n",
            " * Prec@1 38.660\n",
            "\n",
            "Epoch: 2/100\n",
            "Learning rate: 0.099975\n",
            "Epoch: [1][0/391]\tTime 0.326 (0.326)\tData 0.214 (0.214)\tLoss 2.1501 (2.1501)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [1][50/391]\tTime 0.131 (0.122)\tData 0.033 (0.028)\tLoss 2.0036 (2.0366)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [1][100/391]\tTime 0.071 (0.111)\tData 0.002 (0.022)\tLoss 2.0188 (2.0216)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [1][150/391]\tTime 0.181 (0.110)\tData 0.092 (0.019)\tLoss 2.0769 (2.0139)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [1][200/391]\tTime 0.072 (0.109)\tData 0.000 (0.020)\tLoss 1.9154 (2.0081)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [1][250/391]\tTime 0.073 (0.107)\tData 0.000 (0.019)\tLoss 1.8098 (2.0003)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [1][300/391]\tTime 0.089 (0.109)\tData 0.000 (0.020)\tLoss 1.8924 (1.9939)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [1][350/391]\tTime 0.074 (0.108)\tData 0.000 (0.019)\tLoss 2.0459 (1.9891)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.159 (0.159)\tLoss 1.7479 (1.7479)\tPrec@1 44.531 (44.531)\n",
            "Test: [50/79]\tTime 0.038 (0.034)\tLoss 1.8631 (1.7216)\tPrec@1 33.594 (44.455)\n",
            " * Prec@1 44.500\n",
            "\n",
            "Epoch: 3/100\n",
            "Learning rate: 0.099901\n",
            "Epoch: [2][0/391]\tTime 0.326 (0.326)\tData 0.210 (0.210)\tLoss 1.8822 (1.8822)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [2][50/391]\tTime 0.148 (0.103)\tData 0.060 (0.012)\tLoss 1.8523 (1.9208)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [2][100/391]\tTime 0.103 (0.110)\tData 0.004 (0.016)\tLoss 1.6548 (1.9039)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [2][150/391]\tTime 0.098 (0.105)\tData 0.000 (0.012)\tLoss 1.8736 (1.9089)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [2][200/391]\tTime 0.095 (0.104)\tData 0.004 (0.012)\tLoss 1.9541 (1.9051)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [2][250/391]\tTime 0.075 (0.105)\tData 0.000 (0.013)\tLoss 1.6388 (1.8895)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [2][300/391]\tTime 0.088 (0.105)\tData 0.000 (0.013)\tLoss 1.8973 (1.8908)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [2][350/391]\tTime 0.094 (0.106)\tData 0.000 (0.014)\tLoss 1.8849 (1.8835)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.160 (0.160)\tLoss 1.4197 (1.4197)\tPrec@1 57.031 (57.031)\n",
            "Test: [50/79]\tTime 0.016 (0.034)\tLoss 1.5404 (1.4645)\tPrec@1 53.125 (57.077)\n",
            " * Prec@1 57.150\n",
            "\n",
            "Epoch: 4/100\n",
            "Learning rate: 0.099778\n",
            "Epoch: [3][0/391]\tTime 0.329 (0.329)\tData 0.217 (0.217)\tLoss 1.9309 (1.9309)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [3][50/391]\tTime 0.079 (0.118)\tData 0.000 (0.022)\tLoss 2.0959 (1.8277)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [3][100/391]\tTime 0.093 (0.109)\tData 0.000 (0.019)\tLoss 1.7593 (1.8404)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [3][150/391]\tTime 0.153 (0.112)\tData 0.057 (0.020)\tLoss 1.8800 (1.8476)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [3][200/391]\tTime 0.090 (0.108)\tData 0.000 (0.016)\tLoss 1.7548 (1.8487)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [3][250/391]\tTime 0.115 (0.108)\tData 0.000 (0.016)\tLoss 1.9758 (1.8434)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [3][300/391]\tTime 0.097 (0.108)\tData 0.006 (0.017)\tLoss 2.0466 (1.8388)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [3][350/391]\tTime 0.073 (0.107)\tData 0.000 (0.015)\tLoss 1.5326 (1.8375)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.150 (0.150)\tLoss 1.5678 (1.5678)\tPrec@1 54.688 (54.688)\n",
            "Test: [50/79]\tTime 0.028 (0.033)\tLoss 1.7953 (1.6822)\tPrec@1 48.438 (50.873)\n",
            " * Prec@1 50.580\n",
            "\n",
            "Epoch: 5/100\n",
            "Learning rate: 0.099606\n",
            "Epoch: [4][0/391]\tTime 0.340 (0.340)\tData 0.226 (0.226)\tLoss 1.6065 (1.6065)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [4][50/391]\tTime 0.092 (0.102)\tData 0.000 (0.007)\tLoss 1.9824 (1.8287)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [4][100/391]\tTime 0.094 (0.109)\tData 0.000 (0.016)\tLoss 1.9611 (1.8225)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [4][150/391]\tTime 0.096 (0.106)\tData 0.000 (0.016)\tLoss 1.8444 (1.8138)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [4][200/391]\tTime 0.099 (0.108)\tData 0.000 (0.015)\tLoss 1.7208 (1.8182)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [4][250/391]\tTime 0.085 (0.107)\tData 0.000 (0.015)\tLoss 1.8097 (1.8221)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [4][300/391]\tTime 0.074 (0.106)\tData 0.000 (0.015)\tLoss 1.8992 (1.8209)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [4][350/391]\tTime 0.098 (0.107)\tData 0.005 (0.016)\tLoss 1.9035 (1.8184)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.148 (0.148)\tLoss 1.3613 (1.3613)\tPrec@1 63.281 (63.281)\n",
            "Test: [50/79]\tTime 0.026 (0.032)\tLoss 1.4575 (1.4329)\tPrec@1 60.156 (59.911)\n",
            " * Prec@1 59.870\n",
            "\n",
            "Epoch: 6/100\n",
            "Learning rate: 0.099385\n",
            "Epoch: [5][0/391]\tTime 0.326 (0.326)\tData 0.214 (0.214)\tLoss 1.6269 (1.6269)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [5][50/391]\tTime 0.095 (0.120)\tData 0.010 (0.024)\tLoss 1.7996 (1.7813)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [5][100/391]\tTime 0.078 (0.108)\tData 0.000 (0.014)\tLoss 2.0081 (1.7778)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [5][150/391]\tTime 0.098 (0.111)\tData 0.006 (0.015)\tLoss 1.9697 (1.7722)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [5][200/391]\tTime 0.074 (0.108)\tData 0.000 (0.016)\tLoss 1.8713 (1.7620)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [5][250/391]\tTime 0.105 (0.108)\tData 0.000 (0.015)\tLoss 1.8697 (1.7589)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [5][300/391]\tTime 0.072 (0.108)\tData 0.000 (0.016)\tLoss 1.7088 (1.7600)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [5][350/391]\tTime 0.122 (0.107)\tData 0.050 (0.017)\tLoss 1.8107 (1.7574)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.141 (0.141)\tLoss 1.3632 (1.3632)\tPrec@1 57.812 (57.812)\n",
            "Test: [50/79]\tTime 0.035 (0.034)\tLoss 1.4107 (1.3471)\tPrec@1 58.594 (62.377)\n",
            " * Prec@1 62.490\n",
            "\n",
            "Epoch: 7/100\n",
            "Learning rate: 0.099115\n",
            "Epoch: [6][0/391]\tTime 0.316 (0.316)\tData 0.210 (0.210)\tLoss 1.6497 (1.6497)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [6][50/391]\tTime 0.071 (0.103)\tData 0.000 (0.014)\tLoss 1.9147 (1.7694)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [6][100/391]\tTime 0.099 (0.111)\tData 0.005 (0.022)\tLoss 1.7297 (1.7861)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [6][150/391]\tTime 0.075 (0.107)\tData 0.000 (0.019)\tLoss 1.4271 (1.7801)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [6][200/391]\tTime 0.096 (0.109)\tData 0.005 (0.020)\tLoss 1.8772 (1.7785)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [6][250/391]\tTime 0.091 (0.107)\tData 0.005 (0.019)\tLoss 1.4613 (1.7670)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [6][300/391]\tTime 0.212 (0.109)\tData 0.108 (0.020)\tLoss 1.8645 (1.7644)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [6][350/391]\tTime 0.075 (0.108)\tData 0.000 (0.020)\tLoss 1.9310 (1.7550)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.144 (0.144)\tLoss 1.2789 (1.2789)\tPrec@1 69.531 (69.531)\n",
            "Test: [50/79]\tTime 0.040 (0.034)\tLoss 1.3338 (1.2844)\tPrec@1 65.625 (67.923)\n",
            " * Prec@1 67.790\n",
            "\n",
            "Epoch: 8/100\n",
            "Learning rate: 0.098797\n",
            "Epoch: [7][0/391]\tTime 0.492 (0.492)\tData 0.367 (0.367)\tLoss 1.8257 (1.8257)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [7][50/391]\tTime 0.117 (0.109)\tData 0.021 (0.017)\tLoss 1.8503 (1.6613)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [7][100/391]\tTime 0.096 (0.104)\tData 0.000 (0.012)\tLoss 1.8097 (1.7174)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [7][150/391]\tTime 0.076 (0.108)\tData 0.000 (0.015)\tLoss 1.8630 (1.7015)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [7][200/391]\tTime 0.090 (0.106)\tData 0.000 (0.015)\tLoss 1.8969 (1.7075)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [7][250/391]\tTime 0.099 (0.108)\tData 0.004 (0.016)\tLoss 1.9476 (1.7186)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [7][300/391]\tTime 0.092 (0.106)\tData 0.000 (0.016)\tLoss 1.4086 (1.7153)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [7][350/391]\tTime 0.140 (0.106)\tData 0.013 (0.016)\tLoss 1.6246 (1.7134)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.156 (0.156)\tLoss 1.2433 (1.2433)\tPrec@1 72.656 (72.656)\n",
            "Test: [50/79]\tTime 0.042 (0.033)\tLoss 1.2669 (1.2618)\tPrec@1 71.094 (68.551)\n",
            " * Prec@1 68.830\n",
            "\n",
            "Epoch: 9/100\n",
            "Learning rate: 0.098431\n",
            "Epoch: [8][0/391]\tTime 0.314 (0.314)\tData 0.205 (0.205)\tLoss 1.6464 (1.6464)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [8][50/391]\tTime 0.244 (0.111)\tData 0.072 (0.020)\tLoss 1.9345 (1.7007)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [8][100/391]\tTime 0.113 (0.110)\tData 0.040 (0.020)\tLoss 1.3456 (1.7121)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [8][150/391]\tTime 0.123 (0.107)\tData 0.031 (0.018)\tLoss 1.6158 (1.7070)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [8][200/391]\tTime 0.156 (0.109)\tData 0.067 (0.021)\tLoss 1.5769 (1.7100)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [8][250/391]\tTime 0.108 (0.107)\tData 0.005 (0.018)\tLoss 1.9464 (1.7040)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [8][300/391]\tTime 0.093 (0.109)\tData 0.002 (0.019)\tLoss 1.5699 (1.6982)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [8][350/391]\tTime 0.095 (0.107)\tData 0.005 (0.017)\tLoss 1.4285 (1.6969)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.161 (0.161)\tLoss 1.2916 (1.2916)\tPrec@1 65.625 (65.625)\n",
            "Test: [50/79]\tTime 0.051 (0.040)\tLoss 1.3543 (1.2790)\tPrec@1 65.625 (66.774)\n",
            " * Prec@1 67.310\n",
            "\n",
            "Epoch: 10/100\n",
            "Learning rate: 0.098017\n",
            "Epoch: [9][0/391]\tTime 0.440 (0.440)\tData 0.321 (0.321)\tLoss 1.7312 (1.7312)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [9][50/391]\tTime 0.129 (0.105)\tData 0.058 (0.017)\tLoss 1.5293 (1.6867)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [9][100/391]\tTime 0.199 (0.103)\tData 0.090 (0.018)\tLoss 1.6613 (1.6732)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [9][150/391]\tTime 0.099 (0.108)\tData 0.011 (0.020)\tLoss 1.3138 (1.6840)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [9][200/391]\tTime 0.096 (0.106)\tData 0.009 (0.016)\tLoss 1.2937 (1.6964)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [9][250/391]\tTime 0.096 (0.107)\tData 0.000 (0.017)\tLoss 1.4645 (1.6921)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [9][300/391]\tTime 0.116 (0.106)\tData 0.024 (0.015)\tLoss 1.3383 (1.7001)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [9][350/391]\tTime 0.237 (0.106)\tData 0.138 (0.015)\tLoss 1.8913 (1.7024)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.145 (0.145)\tLoss 1.3878 (1.3878)\tPrec@1 63.281 (63.281)\n",
            "Test: [50/79]\tTime 0.015 (0.033)\tLoss 1.3484 (1.3507)\tPrec@1 68.750 (64.752)\n",
            " * Prec@1 65.130\n",
            "\n",
            "Epoch: 11/100\n",
            "Learning rate: 0.097555\n",
            "Epoch: [10][0/391]\tTime 0.325 (0.325)\tData 0.211 (0.211)\tLoss 1.3579 (1.3579)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [10][50/391]\tTime 0.197 (0.114)\tData 0.096 (0.016)\tLoss 1.3580 (1.7066)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [10][100/391]\tTime 0.109 (0.110)\tData 0.004 (0.015)\tLoss 1.8747 (1.6570)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [10][150/391]\tTime 0.135 (0.107)\tData 0.064 (0.015)\tLoss 1.3229 (1.6754)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [10][200/391]\tTime 0.091 (0.109)\tData 0.000 (0.017)\tLoss 1.8386 (1.6842)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [10][250/391]\tTime 0.099 (0.107)\tData 0.007 (0.016)\tLoss 1.5095 (1.6761)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [10][300/391]\tTime 0.073 (0.109)\tData 0.000 (0.018)\tLoss 1.9366 (1.6676)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [10][350/391]\tTime 0.082 (0.107)\tData 0.000 (0.018)\tLoss 1.4096 (1.6713)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.154 (0.154)\tLoss 1.2137 (1.2137)\tPrec@1 72.656 (72.656)\n",
            "Test: [50/79]\tTime 0.034 (0.047)\tLoss 1.2405 (1.2279)\tPrec@1 72.656 (70.144)\n",
            " * Prec@1 70.580\n",
            "\n",
            "Epoch: 12/100\n",
            "Learning rate: 0.097047\n",
            "Epoch: [11][0/391]\tTime 0.323 (0.323)\tData 0.206 (0.206)\tLoss 1.8595 (1.8595)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [11][50/391]\tTime 0.094 (0.101)\tData 0.000 (0.010)\tLoss 1.9114 (1.6521)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [11][100/391]\tTime 0.172 (0.100)\tData 0.086 (0.008)\tLoss 1.6746 (1.6580)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [11][150/391]\tTime 0.108 (0.105)\tData 0.038 (0.012)\tLoss 1.6629 (1.6589)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [11][200/391]\tTime 0.073 (0.104)\tData 0.000 (0.012)\tLoss 2.0349 (1.6583)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [11][250/391]\tTime 0.095 (0.106)\tData 0.001 (0.014)\tLoss 1.5977 (1.6599)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [11][300/391]\tTime 0.123 (0.106)\tData 0.054 (0.015)\tLoss 1.4519 (1.6547)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [11][350/391]\tTime 0.072 (0.108)\tData 0.000 (0.016)\tLoss 1.6912 (1.6549)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.151 (0.151)\tLoss 1.1894 (1.1894)\tPrec@1 75.000 (75.000)\n",
            "Test: [50/79]\tTime 0.033 (0.034)\tLoss 1.1944 (1.1955)\tPrec@1 75.000 (72.442)\n",
            " * Prec@1 72.920\n",
            "\n",
            "Epoch: 13/100\n",
            "Learning rate: 0.096492\n",
            "Epoch: [12][0/391]\tTime 0.328 (0.328)\tData 0.210 (0.210)\tLoss 1.8794 (1.8794)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [12][50/391]\tTime 0.102 (0.120)\tData 0.012 (0.019)\tLoss 1.8526 (1.6497)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [12][100/391]\tTime 0.078 (0.109)\tData 0.000 (0.016)\tLoss 1.6080 (1.6549)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [12][150/391]\tTime 0.127 (0.107)\tData 0.032 (0.015)\tLoss 1.8935 (1.6632)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [12][200/391]\tTime 0.171 (0.109)\tData 0.080 (0.018)\tLoss 1.3876 (1.6656)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [12][250/391]\tTime 0.120 (0.108)\tData 0.050 (0.019)\tLoss 1.8907 (1.6711)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [12][300/391]\tTime 0.125 (0.109)\tData 0.057 (0.020)\tLoss 1.2325 (1.6674)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [12][350/391]\tTime 0.114 (0.108)\tData 0.021 (0.019)\tLoss 1.8846 (1.6705)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.227 (0.227)\tLoss 1.1755 (1.1755)\tPrec@1 71.875 (71.875)\n",
            "Test: [50/79]\tTime 0.028 (0.042)\tLoss 1.2316 (1.2243)\tPrec@1 67.188 (69.240)\n",
            " * Prec@1 69.720\n",
            "\n",
            "Epoch: 14/100\n",
            "Learning rate: 0.095892\n",
            "Epoch: [13][0/391]\tTime 0.329 (0.329)\tData 0.210 (0.210)\tLoss 1.9819 (1.9819)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [13][50/391]\tTime 0.093 (0.103)\tData 0.000 (0.012)\tLoss 1.8417 (1.6390)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [13][100/391]\tTime 0.086 (0.108)\tData 0.000 (0.017)\tLoss 1.3355 (1.6453)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [13][150/391]\tTime 0.134 (0.106)\tData 0.065 (0.018)\tLoss 1.6574 (1.6487)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [13][200/391]\tTime 0.095 (0.105)\tData 0.000 (0.017)\tLoss 1.4190 (1.6553)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [13][250/391]\tTime 0.095 (0.107)\tData 0.005 (0.021)\tLoss 1.5826 (1.6524)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [13][300/391]\tTime 0.139 (0.106)\tData 0.043 (0.018)\tLoss 1.8277 (1.6512)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [13][350/391]\tTime 0.114 (0.107)\tData 0.015 (0.019)\tLoss 1.3986 (1.6560)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.149 (0.149)\tLoss 1.2260 (1.2260)\tPrec@1 69.531 (69.531)\n",
            "Test: [50/79]\tTime 0.033 (0.033)\tLoss 1.2175 (1.2054)\tPrec@1 73.438 (72.105)\n",
            " * Prec@1 72.190\n",
            "\n",
            "Epoch: 15/100\n",
            "Learning rate: 0.095246\n",
            "Epoch: [14][0/391]\tTime 0.319 (0.319)\tData 0.201 (0.201)\tLoss 1.8106 (1.8106)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [14][50/391]\tTime 0.098 (0.121)\tData 0.004 (0.024)\tLoss 1.3033 (1.6950)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [14][100/391]\tTime 0.103 (0.109)\tData 0.000 (0.015)\tLoss 1.3385 (1.6925)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [14][150/391]\tTime 0.152 (0.109)\tData 0.035 (0.016)\tLoss 1.7591 (1.6753)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [14][200/391]\tTime 0.088 (0.108)\tData 0.000 (0.016)\tLoss 1.4880 (1.6705)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [14][250/391]\tTime 0.076 (0.105)\tData 0.000 (0.014)\tLoss 1.4601 (1.6593)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [14][300/391]\tTime 0.100 (0.107)\tData 0.004 (0.015)\tLoss 1.6074 (1.6625)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [14][350/391]\tTime 0.095 (0.106)\tData 0.000 (0.014)\tLoss 1.7754 (1.6627)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.226 (0.226)\tLoss 1.3425 (1.3425)\tPrec@1 62.500 (62.500)\n",
            "Test: [50/79]\tTime 0.017 (0.036)\tLoss 1.4102 (1.3413)\tPrec@1 64.844 (64.782)\n",
            " * Prec@1 65.080\n",
            "\n",
            "Epoch: 16/100\n",
            "Learning rate: 0.094556\n",
            "Epoch: [15][0/391]\tTime 0.326 (0.326)\tData 0.206 (0.206)\tLoss 1.6779 (1.6779)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [15][50/391]\tTime 0.073 (0.100)\tData 0.000 (0.008)\tLoss 1.6073 (1.6308)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [15][100/391]\tTime 0.079 (0.109)\tData 0.000 (0.015)\tLoss 1.7309 (1.6421)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [15][150/391]\tTime 0.099 (0.107)\tData 0.004 (0.020)\tLoss 1.4040 (1.6449)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [15][200/391]\tTime 0.097 (0.108)\tData 0.000 (0.023)\tLoss 1.4155 (1.6515)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [15][250/391]\tTime 0.074 (0.108)\tData 0.000 (0.022)\tLoss 1.5226 (1.6556)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [15][300/391]\tTime 0.089 (0.107)\tData 0.000 (0.023)\tLoss 1.6252 (1.6631)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [15][350/391]\tTime 0.100 (0.109)\tData 0.003 (0.023)\tLoss 1.8364 (1.6635)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.152 (0.152)\tLoss 1.2519 (1.2519)\tPrec@1 70.312 (70.312)\n",
            "Test: [50/79]\tTime 0.043 (0.034)\tLoss 1.2383 (1.2412)\tPrec@1 71.094 (71.048)\n",
            " * Prec@1 71.120\n",
            "\n",
            "Epoch: 17/100\n",
            "Learning rate: 0.093822\n",
            "Epoch: [16][0/391]\tTime 0.320 (0.320)\tData 0.209 (0.209)\tLoss 1.4058 (1.4058)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [16][50/391]\tTime 0.200 (0.123)\tData 0.060 (0.030)\tLoss 1.3393 (1.6412)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [16][100/391]\tTime 0.097 (0.111)\tData 0.027 (0.025)\tLoss 1.3497 (1.6102)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [16][150/391]\tTime 0.106 (0.114)\tData 0.011 (0.025)\tLoss 1.3237 (1.6189)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [16][200/391]\tTime 0.103 (0.110)\tData 0.026 (0.021)\tLoss 1.7402 (1.6286)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [16][250/391]\tTime 0.243 (0.109)\tData 0.151 (0.020)\tLoss 1.8292 (1.6310)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [16][300/391]\tTime 0.108 (0.110)\tData 0.036 (0.021)\tLoss 1.4276 (1.6370)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [16][350/391]\tTime 0.098 (0.108)\tData 0.005 (0.021)\tLoss 1.7888 (1.6352)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.150 (0.150)\tLoss 1.2993 (1.2993)\tPrec@1 64.844 (64.844)\n",
            "Test: [50/79]\tTime 0.044 (0.034)\tLoss 1.3305 (1.2889)\tPrec@1 69.531 (66.498)\n",
            " * Prec@1 67.080\n",
            "\n",
            "Epoch: 18/100\n",
            "Learning rate: 0.093044\n",
            "Epoch: [17][0/391]\tTime 0.323 (0.323)\tData 0.212 (0.212)\tLoss 1.6768 (1.6768)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [17][50/391]\tTime 0.113 (0.103)\tData 0.018 (0.011)\tLoss 1.9210 (1.6608)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [17][100/391]\tTime 0.095 (0.110)\tData 0.000 (0.017)\tLoss 1.3478 (1.6497)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [17][150/391]\tTime 0.142 (0.106)\tData 0.057 (0.016)\tLoss 1.7290 (1.6490)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [17][200/391]\tTime 0.073 (0.109)\tData 0.000 (0.018)\tLoss 1.6206 (1.6342)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [17][250/391]\tTime 0.133 (0.108)\tData 0.043 (0.017)\tLoss 1.8945 (1.6348)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [17][300/391]\tTime 0.114 (0.106)\tData 0.000 (0.015)\tLoss 1.5749 (1.6424)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [17][350/391]\tTime 0.102 (0.108)\tData 0.032 (0.017)\tLoss 1.9000 (1.6452)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.151 (0.151)\tLoss 1.0387 (1.0387)\tPrec@1 80.469 (80.469)\n",
            "Test: [50/79]\tTime 0.036 (0.033)\tLoss 1.1478 (1.0860)\tPrec@1 70.312 (77.711)\n",
            " * Prec@1 77.670\n",
            "\n",
            "Epoch: 19/100\n",
            "Learning rate: 0.092224\n",
            "Epoch: [18][0/391]\tTime 0.478 (0.478)\tData 0.364 (0.364)\tLoss 1.4844 (1.4844)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [18][50/391]\tTime 0.094 (0.121)\tData 0.006 (0.031)\tLoss 1.6016 (1.6581)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [18][100/391]\tTime 0.103 (0.112)\tData 0.000 (0.020)\tLoss 1.6191 (1.6572)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [18][150/391]\tTime 0.077 (0.113)\tData 0.000 (0.021)\tLoss 1.6814 (1.6442)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [18][200/391]\tTime 0.094 (0.110)\tData 0.000 (0.018)\tLoss 1.8049 (1.6362)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [18][250/391]\tTime 0.112 (0.111)\tData 0.041 (0.019)\tLoss 1.5261 (1.6345)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [18][300/391]\tTime 0.076 (0.109)\tData 0.000 (0.018)\tLoss 1.8630 (1.6298)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [18][350/391]\tTime 0.113 (0.108)\tData 0.000 (0.017)\tLoss 1.3097 (1.6244)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.144 (0.144)\tLoss 1.3001 (1.3001)\tPrec@1 69.531 (69.531)\n",
            "Test: [50/79]\tTime 0.030 (0.032)\tLoss 1.3710 (1.3052)\tPrec@1 61.719 (66.513)\n",
            " * Prec@1 67.180\n",
            "\n",
            "Epoch: 20/100\n",
            "Learning rate: 0.091363\n",
            "Epoch: [19][0/391]\tTime 0.334 (0.334)\tData 0.218 (0.218)\tLoss 1.6518 (1.6518)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [19][50/391]\tTime 0.135 (0.106)\tData 0.000 (0.013)\tLoss 1.8972 (1.6759)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [19][100/391]\tTime 0.097 (0.110)\tData 0.006 (0.018)\tLoss 1.7155 (1.6555)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [19][150/391]\tTime 0.137 (0.107)\tData 0.064 (0.016)\tLoss 1.8987 (1.6488)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [19][200/391]\tTime 0.167 (0.111)\tData 0.082 (0.021)\tLoss 1.6066 (1.6449)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [19][250/391]\tTime 0.095 (0.109)\tData 0.016 (0.020)\tLoss 1.3673 (1.6427)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [19][300/391]\tTime 0.165 (0.110)\tData 0.073 (0.020)\tLoss 1.3295 (1.6334)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [19][350/391]\tTime 0.078 (0.109)\tData 0.000 (0.020)\tLoss 1.7946 (1.6327)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.152 (0.152)\tLoss 1.2750 (1.2750)\tPrec@1 64.844 (64.844)\n",
            "Test: [50/79]\tTime 0.026 (0.042)\tLoss 1.3768 (1.3454)\tPrec@1 63.281 (64.660)\n",
            " * Prec@1 64.470\n",
            "\n",
            "Epoch: 21/100\n",
            "Learning rate: 0.090460\n",
            "Epoch: [20][0/391]\tTime 0.346 (0.346)\tData 0.223 (0.223)\tLoss 1.6791 (1.6791)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [20][50/391]\tTime 0.074 (0.104)\tData 0.000 (0.014)\tLoss 1.6871 (1.6667)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [20][100/391]\tTime 0.151 (0.105)\tData 0.058 (0.012)\tLoss 1.4503 (1.6268)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [20][150/391]\tTime 0.095 (0.109)\tData 0.000 (0.016)\tLoss 1.2033 (1.6133)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [20][200/391]\tTime 0.103 (0.106)\tData 0.002 (0.014)\tLoss 1.9044 (1.6175)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [20][250/391]\tTime 0.094 (0.109)\tData 0.006 (0.016)\tLoss 1.6547 (1.6192)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [20][300/391]\tTime 0.112 (0.107)\tData 0.021 (0.016)\tLoss 1.8159 (1.6251)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [20][350/391]\tTime 0.094 (0.109)\tData 0.000 (0.018)\tLoss 1.7359 (1.6138)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.155 (0.155)\tLoss 1.1134 (1.1134)\tPrec@1 76.562 (76.562)\n",
            "Test: [50/79]\tTime 0.016 (0.034)\tLoss 1.1152 (1.1343)\tPrec@1 75.781 (74.311)\n",
            " * Prec@1 74.640\n",
            "\n",
            "Epoch: 22/100\n",
            "Learning rate: 0.089518\n",
            "Epoch: [21][0/391]\tTime 0.310 (0.310)\tData 0.222 (0.222)\tLoss 1.7037 (1.7037)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [21][50/391]\tTime 0.074 (0.121)\tData 0.000 (0.025)\tLoss 1.1608 (1.6032)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [21][100/391]\tTime 0.106 (0.111)\tData 0.007 (0.018)\tLoss 1.6644 (1.5970)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [21][150/391]\tTime 0.161 (0.108)\tData 0.055 (0.018)\tLoss 1.7886 (1.5810)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [21][200/391]\tTime 0.097 (0.110)\tData 0.000 (0.018)\tLoss 1.1541 (1.5879)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [21][250/391]\tTime 0.091 (0.108)\tData 0.000 (0.017)\tLoss 1.5396 (1.5895)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [21][300/391]\tTime 0.103 (0.110)\tData 0.007 (0.018)\tLoss 1.3475 (1.6006)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [21][350/391]\tTime 0.102 (0.108)\tData 0.005 (0.017)\tLoss 1.7690 (1.6114)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.148 (0.148)\tLoss 1.0714 (1.0714)\tPrec@1 78.125 (78.125)\n",
            "Test: [50/79]\tTime 0.018 (0.041)\tLoss 1.0698 (1.1090)\tPrec@1 79.688 (75.705)\n",
            " * Prec@1 75.870\n",
            "\n",
            "Epoch: 23/100\n",
            "Learning rate: 0.088537\n",
            "Epoch: [22][0/391]\tTime 0.321 (0.321)\tData 0.208 (0.208)\tLoss 1.5316 (1.5316)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [22][50/391]\tTime 0.074 (0.101)\tData 0.000 (0.009)\tLoss 1.1631 (1.5903)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [22][100/391]\tTime 0.120 (0.109)\tData 0.008 (0.015)\tLoss 1.5994 (1.6155)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [22][150/391]\tTime 0.081 (0.106)\tData 0.000 (0.014)\tLoss 1.7727 (1.6145)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [22][200/391]\tTime 0.150 (0.105)\tData 0.043 (0.015)\tLoss 1.6502 (1.5952)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [22][250/391]\tTime 0.156 (0.108)\tData 0.067 (0.018)\tLoss 1.4785 (1.6095)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [22][300/391]\tTime 0.132 (0.107)\tData 0.040 (0.019)\tLoss 1.7822 (1.6115)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [22][350/391]\tTime 0.125 (0.109)\tData 0.056 (0.021)\tLoss 1.5497 (1.6105)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.142 (0.142)\tLoss 1.1066 (1.1066)\tPrec@1 75.781 (75.781)\n",
            "Test: [50/79]\tTime 0.015 (0.033)\tLoss 1.0947 (1.1086)\tPrec@1 80.469 (75.812)\n",
            " * Prec@1 75.900\n",
            "\n",
            "Epoch: 24/100\n",
            "Learning rate: 0.087518\n",
            "Epoch: [23][0/391]\tTime 0.333 (0.333)\tData 0.212 (0.212)\tLoss 1.6133 (1.6133)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [23][50/391]\tTime 0.096 (0.121)\tData 0.000 (0.024)\tLoss 1.2567 (1.6109)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [23][100/391]\tTime 0.094 (0.109)\tData 0.000 (0.015)\tLoss 1.6127 (1.5980)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [23][150/391]\tTime 0.092 (0.112)\tData 0.006 (0.018)\tLoss 1.3822 (1.5937)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [23][200/391]\tTime 0.078 (0.108)\tData 0.000 (0.016)\tLoss 1.8945 (1.6100)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [23][250/391]\tTime 0.085 (0.108)\tData 0.000 (0.018)\tLoss 1.8136 (1.6090)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [23][300/391]\tTime 0.131 (0.109)\tData 0.034 (0.019)\tLoss 1.4858 (1.5984)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [23][350/391]\tTime 0.113 (0.108)\tData 0.029 (0.017)\tLoss 1.8666 (1.5963)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.145 (0.145)\tLoss 1.1234 (1.1234)\tPrec@1 78.906 (78.906)\n",
            "Test: [50/79]\tTime 0.045 (0.033)\tLoss 1.0543 (1.0686)\tPrec@1 81.250 (79.274)\n",
            " * Prec@1 79.580\n",
            "\n",
            "Epoch: 25/100\n",
            "Learning rate: 0.086462\n",
            "Epoch: [24][0/391]\tTime 0.318 (0.318)\tData 0.212 (0.212)\tLoss 1.8296 (1.8296)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [24][50/391]\tTime 0.115 (0.104)\tData 0.006 (0.012)\tLoss 1.7592 (1.6198)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [24][100/391]\tTime 0.084 (0.111)\tData 0.007 (0.017)\tLoss 1.9551 (1.5952)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [24][150/391]\tTime 0.083 (0.107)\tData 0.001 (0.015)\tLoss 1.6055 (1.5958)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [24][200/391]\tTime 0.128 (0.109)\tData 0.004 (0.016)\tLoss 1.7871 (1.6045)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [24][250/391]\tTime 0.099 (0.108)\tData 0.005 (0.016)\tLoss 1.6507 (1.6045)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [24][300/391]\tTime 0.127 (0.106)\tData 0.016 (0.015)\tLoss 1.5554 (1.6101)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [24][350/391]\tTime 0.132 (0.108)\tData 0.000 (0.016)\tLoss 1.9227 (1.6048)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.148 (0.148)\tLoss 0.9989 (0.9989)\tPrec@1 79.688 (79.688)\n",
            "Test: [50/79]\tTime 0.034 (0.034)\tLoss 1.0380 (1.0096)\tPrec@1 79.688 (81.173)\n",
            " * Prec@1 81.210\n",
            "\n",
            "Epoch: 26/100\n",
            "Learning rate: 0.085370\n",
            "Epoch: [25][0/391]\tTime 0.413 (0.413)\tData 0.302 (0.302)\tLoss 1.5530 (1.5530)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [25][50/391]\tTime 0.114 (0.120)\tData 0.016 (0.031)\tLoss 1.7436 (1.5535)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [25][100/391]\tTime 0.122 (0.109)\tData 0.027 (0.020)\tLoss 1.8937 (1.5636)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [25][150/391]\tTime 0.073 (0.112)\tData 0.000 (0.022)\tLoss 1.1695 (1.5955)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [25][200/391]\tTime 0.096 (0.109)\tData 0.007 (0.019)\tLoss 1.7666 (1.5892)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [25][250/391]\tTime 0.099 (0.111)\tData 0.003 (0.020)\tLoss 1.6380 (1.6069)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [25][300/391]\tTime 0.129 (0.109)\tData 0.059 (0.020)\tLoss 1.5849 (1.6123)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [25][350/391]\tTime 0.147 (0.108)\tData 0.052 (0.019)\tLoss 1.2273 (1.6162)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.151 (0.151)\tLoss 1.1120 (1.1120)\tPrec@1 81.250 (81.250)\n",
            "Test: [50/79]\tTime 0.032 (0.033)\tLoss 1.0951 (1.1177)\tPrec@1 76.562 (76.287)\n",
            " * Prec@1 75.940\n",
            "\n",
            "Epoch: 27/100\n",
            "Learning rate: 0.084243\n",
            "Epoch: [26][0/391]\tTime 0.335 (0.335)\tData 0.214 (0.214)\tLoss 1.9392 (1.9392)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [26][50/391]\tTime 0.100 (0.103)\tData 0.000 (0.006)\tLoss 1.7579 (1.6325)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [26][100/391]\tTime 0.076 (0.110)\tData 0.003 (0.013)\tLoss 1.2275 (1.6185)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [26][150/391]\tTime 0.139 (0.108)\tData 0.070 (0.015)\tLoss 1.1556 (1.6078)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [26][200/391]\tTime 0.074 (0.111)\tData 0.000 (0.019)\tLoss 1.2763 (1.6115)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [26][250/391]\tTime 0.095 (0.109)\tData 0.002 (0.018)\tLoss 1.3725 (1.6114)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [26][300/391]\tTime 0.164 (0.109)\tData 0.059 (0.019)\tLoss 1.9534 (1.6024)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [26][350/391]\tTime 0.097 (0.109)\tData 0.000 (0.018)\tLoss 1.1367 (1.5996)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.165 (0.165)\tLoss 1.1922 (1.1922)\tPrec@1 75.781 (75.781)\n",
            "Test: [50/79]\tTime 0.039 (0.037)\tLoss 1.2150 (1.1678)\tPrec@1 69.531 (73.315)\n",
            " * Prec@1 73.440\n",
            "\n",
            "Epoch: 28/100\n",
            "Learning rate: 0.083083\n",
            "Epoch: [27][0/391]\tTime 0.457 (0.457)\tData 0.340 (0.340)\tLoss 1.5331 (1.5331)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [27][50/391]\tTime 0.095 (0.106)\tData 0.000 (0.011)\tLoss 1.4098 (1.5732)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [27][100/391]\tTime 0.088 (0.103)\tData 0.000 (0.007)\tLoss 1.8077 (1.6022)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [27][150/391]\tTime 0.100 (0.107)\tData 0.000 (0.011)\tLoss 1.5142 (1.5927)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [27][200/391]\tTime 0.114 (0.104)\tData 0.037 (0.010)\tLoss 1.4300 (1.5879)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [27][250/391]\tTime 0.100 (0.107)\tData 0.000 (0.013)\tLoss 1.4429 (1.5883)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [27][300/391]\tTime 0.082 (0.106)\tData 0.009 (0.013)\tLoss 1.5469 (1.5967)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [27][350/391]\tTime 0.181 (0.107)\tData 0.078 (0.014)\tLoss 1.8661 (1.5994)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.144 (0.144)\tLoss 1.1225 (1.1225)\tPrec@1 78.125 (78.125)\n",
            "Test: [50/79]\tTime 0.015 (0.033)\tLoss 1.1318 (1.1365)\tPrec@1 75.781 (76.118)\n",
            " * Prec@1 76.180\n",
            "\n",
            "Epoch: 29/100\n",
            "Learning rate: 0.081889\n",
            "Epoch: [28][0/391]\tTime 0.320 (0.320)\tData 0.204 (0.204)\tLoss 1.3230 (1.3230)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [28][50/391]\tTime 0.102 (0.111)\tData 0.000 (0.017)\tLoss 1.8822 (1.6212)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [28][100/391]\tTime 0.083 (0.112)\tData 0.009 (0.020)\tLoss 1.6594 (1.6272)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [28][150/391]\tTime 0.107 (0.108)\tData 0.000 (0.017)\tLoss 1.5881 (1.6201)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [28][200/391]\tTime 0.099 (0.111)\tData 0.002 (0.019)\tLoss 1.6890 (1.6020)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [28][250/391]\tTime 0.072 (0.109)\tData 0.000 (0.017)\tLoss 1.3510 (1.5990)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [28][300/391]\tTime 0.080 (0.110)\tData 0.000 (0.018)\tLoss 1.8456 (1.6027)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [28][350/391]\tTime 0.105 (0.109)\tData 0.018 (0.018)\tLoss 1.1565 (1.5917)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.164 (0.164)\tLoss 1.0669 (1.0669)\tPrec@1 78.906 (78.906)\n",
            "Test: [50/79]\tTime 0.085 (0.048)\tLoss 1.0787 (1.1030)\tPrec@1 74.219 (75.827)\n",
            " * Prec@1 75.620\n",
            "\n",
            "Epoch: 30/100\n",
            "Learning rate: 0.080665\n",
            "Epoch: [29][0/391]\tTime 0.325 (0.325)\tData 0.217 (0.217)\tLoss 1.2272 (1.2272)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [29][50/391]\tTime 0.096 (0.104)\tData 0.000 (0.010)\tLoss 1.8116 (1.6032)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [29][100/391]\tTime 0.158 (0.109)\tData 0.005 (0.016)\tLoss 1.6086 (1.5979)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [29][150/391]\tTime 0.134 (0.109)\tData 0.044 (0.016)\tLoss 1.8944 (1.6016)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [29][200/391]\tTime 0.101 (0.107)\tData 0.000 (0.015)\tLoss 1.6025 (1.6087)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [29][250/391]\tTime 0.137 (0.110)\tData 0.066 (0.018)\tLoss 1.4938 (1.6198)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [29][300/391]\tTime 0.138 (0.109)\tData 0.066 (0.018)\tLoss 1.6244 (1.6155)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [29][350/391]\tTime 0.075 (0.110)\tData 0.000 (0.019)\tLoss 1.4267 (1.6090)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.147 (0.147)\tLoss 1.0927 (1.0927)\tPrec@1 79.688 (79.688)\n",
            "Test: [50/79]\tTime 0.026 (0.034)\tLoss 1.1726 (1.1172)\tPrec@1 75.781 (76.823)\n",
            " * Prec@1 76.600\n",
            "\n",
            "Epoch: 31/100\n",
            "Learning rate: 0.079410\n",
            "Epoch: [30][0/391]\tTime 0.323 (0.323)\tData 0.206 (0.206)\tLoss 1.8276 (1.8276)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [30][50/391]\tTime 0.107 (0.121)\tData 0.036 (0.020)\tLoss 1.1865 (1.6509)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [30][100/391]\tTime 0.094 (0.112)\tData 0.000 (0.012)\tLoss 1.8520 (1.6202)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [30][150/391]\tTime 0.112 (0.111)\tData 0.000 (0.015)\tLoss 1.5523 (1.6133)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [30][200/391]\tTime 0.080 (0.111)\tData 0.000 (0.018)\tLoss 1.8658 (1.5973)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [30][250/391]\tTime 0.135 (0.109)\tData 0.059 (0.019)\tLoss 1.8083 (1.5857)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [30][300/391]\tTime 0.107 (0.111)\tData 0.033 (0.021)\tLoss 1.8882 (1.5907)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [30][350/391]\tTime 0.103 (0.109)\tData 0.009 (0.019)\tLoss 1.0361 (1.5863)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.162 (0.162)\tLoss 1.1288 (1.1288)\tPrec@1 75.000 (75.000)\n",
            "Test: [50/79]\tTime 0.017 (0.033)\tLoss 1.2826 (1.2026)\tPrec@1 68.750 (71.201)\n",
            " * Prec@1 71.260\n",
            "\n",
            "Epoch: 32/100\n",
            "Learning rate: 0.078126\n",
            "Epoch: [31][0/391]\tTime 0.323 (0.323)\tData 0.208 (0.208)\tLoss 1.6363 (1.6363)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [31][50/391]\tTime 0.100 (0.104)\tData 0.000 (0.016)\tLoss 1.9548 (1.5922)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [31][100/391]\tTime 0.083 (0.111)\tData 0.000 (0.021)\tLoss 1.6729 (1.5414)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [31][150/391]\tTime 0.096 (0.108)\tData 0.000 (0.020)\tLoss 1.8157 (1.5505)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [31][200/391]\tTime 0.098 (0.108)\tData 0.002 (0.018)\tLoss 1.5618 (1.5521)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [31][250/391]\tTime 0.072 (0.108)\tData 0.000 (0.019)\tLoss 1.8352 (1.5597)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [31][300/391]\tTime 0.093 (0.107)\tData 0.000 (0.019)\tLoss 1.7530 (1.5623)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [31][350/391]\tTime 0.075 (0.109)\tData 0.000 (0.020)\tLoss 1.1681 (1.5657)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.160 (0.160)\tLoss 1.0546 (1.0546)\tPrec@1 78.125 (78.125)\n",
            "Test: [50/79]\tTime 0.033 (0.033)\tLoss 0.9835 (1.0385)\tPrec@1 82.031 (79.550)\n",
            " * Prec@1 79.620\n",
            "\n",
            "Epoch: 33/100\n",
            "Learning rate: 0.076815\n",
            "Epoch: [32][0/391]\tTime 0.337 (0.337)\tData 0.215 (0.215)\tLoss 1.5185 (1.5185)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [32][50/391]\tTime 0.112 (0.119)\tData 0.006 (0.020)\tLoss 1.6286 (1.5530)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [32][100/391]\tTime 0.094 (0.109)\tData 0.000 (0.015)\tLoss 1.3484 (1.5555)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [32][150/391]\tTime 0.074 (0.113)\tData 0.000 (0.018)\tLoss 1.4710 (1.5741)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [32][200/391]\tTime 0.129 (0.110)\tData 0.060 (0.020)\tLoss 1.6717 (1.5637)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [32][250/391]\tTime 0.197 (0.111)\tData 0.100 (0.021)\tLoss 1.8840 (1.5717)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [32][300/391]\tTime 0.110 (0.110)\tData 0.040 (0.020)\tLoss 1.7344 (1.5731)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [32][350/391]\tTime 0.106 (0.108)\tData 0.011 (0.018)\tLoss 1.2598 (1.5688)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.147 (0.147)\tLoss 1.1061 (1.1061)\tPrec@1 79.688 (79.688)\n",
            "Test: [50/79]\tTime 0.016 (0.033)\tLoss 1.0693 (1.0852)\tPrec@1 81.250 (78.033)\n",
            " * Prec@1 78.150\n",
            "\n",
            "Epoch: 34/100\n",
            "Learning rate: 0.075477\n",
            "Epoch: [33][0/391]\tTime 0.322 (0.322)\tData 0.210 (0.210)\tLoss 1.3639 (1.3639)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [33][50/391]\tTime 0.101 (0.103)\tData 0.000 (0.013)\tLoss 1.2847 (1.5812)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [33][100/391]\tTime 0.099 (0.109)\tData 0.006 (0.016)\tLoss 1.7088 (1.5837)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [33][150/391]\tTime 0.073 (0.106)\tData 0.000 (0.015)\tLoss 1.8824 (1.5685)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [33][200/391]\tTime 0.076 (0.109)\tData 0.000 (0.019)\tLoss 1.7820 (1.5759)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [33][250/391]\tTime 0.091 (0.108)\tData 0.000 (0.019)\tLoss 1.7943 (1.5825)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [33][300/391]\tTime 0.107 (0.108)\tData 0.019 (0.017)\tLoss 1.2848 (1.5819)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [33][350/391]\tTime 0.094 (0.108)\tData 0.007 (0.018)\tLoss 1.3626 (1.5835)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.149 (0.149)\tLoss 1.1672 (1.1672)\tPrec@1 70.312 (70.312)\n",
            "Test: [50/79]\tTime 0.032 (0.035)\tLoss 1.1122 (1.1268)\tPrec@1 71.875 (75.751)\n",
            " * Prec@1 75.570\n",
            "\n",
            "Epoch: 35/100\n",
            "Learning rate: 0.074114\n",
            "Epoch: [34][0/391]\tTime 0.468 (0.468)\tData 0.326 (0.326)\tLoss 1.4421 (1.4421)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [34][50/391]\tTime 0.100 (0.112)\tData 0.004 (0.019)\tLoss 1.3276 (1.5975)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [34][100/391]\tTime 0.099 (0.107)\tData 0.000 (0.014)\tLoss 1.7784 (1.5742)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [34][150/391]\tTime 0.122 (0.110)\tData 0.022 (0.017)\tLoss 1.8416 (1.5659)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [34][200/391]\tTime 0.109 (0.109)\tData 0.016 (0.017)\tLoss 1.4735 (1.5694)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [34][250/391]\tTime 0.098 (0.111)\tData 0.009 (0.018)\tLoss 1.3087 (1.5795)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [34][300/391]\tTime 0.128 (0.109)\tData 0.043 (0.017)\tLoss 1.5247 (1.5898)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [34][350/391]\tTime 0.159 (0.109)\tData 0.033 (0.017)\tLoss 1.7198 (1.5941)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.151 (0.151)\tLoss 1.0963 (1.0963)\tPrec@1 78.906 (78.906)\n",
            "Test: [50/79]\tTime 0.036 (0.033)\tLoss 1.0795 (1.0668)\tPrec@1 78.906 (78.186)\n",
            " * Prec@1 78.310\n",
            "\n",
            "Epoch: 36/100\n",
            "Learning rate: 0.072727\n",
            "Epoch: [35][0/391]\tTime 0.346 (0.346)\tData 0.227 (0.227)\tLoss 1.2900 (1.2900)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [35][50/391]\tTime 0.245 (0.119)\tData 0.138 (0.027)\tLoss 1.6102 (1.6059)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [35][100/391]\tTime 0.134 (0.111)\tData 0.063 (0.022)\tLoss 1.6413 (1.5996)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [35][150/391]\tTime 0.097 (0.108)\tData 0.000 (0.021)\tLoss 1.8883 (1.5864)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [35][200/391]\tTime 0.085 (0.111)\tData 0.000 (0.023)\tLoss 1.4376 (1.5842)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [35][250/391]\tTime 0.138 (0.110)\tData 0.066 (0.022)\tLoss 1.6122 (1.5749)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [35][300/391]\tTime 0.100 (0.111)\tData 0.005 (0.022)\tLoss 1.1366 (1.5666)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [35][350/391]\tTime 0.117 (0.109)\tData 0.027 (0.021)\tLoss 1.8244 (1.5718)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.219 (0.219)\tLoss 1.0458 (1.0458)\tPrec@1 78.125 (78.125)\n",
            "Test: [50/79]\tTime 0.053 (0.048)\tLoss 0.9940 (0.9956)\tPrec@1 82.031 (80.591)\n",
            " * Prec@1 80.730\n",
            "\n",
            "Epoch: 37/100\n",
            "Learning rate: 0.071318\n",
            "Epoch: [36][0/391]\tTime 0.328 (0.328)\tData 0.212 (0.212)\tLoss 1.8016 (1.8016)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [36][50/391]\tTime 0.075 (0.102)\tData 0.000 (0.014)\tLoss 1.7491 (1.5811)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [36][100/391]\tTime 0.247 (0.107)\tData 0.158 (0.017)\tLoss 1.3804 (1.5724)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [36][150/391]\tTime 0.100 (0.107)\tData 0.005 (0.017)\tLoss 1.9658 (1.5736)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [36][200/391]\tTime 0.083 (0.107)\tData 0.000 (0.021)\tLoss 1.7992 (1.5748)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [36][250/391]\tTime 0.159 (0.110)\tData 0.074 (0.023)\tLoss 1.8239 (1.5674)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [36][300/391]\tTime 0.156 (0.109)\tData 0.086 (0.021)\tLoss 1.8054 (1.5700)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [36][350/391]\tTime 0.091 (0.111)\tData 0.000 (0.022)\tLoss 1.4743 (1.5715)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.160 (0.160)\tLoss 1.0710 (1.0710)\tPrec@1 77.344 (77.344)\n",
            "Test: [50/79]\tTime 0.041 (0.035)\tLoss 1.0546 (1.0892)\tPrec@1 79.688 (77.313)\n",
            " * Prec@1 77.300\n",
            "\n",
            "Epoch: 38/100\n",
            "Learning rate: 0.069888\n",
            "Epoch: [37][0/391]\tTime 0.339 (0.339)\tData 0.229 (0.229)\tLoss 1.3561 (1.3561)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [37][50/391]\tTime 0.106 (0.124)\tData 0.007 (0.025)\tLoss 1.6673 (1.5667)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [37][100/391]\tTime 0.115 (0.112)\tData 0.023 (0.017)\tLoss 1.4491 (1.5353)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [37][150/391]\tTime 0.104 (0.114)\tData 0.009 (0.019)\tLoss 1.3017 (1.5333)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [37][200/391]\tTime 0.111 (0.111)\tData 0.007 (0.018)\tLoss 1.5916 (1.5457)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [37][250/391]\tTime 0.163 (0.110)\tData 0.070 (0.016)\tLoss 1.6951 (1.5556)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [37][300/391]\tTime 0.152 (0.111)\tData 0.061 (0.018)\tLoss 1.6466 (1.5554)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [37][350/391]\tTime 0.114 (0.110)\tData 0.015 (0.017)\tLoss 1.9484 (1.5641)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.139 (0.139)\tLoss 1.1029 (1.1029)\tPrec@1 78.906 (78.906)\n",
            "Test: [50/79]\tTime 0.031 (0.035)\tLoss 1.0993 (1.1101)\tPrec@1 76.562 (75.720)\n",
            " * Prec@1 75.940\n",
            "\n",
            "Epoch: 39/100\n",
            "Learning rate: 0.068438\n",
            "Epoch: [38][0/391]\tTime 0.336 (0.336)\tData 0.213 (0.213)\tLoss 1.8424 (1.8424)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [38][50/391]\tTime 0.075 (0.104)\tData 0.000 (0.011)\tLoss 1.4220 (1.5925)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [38][100/391]\tTime 0.094 (0.112)\tData 0.000 (0.019)\tLoss 1.2958 (1.5810)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [38][150/391]\tTime 0.077 (0.110)\tData 0.000 (0.020)\tLoss 1.8931 (1.5839)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [38][200/391]\tTime 0.074 (0.112)\tData 0.000 (0.023)\tLoss 1.0752 (1.5698)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [38][250/391]\tTime 0.074 (0.111)\tData 0.000 (0.024)\tLoss 1.8423 (1.5668)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [38][300/391]\tTime 0.083 (0.112)\tData 0.000 (0.024)\tLoss 1.3448 (1.5693)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [38][350/391]\tTime 0.103 (0.111)\tData 0.001 (0.023)\tLoss 1.5028 (1.5674)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.163 (0.163)\tLoss 1.0674 (1.0674)\tPrec@1 78.125 (78.125)\n",
            "Test: [50/79]\tTime 0.044 (0.040)\tLoss 1.0340 (1.0630)\tPrec@1 79.688 (78.554)\n",
            " * Prec@1 78.400\n",
            "\n",
            "Epoch: 40/100\n",
            "Learning rate: 0.066970\n",
            "Epoch: [39][0/391]\tTime 0.470 (0.470)\tData 0.371 (0.371)\tLoss 1.8291 (1.8291)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [39][50/391]\tTime 0.099 (0.107)\tData 0.005 (0.013)\tLoss 1.8384 (1.5545)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [39][100/391]\tTime 0.124 (0.103)\tData 0.028 (0.012)\tLoss 1.6442 (1.5526)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [39][150/391]\tTime 0.098 (0.109)\tData 0.000 (0.017)\tLoss 1.5423 (1.5585)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [39][200/391]\tTime 0.093 (0.107)\tData 0.000 (0.016)\tLoss 1.6018 (1.5529)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [39][250/391]\tTime 0.086 (0.109)\tData 0.000 (0.017)\tLoss 1.3901 (1.5497)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [39][300/391]\tTime 0.072 (0.108)\tData 0.000 (0.016)\tLoss 1.2350 (1.5478)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [39][350/391]\tTime 0.107 (0.109)\tData 0.016 (0.018)\tLoss 1.7442 (1.5487)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.158 (0.158)\tLoss 1.0214 (1.0214)\tPrec@1 82.031 (82.031)\n",
            "Test: [50/79]\tTime 0.015 (0.033)\tLoss 1.0095 (1.0010)\tPrec@1 81.250 (81.495)\n",
            " * Prec@1 81.740\n",
            "\n",
            "Epoch: 41/100\n",
            "Learning rate: 0.065485\n",
            "Epoch: [40][0/391]\tTime 0.330 (0.330)\tData 0.218 (0.218)\tLoss 1.4140 (1.4140)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [40][50/391]\tTime 0.075 (0.124)\tData 0.000 (0.028)\tLoss 1.8247 (1.5341)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [40][100/391]\tTime 0.100 (0.114)\tData 0.006 (0.024)\tLoss 1.8300 (1.5653)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [40][150/391]\tTime 0.090 (0.113)\tData 0.000 (0.022)\tLoss 1.8327 (1.5551)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [40][200/391]\tTime 0.103 (0.112)\tData 0.000 (0.021)\tLoss 1.5029 (1.5616)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [40][250/391]\tTime 0.127 (0.109)\tData 0.056 (0.019)\tLoss 1.6348 (1.5577)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [40][300/391]\tTime 0.120 (0.111)\tData 0.045 (0.020)\tLoss 1.6221 (1.5560)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [40][350/391]\tTime 0.121 (0.110)\tData 0.006 (0.019)\tLoss 1.5138 (1.5608)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.215 (0.215)\tLoss 1.0046 (1.0046)\tPrec@1 82.031 (82.031)\n",
            "Test: [50/79]\tTime 0.036 (0.040)\tLoss 1.0694 (1.0240)\tPrec@1 78.906 (80.714)\n",
            " * Prec@1 81.010\n",
            "\n",
            "Epoch: 42/100\n",
            "Learning rate: 0.063986\n",
            "Epoch: [41][0/391]\tTime 0.330 (0.330)\tData 0.215 (0.215)\tLoss 1.6738 (1.6738)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [41][50/391]\tTime 0.096 (0.104)\tData 0.005 (0.017)\tLoss 1.8084 (1.5552)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [41][100/391]\tTime 0.091 (0.111)\tData 0.005 (0.020)\tLoss 1.3093 (1.5746)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [41][150/391]\tTime 0.094 (0.108)\tData 0.006 (0.021)\tLoss 1.0727 (1.5374)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [41][200/391]\tTime 0.117 (0.110)\tData 0.007 (0.021)\tLoss 1.2348 (1.5424)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [41][250/391]\tTime 0.092 (0.110)\tData 0.000 (0.021)\tLoss 1.6523 (1.5362)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [41][300/391]\tTime 0.072 (0.108)\tData 0.000 (0.021)\tLoss 1.9380 (1.5399)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [41][350/391]\tTime 0.105 (0.110)\tData 0.010 (0.022)\tLoss 1.2051 (1.5408)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.150 (0.150)\tLoss 1.0982 (1.0982)\tPrec@1 78.906 (78.906)\n",
            "Test: [50/79]\tTime 0.044 (0.034)\tLoss 1.0860 (1.0869)\tPrec@1 74.219 (77.987)\n",
            " * Prec@1 78.140\n",
            "\n",
            "Epoch: 43/100\n",
            "Learning rate: 0.062472\n",
            "Epoch: [42][0/391]\tTime 0.344 (0.344)\tData 0.225 (0.225)\tLoss 1.3805 (1.3805)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [42][50/391]\tTime 0.093 (0.121)\tData 0.000 (0.025)\tLoss 1.3274 (1.4455)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [42][100/391]\tTime 0.089 (0.112)\tData 0.000 (0.019)\tLoss 1.2240 (1.4904)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [42][150/391]\tTime 0.094 (0.114)\tData 0.000 (0.021)\tLoss 1.4799 (1.5146)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [42][200/391]\tTime 0.074 (0.111)\tData 0.000 (0.019)\tLoss 1.5739 (1.5200)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [42][250/391]\tTime 0.208 (0.112)\tData 0.106 (0.022)\tLoss 1.7755 (1.5110)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [42][300/391]\tTime 0.074 (0.110)\tData 0.000 (0.020)\tLoss 1.8604 (1.5234)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [42][350/391]\tTime 0.128 (0.109)\tData 0.027 (0.020)\tLoss 1.1160 (1.5222)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.158 (0.158)\tLoss 1.0160 (1.0160)\tPrec@1 82.812 (82.812)\n",
            "Test: [50/79]\tTime 0.030 (0.033)\tLoss 0.9623 (0.9851)\tPrec@1 86.719 (82.889)\n",
            " * Prec@1 82.980\n",
            "\n",
            "Epoch: 44/100\n",
            "Learning rate: 0.060946\n",
            "Epoch: [43][0/391]\tTime 0.330 (0.330)\tData 0.221 (0.221)\tLoss 1.7587 (1.7587)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [43][50/391]\tTime 0.149 (0.104)\tData 0.071 (0.019)\tLoss 1.4406 (1.5445)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [43][100/391]\tTime 0.142 (0.112)\tData 0.067 (0.026)\tLoss 1.2396 (1.5568)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [43][150/391]\tTime 0.077 (0.109)\tData 0.000 (0.022)\tLoss 1.2165 (1.5653)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [43][200/391]\tTime 0.099 (0.112)\tData 0.007 (0.024)\tLoss 1.6191 (1.5632)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [43][250/391]\tTime 0.108 (0.110)\tData 0.007 (0.022)\tLoss 1.2035 (1.5561)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [43][300/391]\tTime 0.099 (0.112)\tData 0.000 (0.024)\tLoss 1.9264 (1.5604)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [43][350/391]\tTime 0.091 (0.111)\tData 0.000 (0.023)\tLoss 1.7535 (1.5578)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.157 (0.157)\tLoss 0.9659 (0.9659)\tPrec@1 83.594 (83.594)\n",
            "Test: [50/79]\tTime 0.069 (0.044)\tLoss 0.9977 (0.9808)\tPrec@1 81.250 (81.801)\n",
            " * Prec@1 81.780\n",
            "\n",
            "Epoch: 45/100\n",
            "Learning rate: 0.059410\n",
            "Epoch: [44][0/391]\tTime 0.320 (0.320)\tData 0.205 (0.205)\tLoss 1.7367 (1.7367)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [44][50/391]\tTime 0.102 (0.104)\tData 0.000 (0.019)\tLoss 1.8439 (1.6036)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [44][100/391]\tTime 0.100 (0.105)\tData 0.000 (0.019)\tLoss 1.5553 (1.5654)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [44][150/391]\tTime 0.082 (0.109)\tData 0.000 (0.021)\tLoss 1.8386 (1.5555)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [44][200/391]\tTime 0.121 (0.107)\tData 0.023 (0.021)\tLoss 1.7309 (1.5438)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [44][250/391]\tTime 0.081 (0.109)\tData 0.011 (0.021)\tLoss 1.7295 (1.5421)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [44][300/391]\tTime 0.088 (0.108)\tData 0.000 (0.021)\tLoss 1.4454 (1.5485)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [44][350/391]\tTime 0.102 (0.109)\tData 0.000 (0.021)\tLoss 1.2611 (1.5443)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.154 (0.154)\tLoss 1.0454 (1.0454)\tPrec@1 83.594 (83.594)\n",
            "Test: [50/79]\tTime 0.033 (0.034)\tLoss 1.0835 (1.0688)\tPrec@1 75.781 (78.156)\n",
            " * Prec@1 78.170\n",
            "\n",
            "Epoch: 46/100\n",
            "Learning rate: 0.057864\n",
            "Epoch: [45][0/391]\tTime 0.317 (0.317)\tData 0.211 (0.211)\tLoss 1.8373 (1.8373)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [45][50/391]\tTime 0.100 (0.122)\tData 0.005 (0.028)\tLoss 1.7499 (1.5369)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [45][100/391]\tTime 0.074 (0.111)\tData 0.000 (0.020)\tLoss 1.1942 (1.5324)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [45][150/391]\tTime 0.090 (0.110)\tData 0.004 (0.022)\tLoss 1.7236 (1.5303)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [45][200/391]\tTime 0.188 (0.111)\tData 0.096 (0.022)\tLoss 0.9931 (1.5195)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [45][250/391]\tTime 0.099 (0.108)\tData 0.000 (0.019)\tLoss 1.7863 (1.5307)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [45][300/391]\tTime 0.107 (0.110)\tData 0.000 (0.020)\tLoss 1.7481 (1.5134)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [45][350/391]\tTime 0.088 (0.109)\tData 0.017 (0.019)\tLoss 1.3928 (1.5194)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.216 (0.216)\tLoss 0.9081 (0.9081)\tPrec@1 85.938 (85.938)\n",
            "Test: [50/79]\tTime 0.017 (0.035)\tLoss 0.9470 (0.9421)\tPrec@1 82.812 (84.084)\n",
            " * Prec@1 84.080\n",
            "\n",
            "Epoch: 47/100\n",
            "Learning rate: 0.056310\n",
            "Epoch: [46][0/391]\tTime 0.327 (0.327)\tData 0.218 (0.218)\tLoss 1.7973 (1.7973)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [46][50/391]\tTime 0.092 (0.102)\tData 0.000 (0.011)\tLoss 1.7112 (1.5444)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [46][100/391]\tTime 0.098 (0.110)\tData 0.005 (0.016)\tLoss 1.7915 (1.5407)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [46][150/391]\tTime 0.097 (0.106)\tData 0.005 (0.014)\tLoss 1.6120 (1.5378)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [46][200/391]\tTime 0.108 (0.106)\tData 0.012 (0.015)\tLoss 1.7644 (1.5369)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [46][250/391]\tTime 0.105 (0.107)\tData 0.019 (0.015)\tLoss 1.2092 (1.5364)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [46][300/391]\tTime 0.100 (0.105)\tData 0.005 (0.014)\tLoss 1.4012 (1.5296)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [46][350/391]\tTime 0.082 (0.107)\tData 0.000 (0.015)\tLoss 1.7026 (1.5412)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.144 (0.144)\tLoss 0.9565 (0.9565)\tPrec@1 83.594 (83.594)\n",
            "Test: [50/79]\tTime 0.023 (0.033)\tLoss 0.9521 (0.9777)\tPrec@1 82.812 (81.725)\n",
            " * Prec@1 81.890\n",
            "\n",
            "Epoch: 48/100\n",
            "Learning rate: 0.054751\n",
            "Epoch: [47][0/391]\tTime 0.356 (0.356)\tData 0.243 (0.243)\tLoss 1.7589 (1.7589)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [47][50/391]\tTime 0.095 (0.121)\tData 0.000 (0.032)\tLoss 1.1637 (1.5658)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [47][100/391]\tTime 0.098 (0.112)\tData 0.007 (0.025)\tLoss 1.7139 (1.5603)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [47][150/391]\tTime 0.086 (0.113)\tData 0.000 (0.023)\tLoss 1.8357 (1.5519)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [47][200/391]\tTime 0.094 (0.109)\tData 0.000 (0.019)\tLoss 1.5930 (1.5492)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [47][250/391]\tTime 0.253 (0.110)\tData 0.155 (0.021)\tLoss 1.0918 (1.5462)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [47][300/391]\tTime 0.140 (0.110)\tData 0.052 (0.021)\tLoss 1.1698 (1.5532)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [47][350/391]\tTime 0.097 (0.109)\tData 0.004 (0.020)\tLoss 1.4012 (1.5497)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.149 (0.149)\tLoss 0.9807 (0.9807)\tPrec@1 82.031 (82.031)\n",
            "Test: [50/79]\tTime 0.034 (0.033)\tLoss 0.9664 (0.9982)\tPrec@1 82.031 (80.821)\n",
            " * Prec@1 80.800\n",
            "\n",
            "Epoch: 49/100\n",
            "Learning rate: 0.053186\n",
            "Epoch: [48][0/391]\tTime 0.322 (0.322)\tData 0.213 (0.213)\tLoss 1.4842 (1.4842)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [48][50/391]\tTime 0.074 (0.103)\tData 0.000 (0.015)\tLoss 1.7698 (1.5404)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [48][100/391]\tTime 0.103 (0.112)\tData 0.015 (0.020)\tLoss 1.4262 (1.5340)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [48][150/391]\tTime 0.105 (0.108)\tData 0.007 (0.018)\tLoss 1.8377 (1.5319)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [48][200/391]\tTime 0.136 (0.110)\tData 0.041 (0.017)\tLoss 1.6608 (1.5433)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [48][250/391]\tTime 0.090 (0.108)\tData 0.000 (0.015)\tLoss 1.7809 (1.5481)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [48][300/391]\tTime 0.145 (0.107)\tData 0.058 (0.015)\tLoss 1.8305 (1.5514)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [48][350/391]\tTime 0.099 (0.108)\tData 0.005 (0.016)\tLoss 1.8038 (1.5458)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.151 (0.151)\tLoss 1.0118 (1.0118)\tPrec@1 84.375 (84.375)\n",
            "Test: [50/79]\tTime 0.034 (0.033)\tLoss 0.9923 (0.9959)\tPrec@1 85.938 (83.272)\n",
            " * Prec@1 83.420\n",
            "\n",
            "Epoch: 50/100\n",
            "Learning rate: 0.051619\n",
            "Epoch: [49][0/391]\tTime 0.414 (0.414)\tData 0.233 (0.233)\tLoss 1.4901 (1.4901)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [49][50/391]\tTime 0.122 (0.122)\tData 0.034 (0.029)\tLoss 1.8675 (1.5318)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [49][100/391]\tTime 0.125 (0.110)\tData 0.031 (0.017)\tLoss 1.2104 (1.5589)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [49][150/391]\tTime 0.136 (0.112)\tData 0.045 (0.018)\tLoss 1.8138 (1.5306)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [49][200/391]\tTime 0.105 (0.109)\tData 0.000 (0.016)\tLoss 1.5332 (1.5122)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [49][250/391]\tTime 0.169 (0.109)\tData 0.062 (0.018)\tLoss 1.7540 (1.5184)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [49][300/391]\tTime 0.099 (0.108)\tData 0.007 (0.017)\tLoss 1.1552 (1.5151)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [49][350/391]\tTime 0.092 (0.107)\tData 0.007 (0.016)\tLoss 1.8245 (1.5214)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.153 (0.153)\tLoss 1.0080 (1.0080)\tPrec@1 80.469 (80.469)\n",
            "Test: [50/79]\tTime 0.031 (0.034)\tLoss 0.9995 (0.9994)\tPrec@1 82.031 (81.955)\n",
            " * Prec@1 81.890\n",
            "\n",
            "Epoch: 51/100\n",
            "Learning rate: 0.050050\n",
            "Epoch: [50][0/391]\tTime 0.319 (0.319)\tData 0.228 (0.228)\tLoss 1.8284 (1.8284)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [50][50/391]\tTime 0.085 (0.103)\tData 0.000 (0.011)\tLoss 1.5158 (1.5562)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [50][100/391]\tTime 0.077 (0.112)\tData 0.000 (0.018)\tLoss 1.4758 (1.5309)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [50][150/391]\tTime 0.109 (0.108)\tData 0.005 (0.016)\tLoss 1.1825 (1.5169)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [50][200/391]\tTime 0.138 (0.110)\tData 0.049 (0.018)\tLoss 1.6994 (1.5208)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [50][250/391]\tTime 0.086 (0.108)\tData 0.000 (0.016)\tLoss 1.5997 (1.5181)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [50][300/391]\tTime 0.094 (0.108)\tData 0.000 (0.016)\tLoss 1.5650 (1.5136)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [50][350/391]\tTime 0.160 (0.108)\tData 0.067 (0.016)\tLoss 1.1401 (1.5116)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.157 (0.157)\tLoss 0.9854 (0.9854)\tPrec@1 82.812 (82.812)\n",
            "Test: [50/79]\tTime 0.039 (0.034)\tLoss 0.9699 (0.9826)\tPrec@1 85.156 (83.931)\n",
            " * Prec@1 83.740\n",
            "\n",
            "Epoch: 52/100\n",
            "Learning rate: 0.048481\n",
            "Epoch: [51][0/391]\tTime 0.438 (0.438)\tData 0.304 (0.304)\tLoss 1.7003 (1.7003)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [51][50/391]\tTime 0.098 (0.109)\tData 0.013 (0.015)\tLoss 1.5162 (1.5683)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [51][100/391]\tTime 0.098 (0.103)\tData 0.005 (0.011)\tLoss 1.3704 (1.5436)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [51][150/391]\tTime 0.112 (0.109)\tData 0.015 (0.014)\tLoss 1.4697 (1.5453)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [51][200/391]\tTime 0.096 (0.106)\tData 0.000 (0.012)\tLoss 1.3736 (1.5352)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [51][250/391]\tTime 0.075 (0.108)\tData 0.000 (0.014)\tLoss 1.7808 (1.5339)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [51][300/391]\tTime 0.073 (0.107)\tData 0.000 (0.014)\tLoss 1.7203 (1.5355)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [51][350/391]\tTime 0.263 (0.108)\tData 0.123 (0.016)\tLoss 1.6042 (1.5336)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.151 (0.151)\tLoss 0.9664 (0.9664)\tPrec@1 85.938 (85.938)\n",
            "Test: [50/79]\tTime 0.030 (0.034)\tLoss 0.9501 (0.9528)\tPrec@1 79.688 (84.329)\n",
            " * Prec@1 84.460\n",
            "\n",
            "Epoch: 53/100\n",
            "Learning rate: 0.046914\n",
            "Epoch: [52][0/391]\tTime 0.325 (0.325)\tData 0.211 (0.211)\tLoss 1.3284 (1.3284)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [52][50/391]\tTime 0.250 (0.115)\tData 0.147 (0.020)\tLoss 1.7166 (1.4842)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [52][100/391]\tTime 0.093 (0.111)\tData 0.000 (0.016)\tLoss 1.2683 (1.4688)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [52][150/391]\tTime 0.124 (0.108)\tData 0.054 (0.014)\tLoss 1.5383 (1.4869)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [52][200/391]\tTime 0.101 (0.110)\tData 0.012 (0.018)\tLoss 1.7753 (1.4961)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [52][250/391]\tTime 0.093 (0.108)\tData 0.005 (0.016)\tLoss 1.6583 (1.5061)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [52][300/391]\tTime 0.102 (0.109)\tData 0.031 (0.017)\tLoss 1.1817 (1.5115)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [52][350/391]\tTime 0.107 (0.108)\tData 0.003 (0.016)\tLoss 1.1681 (1.5165)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.225 (0.225)\tLoss 1.0301 (1.0301)\tPrec@1 80.469 (80.469)\n",
            "Test: [50/79]\tTime 0.015 (0.049)\tLoss 1.0281 (1.0170)\tPrec@1 78.906 (80.561)\n",
            " * Prec@1 80.830\n",
            "\n",
            "Epoch: 54/100\n",
            "Learning rate: 0.045349\n",
            "Epoch: [53][0/391]\tTime 0.347 (0.347)\tData 0.217 (0.217)\tLoss 1.7095 (1.7095)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [53][50/391]\tTime 0.094 (0.102)\tData 0.004 (0.010)\tLoss 1.5181 (1.5227)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [53][100/391]\tTime 0.119 (0.107)\tData 0.000 (0.017)\tLoss 1.8716 (1.4754)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [53][150/391]\tTime 0.095 (0.106)\tData 0.000 (0.015)\tLoss 1.5265 (1.5181)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [53][200/391]\tTime 0.100 (0.104)\tData 0.000 (0.014)\tLoss 1.6366 (1.5165)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [53][250/391]\tTime 0.096 (0.108)\tData 0.000 (0.015)\tLoss 1.6364 (1.5315)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [53][300/391]\tTime 0.077 (0.107)\tData 0.000 (0.016)\tLoss 1.6859 (1.5301)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [53][350/391]\tTime 0.099 (0.109)\tData 0.005 (0.017)\tLoss 1.3455 (1.5267)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.148 (0.148)\tLoss 1.1016 (1.1016)\tPrec@1 77.344 (77.344)\n",
            "Test: [50/79]\tTime 0.042 (0.033)\tLoss 1.0418 (1.0704)\tPrec@1 82.812 (79.657)\n",
            " * Prec@1 79.760\n",
            "\n",
            "Epoch: 55/100\n",
            "Learning rate: 0.043790\n",
            "Epoch: [54][0/391]\tTime 0.308 (0.308)\tData 0.223 (0.223)\tLoss 1.6278 (1.6278)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [54][50/391]\tTime 0.100 (0.119)\tData 0.010 (0.022)\tLoss 1.3903 (1.5110)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [54][100/391]\tTime 0.092 (0.110)\tData 0.000 (0.016)\tLoss 1.4606 (1.5381)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [54][150/391]\tTime 0.116 (0.110)\tData 0.030 (0.015)\tLoss 1.7221 (1.5314)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [54][200/391]\tTime 0.092 (0.109)\tData 0.000 (0.017)\tLoss 1.0462 (1.5348)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [54][250/391]\tTime 0.096 (0.108)\tData 0.000 (0.017)\tLoss 1.6042 (1.5245)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [54][300/391]\tTime 0.092 (0.110)\tData 0.000 (0.018)\tLoss 1.2614 (1.5127)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [54][350/391]\tTime 0.080 (0.108)\tData 0.000 (0.017)\tLoss 1.7347 (1.5165)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.156 (0.156)\tLoss 0.9813 (0.9813)\tPrec@1 81.250 (81.250)\n",
            "Test: [50/79]\tTime 0.025 (0.034)\tLoss 0.9941 (0.9901)\tPrec@1 80.469 (82.184)\n",
            " * Prec@1 82.370\n",
            "\n",
            "Epoch: 56/100\n",
            "Learning rate: 0.042236\n",
            "Epoch: [55][0/391]\tTime 0.331 (0.331)\tData 0.213 (0.213)\tLoss 1.6536 (1.6536)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [55][50/391]\tTime 0.080 (0.102)\tData 0.007 (0.010)\tLoss 1.2145 (1.5112)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [55][100/391]\tTime 0.095 (0.110)\tData 0.000 (0.015)\tLoss 1.7009 (1.5170)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [55][150/391]\tTime 0.107 (0.107)\tData 0.007 (0.014)\tLoss 1.7397 (1.4973)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [55][200/391]\tTime 0.100 (0.107)\tData 0.000 (0.014)\tLoss 1.0440 (1.4966)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [55][250/391]\tTime 0.109 (0.108)\tData 0.007 (0.015)\tLoss 1.6954 (1.5079)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [55][300/391]\tTime 0.092 (0.107)\tData 0.000 (0.016)\tLoss 1.6639 (1.5080)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [55][350/391]\tTime 0.092 (0.108)\tData 0.005 (0.017)\tLoss 1.8190 (1.5009)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.150 (0.150)\tLoss 1.0036 (1.0036)\tPrec@1 80.469 (80.469)\n",
            "Test: [50/79]\tTime 0.040 (0.033)\tLoss 1.0317 (1.0003)\tPrec@1 78.125 (81.158)\n",
            " * Prec@1 81.320\n",
            "\n",
            "Epoch: 57/100\n",
            "Learning rate: 0.040690\n",
            "Epoch: [56][0/391]\tTime 0.328 (0.328)\tData 0.213 (0.213)\tLoss 1.5268 (1.5268)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [56][50/391]\tTime 0.073 (0.120)\tData 0.000 (0.025)\tLoss 1.2089 (1.5550)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [56][100/391]\tTime 0.099 (0.111)\tData 0.003 (0.019)\tLoss 1.4528 (1.5154)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [56][150/391]\tTime 0.084 (0.113)\tData 0.000 (0.020)\tLoss 1.6010 (1.5250)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [56][200/391]\tTime 0.109 (0.110)\tData 0.005 (0.017)\tLoss 1.7719 (1.5130)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [56][250/391]\tTime 0.151 (0.108)\tData 0.060 (0.015)\tLoss 1.3116 (1.5226)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [56][300/391]\tTime 0.093 (0.109)\tData 0.000 (0.016)\tLoss 1.7339 (1.5193)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [56][350/391]\tTime 0.095 (0.108)\tData 0.000 (0.015)\tLoss 1.8936 (1.5080)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.159 (0.159)\tLoss 1.0029 (1.0029)\tPrec@1 82.031 (82.031)\n",
            "Test: [50/79]\tTime 0.036 (0.033)\tLoss 0.9469 (0.9808)\tPrec@1 82.031 (82.874)\n",
            " * Prec@1 83.290\n",
            "\n",
            "Epoch: 58/100\n",
            "Learning rate: 0.039154\n",
            "Epoch: [57][0/391]\tTime 0.345 (0.345)\tData 0.227 (0.227)\tLoss 1.7026 (1.7026)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [57][50/391]\tTime 0.102 (0.103)\tData 0.011 (0.012)\tLoss 1.6757 (1.5434)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [57][100/391]\tTime 0.095 (0.110)\tData 0.001 (0.017)\tLoss 1.6843 (1.5351)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [57][150/391]\tTime 0.139 (0.106)\tData 0.049 (0.015)\tLoss 1.8104 (1.5164)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [57][200/391]\tTime 0.099 (0.109)\tData 0.000 (0.018)\tLoss 1.6572 (1.5156)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [57][250/391]\tTime 0.073 (0.108)\tData 0.000 (0.017)\tLoss 1.7939 (1.5187)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [57][300/391]\tTime 0.111 (0.108)\tData 0.000 (0.018)\tLoss 0.9761 (1.5141)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [57][350/391]\tTime 0.077 (0.108)\tData 0.000 (0.019)\tLoss 1.5660 (1.5124)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.155 (0.155)\tLoss 0.9573 (0.9573)\tPrec@1 87.500 (87.500)\n",
            "Test: [50/79]\tTime 0.022 (0.033)\tLoss 0.9726 (0.9631)\tPrec@1 84.375 (84.727)\n",
            " * Prec@1 84.820\n",
            "\n",
            "Epoch: 59/100\n",
            "Learning rate: 0.037628\n",
            "Epoch: [58][0/391]\tTime 0.537 (0.537)\tData 0.445 (0.445)\tLoss 1.7540 (1.7540)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [58][50/391]\tTime 0.105 (0.114)\tData 0.016 (0.018)\tLoss 1.7458 (1.5057)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [58][100/391]\tTime 0.108 (0.106)\tData 0.006 (0.013)\tLoss 1.3429 (1.4771)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [58][150/391]\tTime 0.078 (0.110)\tData 0.007 (0.017)\tLoss 1.5447 (1.4971)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [58][200/391]\tTime 0.119 (0.108)\tData 0.033 (0.017)\tLoss 1.0387 (1.5059)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [58][250/391]\tTime 0.133 (0.110)\tData 0.065 (0.020)\tLoss 1.4659 (1.5002)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [58][300/391]\tTime 0.095 (0.108)\tData 0.000 (0.019)\tLoss 1.2601 (1.4929)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [58][350/391]\tTime 0.105 (0.108)\tData 0.000 (0.018)\tLoss 1.0704 (1.4842)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.159 (0.159)\tLoss 0.8978 (0.8978)\tPrec@1 87.500 (87.500)\n",
            "Test: [50/79]\tTime 0.017 (0.033)\tLoss 0.9427 (0.9090)\tPrec@1 83.594 (85.509)\n",
            " * Prec@1 85.770\n",
            "\n",
            "Epoch: 60/100\n",
            "Learning rate: 0.036114\n",
            "Epoch: [59][0/391]\tTime 0.335 (0.335)\tData 0.221 (0.221)\tLoss 1.2275 (1.2275)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [59][50/391]\tTime 0.171 (0.112)\tData 0.089 (0.018)\tLoss 1.7147 (1.5115)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [59][100/391]\tTime 0.110 (0.111)\tData 0.008 (0.018)\tLoss 1.0320 (1.5267)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [59][150/391]\tTime 0.129 (0.108)\tData 0.037 (0.015)\tLoss 1.7245 (1.5232)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [59][200/391]\tTime 0.122 (0.111)\tData 0.036 (0.018)\tLoss 1.3309 (1.5229)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [59][250/391]\tTime 0.095 (0.109)\tData 0.000 (0.018)\tLoss 1.6710 (1.5135)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [59][300/391]\tTime 0.122 (0.110)\tData 0.030 (0.018)\tLoss 1.6727 (1.5165)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [59][350/391]\tTime 0.078 (0.109)\tData 0.000 (0.018)\tLoss 1.1352 (1.5185)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.153 (0.153)\tLoss 0.9602 (0.9602)\tPrec@1 84.375 (84.375)\n",
            "Test: [50/79]\tTime 0.033 (0.047)\tLoss 0.9432 (0.9526)\tPrec@1 83.594 (84.329)\n",
            " * Prec@1 84.510\n",
            "\n",
            "Epoch: 61/100\n",
            "Learning rate: 0.034615\n",
            "Epoch: [60][0/391]\tTime 0.329 (0.329)\tData 0.217 (0.217)\tLoss 1.0841 (1.0841)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [60][50/391]\tTime 0.097 (0.103)\tData 0.004 (0.012)\tLoss 1.8244 (1.4820)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [60][100/391]\tTime 0.102 (0.104)\tData 0.015 (0.013)\tLoss 1.6706 (1.4919)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [60][150/391]\tTime 0.090 (0.108)\tData 0.000 (0.017)\tLoss 1.5367 (1.4839)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [60][200/391]\tTime 0.093 (0.106)\tData 0.000 (0.017)\tLoss 1.0701 (1.4801)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [60][250/391]\tTime 0.101 (0.108)\tData 0.000 (0.018)\tLoss 1.1204 (1.4765)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [60][300/391]\tTime 0.077 (0.107)\tData 0.000 (0.018)\tLoss 1.6392 (1.4831)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [60][350/391]\tTime 0.092 (0.109)\tData 0.000 (0.019)\tLoss 1.5609 (1.4758)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.146 (0.146)\tLoss 0.9529 (0.9529)\tPrec@1 84.375 (84.375)\n",
            "Test: [50/79]\tTime 0.015 (0.033)\tLoss 1.0097 (0.9879)\tPrec@1 77.344 (82.675)\n",
            " * Prec@1 82.770\n",
            "\n",
            "Epoch: 62/100\n",
            "Learning rate: 0.033130\n",
            "Epoch: [61][0/391]\tTime 0.327 (0.327)\tData 0.211 (0.211)\tLoss 1.7362 (1.7362)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [61][50/391]\tTime 0.151 (0.122)\tData 0.060 (0.022)\tLoss 1.0743 (1.4381)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [61][100/391]\tTime 0.100 (0.111)\tData 0.027 (0.018)\tLoss 1.6968 (1.4448)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [61][150/391]\tTime 0.241 (0.110)\tData 0.151 (0.018)\tLoss 1.1235 (1.4716)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [61][200/391]\tTime 0.104 (0.110)\tData 0.000 (0.018)\tLoss 1.8470 (1.4666)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [61][250/391]\tTime 0.096 (0.108)\tData 0.000 (0.018)\tLoss 1.1455 (1.4662)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [61][300/391]\tTime 0.086 (0.110)\tData 0.000 (0.019)\tLoss 1.3167 (1.4674)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [61][350/391]\tTime 0.095 (0.109)\tData 0.000 (0.018)\tLoss 0.9619 (1.4730)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.191 (0.191)\tLoss 0.9693 (0.9693)\tPrec@1 85.938 (85.938)\n",
            "Test: [50/79]\tTime 0.043 (0.040)\tLoss 0.9556 (0.9654)\tPrec@1 84.375 (83.961)\n",
            " * Prec@1 84.330\n",
            "\n",
            "Epoch: 63/100\n",
            "Learning rate: 0.031662\n",
            "Epoch: [62][0/391]\tTime 0.326 (0.326)\tData 0.212 (0.212)\tLoss 1.6944 (1.6944)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [62][50/391]\tTime 0.134 (0.103)\tData 0.045 (0.013)\tLoss 1.0932 (1.4517)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [62][100/391]\tTime 0.077 (0.109)\tData 0.000 (0.017)\tLoss 1.0916 (1.4698)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [62][150/391]\tTime 0.075 (0.106)\tData 0.000 (0.015)\tLoss 1.3473 (1.4474)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [62][200/391]\tTime 0.131 (0.105)\tData 0.038 (0.015)\tLoss 1.6455 (1.4548)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [62][250/391]\tTime 0.099 (0.107)\tData 0.000 (0.016)\tLoss 1.7194 (1.4709)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [62][300/391]\tTime 0.129 (0.106)\tData 0.056 (0.018)\tLoss 1.7007 (1.4718)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [62][350/391]\tTime 0.106 (0.107)\tData 0.005 (0.019)\tLoss 1.8216 (1.4705)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.148 (0.148)\tLoss 0.9678 (0.9678)\tPrec@1 83.594 (83.594)\n",
            "Test: [50/79]\tTime 0.016 (0.035)\tLoss 0.9534 (0.9410)\tPrec@1 79.688 (84.620)\n",
            " * Prec@1 84.590\n",
            "\n",
            "Epoch: 64/100\n",
            "Learning rate: 0.030212\n",
            "Epoch: [63][0/391]\tTime 0.327 (0.327)\tData 0.212 (0.212)\tLoss 1.6458 (1.6458)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [63][50/391]\tTime 0.092 (0.120)\tData 0.007 (0.023)\tLoss 1.2667 (1.4959)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [63][100/391]\tTime 0.151 (0.111)\tData 0.066 (0.020)\tLoss 1.6786 (1.4893)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [63][150/391]\tTime 0.130 (0.113)\tData 0.059 (0.022)\tLoss 1.1315 (1.4946)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [63][200/391]\tTime 0.129 (0.110)\tData 0.059 (0.021)\tLoss 1.3897 (1.4851)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [63][250/391]\tTime 0.101 (0.110)\tData 0.000 (0.022)\tLoss 1.5265 (1.4789)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [63][300/391]\tTime 0.095 (0.110)\tData 0.006 (0.021)\tLoss 1.6718 (1.4733)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [63][350/391]\tTime 0.106 (0.109)\tData 0.006 (0.020)\tLoss 1.4309 (1.4741)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.150 (0.150)\tLoss 0.9027 (0.9027)\tPrec@1 89.062 (89.062)\n",
            "Test: [50/79]\tTime 0.032 (0.034)\tLoss 0.8910 (0.8972)\tPrec@1 85.156 (86.520)\n",
            " * Prec@1 86.730\n",
            "\n",
            "Epoch: 65/100\n",
            "Learning rate: 0.028782\n",
            "Epoch: [64][0/391]\tTime 0.304 (0.304)\tData 0.217 (0.217)\tLoss 1.3687 (1.3687)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [64][50/391]\tTime 0.078 (0.104)\tData 0.005 (0.013)\tLoss 1.3095 (1.4967)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [64][100/391]\tTime 0.089 (0.110)\tData 0.000 (0.016)\tLoss 1.6609 (1.4925)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [64][150/391]\tTime 0.111 (0.108)\tData 0.015 (0.015)\tLoss 1.6523 (1.4813)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [64][200/391]\tTime 0.099 (0.110)\tData 0.025 (0.017)\tLoss 1.2384 (1.4711)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [64][250/391]\tTime 0.095 (0.108)\tData 0.008 (0.016)\tLoss 1.4070 (1.4671)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [64][300/391]\tTime 0.102 (0.107)\tData 0.000 (0.016)\tLoss 1.7988 (1.4699)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [64][350/391]\tTime 0.075 (0.108)\tData 0.000 (0.017)\tLoss 0.9794 (1.4675)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.151 (0.151)\tLoss 0.9035 (0.9035)\tPrec@1 85.156 (85.156)\n",
            "Test: [50/79]\tTime 0.041 (0.035)\tLoss 0.9032 (0.9032)\tPrec@1 86.719 (86.198)\n",
            " * Prec@1 86.290\n",
            "\n",
            "Epoch: 66/100\n",
            "Learning rate: 0.027373\n",
            "Epoch: [65][0/391]\tTime 0.511 (0.511)\tData 0.346 (0.346)\tLoss 1.1284 (1.1284)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [65][50/391]\tTime 0.149 (0.117)\tData 0.052 (0.020)\tLoss 1.6620 (1.5315)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [65][100/391]\tTime 0.091 (0.108)\tData 0.000 (0.016)\tLoss 1.7199 (1.5122)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [65][150/391]\tTime 0.106 (0.110)\tData 0.016 (0.018)\tLoss 1.6980 (1.4690)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [65][200/391]\tTime 0.108 (0.108)\tData 0.023 (0.016)\tLoss 1.2464 (1.4696)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [65][250/391]\tTime 0.094 (0.110)\tData 0.006 (0.017)\tLoss 1.3394 (1.4687)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [65][300/391]\tTime 0.073 (0.108)\tData 0.000 (0.016)\tLoss 1.7658 (1.4557)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [65][350/391]\tTime 0.099 (0.107)\tData 0.000 (0.016)\tLoss 1.4186 (1.4559)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.147 (0.147)\tLoss 0.8962 (0.8962)\tPrec@1 88.281 (88.281)\n",
            "Test: [50/79]\tTime 0.030 (0.033)\tLoss 0.8904 (0.8963)\tPrec@1 85.938 (87.454)\n",
            " * Prec@1 87.500\n",
            "\n",
            "Epoch: 67/100\n",
            "Learning rate: 0.025986\n",
            "Epoch: [66][0/391]\tTime 0.327 (0.327)\tData 0.211 (0.211)\tLoss 1.4113 (1.4113)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [66][50/391]\tTime 0.211 (0.106)\tData 0.100 (0.015)\tLoss 1.3514 (1.4477)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [66][100/391]\tTime 0.098 (0.110)\tData 0.004 (0.020)\tLoss 1.6894 (1.4079)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [66][150/391]\tTime 0.076 (0.108)\tData 0.000 (0.020)\tLoss 1.0744 (1.4276)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [66][200/391]\tTime 0.110 (0.110)\tData 0.020 (0.022)\tLoss 1.3300 (1.4273)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [66][250/391]\tTime 0.088 (0.109)\tData 0.000 (0.022)\tLoss 1.6755 (1.4335)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [66][300/391]\tTime 0.117 (0.111)\tData 0.030 (0.023)\tLoss 1.5195 (1.4437)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [66][350/391]\tTime 0.114 (0.109)\tData 0.026 (0.021)\tLoss 1.5042 (1.4452)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.158 (0.158)\tLoss 0.8835 (0.8835)\tPrec@1 91.406 (91.406)\n",
            "Test: [50/79]\tTime 0.054 (0.047)\tLoss 0.9073 (0.8999)\tPrec@1 84.375 (86.688)\n",
            " * Prec@1 86.770\n",
            "\n",
            "Epoch: 68/100\n",
            "Learning rate: 0.024623\n",
            "Epoch: [67][0/391]\tTime 0.330 (0.330)\tData 0.212 (0.212)\tLoss 1.6535 (1.6535)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [67][50/391]\tTime 0.116 (0.104)\tData 0.025 (0.010)\tLoss 1.0633 (1.3276)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [67][100/391]\tTime 0.107 (0.105)\tData 0.010 (0.010)\tLoss 1.5933 (1.3926)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [67][150/391]\tTime 0.116 (0.107)\tData 0.020 (0.014)\tLoss 1.7449 (1.4281)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [67][200/391]\tTime 0.129 (0.105)\tData 0.058 (0.014)\tLoss 1.7272 (1.4355)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [67][250/391]\tTime 0.094 (0.108)\tData 0.000 (0.015)\tLoss 1.6382 (1.4477)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [67][300/391]\tTime 0.109 (0.106)\tData 0.020 (0.015)\tLoss 1.6629 (1.4524)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [67][350/391]\tTime 0.110 (0.108)\tData 0.017 (0.015)\tLoss 1.1082 (1.4411)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.178 (0.178)\tLoss 0.9418 (0.9418)\tPrec@1 87.500 (87.500)\n",
            "Test: [50/79]\tTime 0.016 (0.033)\tLoss 0.9516 (0.9469)\tPrec@1 84.375 (86.106)\n",
            " * Prec@1 85.980\n",
            "\n",
            "Epoch: 69/100\n",
            "Learning rate: 0.023285\n",
            "Epoch: [68][0/391]\tTime 0.321 (0.321)\tData 0.209 (0.209)\tLoss 1.7996 (1.7996)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [68][50/391]\tTime 0.073 (0.120)\tData 0.000 (0.018)\tLoss 1.7176 (1.4292)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [68][100/391]\tTime 0.101 (0.112)\tData 0.004 (0.015)\tLoss 1.8169 (1.4169)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [68][150/391]\tTime 0.126 (0.109)\tData 0.000 (0.014)\tLoss 1.5422 (1.4332)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [68][200/391]\tTime 0.157 (0.111)\tData 0.068 (0.017)\tLoss 1.6723 (1.4256)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [68][250/391]\tTime 0.098 (0.109)\tData 0.005 (0.017)\tLoss 1.0215 (1.4385)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [68][300/391]\tTime 0.130 (0.110)\tData 0.061 (0.018)\tLoss 1.7635 (1.4463)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [68][350/391]\tTime 0.111 (0.109)\tData 0.040 (0.018)\tLoss 1.5620 (1.4499)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.201 (0.201)\tLoss 0.9047 (0.9047)\tPrec@1 87.500 (87.500)\n",
            "Test: [50/79]\tTime 0.035 (0.041)\tLoss 0.9059 (0.8848)\tPrec@1 85.156 (87.025)\n",
            " * Prec@1 87.380\n",
            "\n",
            "Epoch: 70/100\n",
            "Learning rate: 0.021974\n",
            "Epoch: [69][0/391]\tTime 0.328 (0.328)\tData 0.213 (0.213)\tLoss 0.9086 (0.9086)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [69][50/391]\tTime 0.119 (0.102)\tData 0.036 (0.010)\tLoss 1.6217 (1.3849)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [69][100/391]\tTime 0.191 (0.111)\tData 0.078 (0.018)\tLoss 1.0019 (1.4086)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [69][150/391]\tTime 0.122 (0.107)\tData 0.035 (0.016)\tLoss 1.2600 (1.4398)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [69][200/391]\tTime 0.121 (0.107)\tData 0.022 (0.016)\tLoss 1.6949 (1.4400)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [69][250/391]\tTime 0.100 (0.108)\tData 0.009 (0.018)\tLoss 0.8276 (1.4345)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [69][300/391]\tTime 0.100 (0.107)\tData 0.006 (0.017)\tLoss 1.2949 (1.4382)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [69][350/391]\tTime 0.092 (0.108)\tData 0.000 (0.017)\tLoss 1.2891 (1.4365)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.150 (0.150)\tLoss 0.8733 (0.8733)\tPrec@1 87.500 (87.500)\n",
            "Test: [50/79]\tTime 0.045 (0.033)\tLoss 0.8825 (0.8772)\tPrec@1 87.500 (87.040)\n",
            " * Prec@1 87.590\n",
            "\n",
            "Epoch: 71/100\n",
            "Learning rate: 0.020690\n",
            "Epoch: [70][0/391]\tTime 0.316 (0.316)\tData 0.205 (0.205)\tLoss 1.7277 (1.7277)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [70][50/391]\tTime 0.146 (0.121)\tData 0.056 (0.030)\tLoss 1.4682 (1.4874)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [70][100/391]\tTime 0.145 (0.111)\tData 0.045 (0.023)\tLoss 1.6570 (1.4746)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [70][150/391]\tTime 0.081 (0.113)\tData 0.004 (0.025)\tLoss 1.8068 (1.4704)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [70][200/391]\tTime 0.155 (0.111)\tData 0.064 (0.024)\tLoss 1.7042 (1.4684)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [70][250/391]\tTime 0.192 (0.111)\tData 0.105 (0.025)\tLoss 1.0270 (1.4614)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [70][300/391]\tTime 0.104 (0.110)\tData 0.018 (0.023)\tLoss 0.9974 (1.4627)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [70][350/391]\tTime 0.093 (0.108)\tData 0.000 (0.021)\tLoss 1.6331 (1.4719)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.145 (0.145)\tLoss 0.8666 (0.8666)\tPrec@1 91.406 (91.406)\n",
            "Test: [50/79]\tTime 0.047 (0.033)\tLoss 0.9314 (0.9002)\tPrec@1 85.156 (87.776)\n",
            " * Prec@1 87.910\n",
            "\n",
            "Epoch: 72/100\n",
            "Learning rate: 0.019435\n",
            "Epoch: [71][0/391]\tTime 0.322 (0.322)\tData 0.212 (0.212)\tLoss 1.7250 (1.7250)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [71][50/391]\tTime 0.075 (0.101)\tData 0.000 (0.011)\tLoss 1.2222 (1.3675)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [71][100/391]\tTime 0.091 (0.109)\tData 0.000 (0.016)\tLoss 1.1357 (1.4442)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [71][150/391]\tTime 0.132 (0.105)\tData 0.045 (0.014)\tLoss 1.6445 (1.4251)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [71][200/391]\tTime 0.083 (0.108)\tData 0.000 (0.017)\tLoss 0.9322 (1.4444)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [71][250/391]\tTime 0.097 (0.107)\tData 0.004 (0.017)\tLoss 1.0607 (1.4332)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [71][300/391]\tTime 0.143 (0.106)\tData 0.050 (0.016)\tLoss 1.4822 (1.4359)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [71][350/391]\tTime 0.096 (0.107)\tData 0.000 (0.017)\tLoss 0.9280 (1.4292)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.148 (0.148)\tLoss 0.8405 (0.8405)\tPrec@1 92.188 (92.188)\n",
            "Test: [50/79]\tTime 0.036 (0.033)\tLoss 0.8846 (0.8589)\tPrec@1 86.719 (89.078)\n",
            " * Prec@1 88.950\n",
            "\n",
            "Epoch: 73/100\n",
            "Learning rate: 0.018211\n",
            "Epoch: [72][0/391]\tTime 0.594 (0.594)\tData 0.342 (0.342)\tLoss 1.0237 (1.0237)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [72][50/391]\tTime 0.081 (0.117)\tData 0.000 (0.022)\tLoss 1.0408 (1.3893)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [72][100/391]\tTime 0.093 (0.108)\tData 0.000 (0.016)\tLoss 0.9984 (1.3584)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [72][150/391]\tTime 0.083 (0.111)\tData 0.000 (0.018)\tLoss 1.2431 (1.3954)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [72][200/391]\tTime 0.096 (0.109)\tData 0.004 (0.017)\tLoss 1.2507 (1.4001)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [72][250/391]\tTime 0.101 (0.110)\tData 0.004 (0.018)\tLoss 1.8095 (1.4081)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [72][300/391]\tTime 0.097 (0.109)\tData 0.000 (0.018)\tLoss 1.6321 (1.4181)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [72][350/391]\tTime 0.152 (0.108)\tData 0.061 (0.018)\tLoss 1.7049 (1.4129)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.157 (0.157)\tLoss 0.8839 (0.8839)\tPrec@1 89.062 (89.062)\n",
            "Test: [50/79]\tTime 0.025 (0.034)\tLoss 0.8607 (0.8737)\tPrec@1 90.625 (88.725)\n",
            " * Prec@1 88.710\n",
            "\n",
            "Epoch: 74/100\n",
            "Learning rate: 0.017017\n",
            "Epoch: [73][0/391]\tTime 0.331 (0.331)\tData 0.214 (0.214)\tLoss 1.1820 (1.1820)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [73][50/391]\tTime 0.111 (0.107)\tData 0.000 (0.016)\tLoss 1.0123 (1.4221)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [73][100/391]\tTime 0.104 (0.111)\tData 0.001 (0.018)\tLoss 1.6850 (1.4375)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [73][150/391]\tTime 0.097 (0.106)\tData 0.000 (0.013)\tLoss 1.3809 (1.4281)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [73][200/391]\tTime 0.097 (0.109)\tData 0.000 (0.015)\tLoss 1.2710 (1.4233)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [73][250/391]\tTime 0.073 (0.107)\tData 0.000 (0.015)\tLoss 1.7752 (1.4213)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [73][300/391]\tTime 0.265 (0.109)\tData 0.153 (0.016)\tLoss 1.6463 (1.4260)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [73][350/391]\tTime 0.115 (0.108)\tData 0.020 (0.016)\tLoss 0.9610 (1.4125)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.154 (0.154)\tLoss 0.8516 (0.8516)\tPrec@1 91.406 (91.406)\n",
            "Test: [50/79]\tTime 0.047 (0.039)\tLoss 0.8934 (0.8721)\tPrec@1 88.281 (88.036)\n",
            " * Prec@1 88.030\n",
            "\n",
            "Epoch: 75/100\n",
            "Learning rate: 0.015857\n",
            "Epoch: [74][0/391]\tTime 0.418 (0.418)\tData 0.281 (0.281)\tLoss 1.4580 (1.4580)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [74][50/391]\tTime 0.104 (0.104)\tData 0.000 (0.015)\tLoss 1.5966 (1.4619)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [74][100/391]\tTime 0.112 (0.101)\tData 0.039 (0.012)\tLoss 1.5615 (1.4183)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [74][150/391]\tTime 0.231 (0.107)\tData 0.111 (0.018)\tLoss 1.3957 (1.4181)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [74][200/391]\tTime 0.098 (0.105)\tData 0.007 (0.016)\tLoss 1.0320 (1.4243)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [74][250/391]\tTime 0.113 (0.108)\tData 0.030 (0.018)\tLoss 1.4323 (1.4320)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [74][300/391]\tTime 0.077 (0.106)\tData 0.004 (0.018)\tLoss 1.6424 (1.4264)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [74][350/391]\tTime 0.103 (0.106)\tData 0.000 (0.017)\tLoss 1.6082 (1.4198)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.149 (0.149)\tLoss 0.9350 (0.9350)\tPrec@1 86.719 (86.719)\n",
            "Test: [50/79]\tTime 0.032 (0.033)\tLoss 0.9335 (0.9105)\tPrec@1 84.375 (86.305)\n",
            " * Prec@1 86.350\n",
            "\n",
            "Epoch: 76/100\n",
            "Learning rate: 0.014730\n",
            "Epoch: [75][0/391]\tTime 0.332 (0.332)\tData 0.214 (0.214)\tLoss 1.5058 (1.5058)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [75][50/391]\tTime 0.283 (0.117)\tData 0.138 (0.020)\tLoss 1.3067 (1.4799)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [75][100/391]\tTime 0.080 (0.110)\tData 0.005 (0.017)\tLoss 1.6609 (1.4253)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [75][150/391]\tTime 0.093 (0.106)\tData 0.008 (0.016)\tLoss 1.4297 (1.4277)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [75][200/391]\tTime 0.094 (0.109)\tData 0.000 (0.018)\tLoss 1.3106 (1.4390)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [75][250/391]\tTime 0.096 (0.106)\tData 0.000 (0.016)\tLoss 1.2346 (1.4317)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [75][300/391]\tTime 0.073 (0.108)\tData 0.000 (0.017)\tLoss 1.1050 (1.4293)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [75][350/391]\tTime 0.099 (0.107)\tData 0.008 (0.017)\tLoss 1.0682 (1.4294)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.148 (0.148)\tLoss 0.7876 (0.7876)\tPrec@1 92.969 (92.969)\n",
            "Test: [50/79]\tTime 0.050 (0.046)\tLoss 0.8706 (0.8395)\tPrec@1 85.938 (89.813)\n",
            " * Prec@1 90.040\n",
            "\n",
            "Epoch: 77/100\n",
            "Learning rate: 0.013638\n",
            "Epoch: [76][0/391]\tTime 0.333 (0.333)\tData 0.224 (0.224)\tLoss 1.4488 (1.4488)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [76][50/391]\tTime 0.099 (0.101)\tData 0.005 (0.008)\tLoss 1.6542 (1.3622)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [76][100/391]\tTime 0.236 (0.102)\tData 0.153 (0.009)\tLoss 1.6273 (1.3919)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [76][150/391]\tTime 0.091 (0.105)\tData 0.000 (0.013)\tLoss 1.0126 (1.3667)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [76][200/391]\tTime 0.092 (0.103)\tData 0.003 (0.010)\tLoss 1.3080 (1.3842)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [76][250/391]\tTime 0.101 (0.105)\tData 0.000 (0.012)\tLoss 0.9347 (1.3870)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [76][300/391]\tTime 0.092 (0.104)\tData 0.000 (0.012)\tLoss 1.5773 (1.3970)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [76][350/391]\tTime 0.095 (0.106)\tData 0.000 (0.014)\tLoss 1.6129 (1.4070)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.150 (0.150)\tLoss 0.8419 (0.8419)\tPrec@1 91.406 (91.406)\n",
            "Test: [50/79]\tTime 0.049 (0.034)\tLoss 0.8777 (0.8673)\tPrec@1 86.719 (88.971)\n",
            " * Prec@1 89.290\n",
            "\n",
            "Epoch: 78/100\n",
            "Learning rate: 0.012582\n",
            "Epoch: [77][0/391]\tTime 0.322 (0.322)\tData 0.209 (0.209)\tLoss 1.0474 (1.0474)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [77][50/391]\tTime 0.097 (0.118)\tData 0.000 (0.023)\tLoss 1.1960 (1.4157)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [77][100/391]\tTime 0.102 (0.108)\tData 0.008 (0.015)\tLoss 1.3052 (1.3880)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [77][150/391]\tTime 0.098 (0.105)\tData 0.000 (0.013)\tLoss 1.2711 (1.3926)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [77][200/391]\tTime 0.101 (0.108)\tData 0.028 (0.017)\tLoss 1.5671 (1.4052)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [77][250/391]\tTime 0.097 (0.107)\tData 0.000 (0.018)\tLoss 1.0219 (1.4075)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [77][300/391]\tTime 0.090 (0.109)\tData 0.005 (0.018)\tLoss 0.9764 (1.4058)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [77][350/391]\tTime 0.074 (0.107)\tData 0.000 (0.017)\tLoss 1.6667 (1.4056)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.265 (0.265)\tLoss 0.8331 (0.8331)\tPrec@1 92.188 (92.188)\n",
            "Test: [50/79]\tTime 0.015 (0.050)\tLoss 0.8637 (0.8613)\tPrec@1 89.844 (89.522)\n",
            " * Prec@1 89.780\n",
            "\n",
            "Epoch: 79/100\n",
            "Learning rate: 0.011563\n",
            "Epoch: [78][0/391]\tTime 0.330 (0.330)\tData 0.219 (0.219)\tLoss 1.0044 (1.0044)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [78][50/391]\tTime 0.095 (0.104)\tData 0.004 (0.016)\tLoss 1.7032 (1.3376)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [78][100/391]\tTime 0.225 (0.108)\tData 0.123 (0.017)\tLoss 1.6182 (1.3613)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [78][150/391]\tTime 0.092 (0.107)\tData 0.005 (0.015)\tLoss 1.4868 (1.3854)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [78][200/391]\tTime 0.084 (0.105)\tData 0.002 (0.013)\tLoss 1.6247 (1.3959)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [78][250/391]\tTime 0.128 (0.107)\tData 0.037 (0.016)\tLoss 1.0526 (1.3982)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [78][300/391]\tTime 0.103 (0.106)\tData 0.005 (0.016)\tLoss 1.6897 (1.4048)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [78][350/391]\tTime 0.152 (0.108)\tData 0.056 (0.017)\tLoss 1.4998 (1.4028)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.145 (0.145)\tLoss 0.8043 (0.8043)\tPrec@1 92.188 (92.188)\n",
            "Test: [50/79]\tTime 0.022 (0.033)\tLoss 0.7882 (0.8324)\tPrec@1 91.406 (90.150)\n",
            " * Prec@1 90.230\n",
            "\n",
            "Epoch: 80/100\n",
            "Learning rate: 0.010582\n",
            "Epoch: [79][0/391]\tTime 0.333 (0.333)\tData 0.219 (0.219)\tLoss 1.6162 (1.6162)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [79][50/391]\tTime 0.110 (0.121)\tData 0.019 (0.022)\tLoss 1.7094 (1.3511)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [79][100/391]\tTime 0.098 (0.110)\tData 0.004 (0.017)\tLoss 1.5720 (1.3637)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [79][150/391]\tTime 0.113 (0.109)\tData 0.011 (0.015)\tLoss 1.2163 (1.3701)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [79][200/391]\tTime 0.100 (0.109)\tData 0.006 (0.015)\tLoss 1.1394 (1.3673)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [79][250/391]\tTime 0.115 (0.107)\tData 0.007 (0.015)\tLoss 1.3802 (1.3753)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [79][300/391]\tTime 0.073 (0.109)\tData 0.000 (0.017)\tLoss 1.0527 (1.3696)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [79][350/391]\tTime 0.101 (0.108)\tData 0.000 (0.016)\tLoss 1.4747 (1.3718)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.223 (0.223)\tLoss 0.8183 (0.8183)\tPrec@1 89.844 (89.844)\n",
            "Test: [50/79]\tTime 0.015 (0.034)\tLoss 0.8213 (0.8465)\tPrec@1 89.844 (89.798)\n",
            " * Prec@1 90.020\n",
            "\n",
            "Epoch: 81/100\n",
            "Learning rate: 0.009640\n",
            "Epoch: [80][0/391]\tTime 0.328 (0.328)\tData 0.213 (0.213)\tLoss 1.7608 (1.7608)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [80][50/391]\tTime 0.094 (0.102)\tData 0.000 (0.009)\tLoss 1.1236 (1.4071)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [80][100/391]\tTime 0.104 (0.109)\tData 0.010 (0.016)\tLoss 1.3949 (1.4071)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [80][150/391]\tTime 0.096 (0.107)\tData 0.000 (0.017)\tLoss 1.0984 (1.3973)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [80][200/391]\tTime 0.086 (0.106)\tData 0.000 (0.016)\tLoss 1.2566 (1.3846)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [80][250/391]\tTime 0.102 (0.107)\tData 0.009 (0.017)\tLoss 1.6114 (1.3871)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [80][300/391]\tTime 0.093 (0.105)\tData 0.000 (0.015)\tLoss 1.5670 (1.3955)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [80][350/391]\tTime 0.089 (0.107)\tData 0.000 (0.015)\tLoss 1.6974 (1.4007)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.166 (0.166)\tLoss 0.7978 (0.7978)\tPrec@1 90.625 (90.625)\n",
            "Test: [50/79]\tTime 0.047 (0.033)\tLoss 0.7984 (0.8221)\tPrec@1 92.188 (90.794)\n",
            " * Prec@1 90.860\n",
            "\n",
            "Epoch: 82/100\n",
            "Learning rate: 0.008737\n",
            "Epoch: [81][0/391]\tTime 0.328 (0.328)\tData 0.215 (0.215)\tLoss 1.6135 (1.6135)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [81][50/391]\tTime 0.096 (0.119)\tData 0.007 (0.024)\tLoss 1.3181 (1.2929)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [81][100/391]\tTime 0.094 (0.109)\tData 0.000 (0.016)\tLoss 1.6557 (1.3379)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [81][150/391]\tTime 0.130 (0.111)\tData 0.041 (0.019)\tLoss 0.9034 (1.3579)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [81][200/391]\tTime 0.102 (0.108)\tData 0.010 (0.018)\tLoss 1.3087 (1.3556)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [81][250/391]\tTime 0.131 (0.106)\tData 0.020 (0.015)\tLoss 0.9297 (1.3575)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [81][300/391]\tTime 0.118 (0.109)\tData 0.027 (0.018)\tLoss 1.2100 (1.3644)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [81][350/391]\tTime 0.100 (0.107)\tData 0.000 (0.017)\tLoss 1.5268 (1.3700)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.150 (0.150)\tLoss 0.8162 (0.8162)\tPrec@1 92.188 (92.188)\n",
            "Test: [50/79]\tTime 0.035 (0.033)\tLoss 0.8139 (0.8369)\tPrec@1 92.188 (90.993)\n",
            " * Prec@1 91.050\n",
            "\n",
            "Epoch: 83/100\n",
            "Learning rate: 0.007876\n",
            "Epoch: [82][0/391]\tTime 0.353 (0.353)\tData 0.243 (0.243)\tLoss 0.9833 (0.9833)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [82][50/391]\tTime 0.074 (0.100)\tData 0.000 (0.011)\tLoss 1.6214 (1.4205)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [82][100/391]\tTime 0.104 (0.109)\tData 0.002 (0.018)\tLoss 1.6673 (1.4365)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [82][150/391]\tTime 0.091 (0.106)\tData 0.000 (0.015)\tLoss 0.9400 (1.4377)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [82][200/391]\tTime 0.074 (0.109)\tData 0.000 (0.018)\tLoss 1.4057 (1.4415)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [82][250/391]\tTime 0.112 (0.107)\tData 0.042 (0.016)\tLoss 1.0781 (1.4291)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [82][300/391]\tTime 0.120 (0.106)\tData 0.016 (0.016)\tLoss 0.9284 (1.4204)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [82][350/391]\tTime 0.101 (0.108)\tData 0.003 (0.016)\tLoss 0.8889 (1.4118)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.152 (0.152)\tLoss 0.7767 (0.7767)\tPrec@1 92.188 (92.188)\n",
            "Test: [50/79]\tTime 0.022 (0.034)\tLoss 0.8008 (0.8210)\tPrec@1 92.188 (90.962)\n",
            " * Prec@1 91.150\n",
            "\n",
            "Epoch: 84/100\n",
            "Learning rate: 0.007056\n",
            "Epoch: [83][0/391]\tTime 0.577 (0.577)\tData 0.415 (0.415)\tLoss 1.6851 (1.6851)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [83][50/391]\tTime 0.090 (0.121)\tData 0.003 (0.025)\tLoss 1.5937 (1.3443)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [83][100/391]\tTime 0.108 (0.111)\tData 0.005 (0.019)\tLoss 1.6225 (1.3956)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [83][150/391]\tTime 0.073 (0.113)\tData 0.000 (0.021)\tLoss 1.3676 (1.3851)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [83][200/391]\tTime 0.107 (0.109)\tData 0.025 (0.018)\tLoss 1.2958 (1.3797)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [83][250/391]\tTime 0.218 (0.111)\tData 0.130 (0.020)\tLoss 1.6409 (1.3861)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [83][300/391]\tTime 0.073 (0.110)\tData 0.000 (0.019)\tLoss 1.3683 (1.3769)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [83][350/391]\tTime 0.096 (0.108)\tData 0.000 (0.019)\tLoss 1.5371 (1.3832)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.155 (0.155)\tLoss 0.7856 (0.7856)\tPrec@1 94.531 (94.531)\n",
            "Test: [50/79]\tTime 0.037 (0.034)\tLoss 0.8096 (0.8174)\tPrec@1 90.625 (91.452)\n",
            " * Prec@1 91.620\n",
            "\n",
            "Epoch: 85/100\n",
            "Learning rate: 0.006278\n",
            "Epoch: [84][0/391]\tTime 0.332 (0.332)\tData 0.222 (0.222)\tLoss 1.5340 (1.5340)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [84][50/391]\tTime 0.114 (0.105)\tData 0.000 (0.014)\tLoss 1.4030 (1.3779)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [84][100/391]\tTime 0.074 (0.111)\tData 0.000 (0.020)\tLoss 1.4437 (1.3695)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [84][150/391]\tTime 0.129 (0.108)\tData 0.035 (0.020)\tLoss 1.5941 (1.3767)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [84][200/391]\tTime 0.074 (0.110)\tData 0.000 (0.021)\tLoss 1.6537 (1.3869)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [84][250/391]\tTime 0.073 (0.108)\tData 0.000 (0.021)\tLoss 1.6281 (1.3830)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [84][300/391]\tTime 0.111 (0.110)\tData 0.000 (0.022)\tLoss 1.2225 (1.3855)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [84][350/391]\tTime 0.072 (0.109)\tData 0.000 (0.021)\tLoss 1.3536 (1.3781)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.142 (0.142)\tLoss 0.7853 (0.7853)\tPrec@1 92.969 (92.969)\n",
            "Test: [50/79]\tTime 0.037 (0.046)\tLoss 0.7744 (0.7989)\tPrec@1 94.531 (91.896)\n",
            " * Prec@1 92.060\n",
            "\n",
            "Epoch: 86/100\n",
            "Learning rate: 0.005544\n",
            "Epoch: [85][0/391]\tTime 0.327 (0.327)\tData 0.214 (0.214)\tLoss 1.2854 (1.2854)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [85][50/391]\tTime 0.075 (0.103)\tData 0.000 (0.018)\tLoss 1.2290 (1.3129)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [85][100/391]\tTime 0.161 (0.110)\tData 0.000 (0.025)\tLoss 0.9677 (1.3357)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [85][150/391]\tTime 0.077 (0.108)\tData 0.000 (0.021)\tLoss 1.0450 (1.3464)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [85][200/391]\tTime 0.098 (0.106)\tData 0.000 (0.018)\tLoss 1.4698 (1.3492)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [85][250/391]\tTime 0.129 (0.108)\tData 0.059 (0.019)\tLoss 1.5110 (1.3572)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [85][300/391]\tTime 0.145 (0.107)\tData 0.059 (0.020)\tLoss 1.1071 (1.3486)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [85][350/391]\tTime 0.115 (0.109)\tData 0.040 (0.021)\tLoss 1.5093 (1.3588)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.146 (0.146)\tLoss 0.7669 (0.7669)\tPrec@1 92.969 (92.969)\n",
            "Test: [50/79]\tTime 0.046 (0.033)\tLoss 0.7957 (0.7988)\tPrec@1 90.625 (91.682)\n",
            " * Prec@1 91.880\n",
            "\n",
            "Epoch: 87/100\n",
            "Learning rate: 0.004854\n",
            "Epoch: [86][0/391]\tTime 0.327 (0.327)\tData 0.210 (0.210)\tLoss 1.6310 (1.6310)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [86][50/391]\tTime 0.086 (0.118)\tData 0.009 (0.025)\tLoss 1.6069 (1.3637)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [86][100/391]\tTime 0.096 (0.109)\tData 0.005 (0.021)\tLoss 1.1357 (1.3230)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [86][150/391]\tTime 0.112 (0.107)\tData 0.027 (0.018)\tLoss 1.5893 (1.3501)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [86][200/391]\tTime 0.084 (0.108)\tData 0.000 (0.019)\tLoss 1.2814 (1.3525)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [86][250/391]\tTime 0.089 (0.107)\tData 0.000 (0.019)\tLoss 1.3801 (1.3644)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [86][300/391]\tTime 0.077 (0.108)\tData 0.000 (0.019)\tLoss 1.2243 (1.3709)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [86][350/391]\tTime 0.102 (0.107)\tData 0.006 (0.019)\tLoss 1.3876 (1.3697)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.240 (0.240)\tLoss 0.7743 (0.7743)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.042 (0.040)\tLoss 0.7957 (0.8243)\tPrec@1 92.969 (91.330)\n",
            " * Prec@1 91.460\n",
            "\n",
            "Epoch: 88/100\n",
            "Learning rate: 0.004208\n",
            "Epoch: [87][0/391]\tTime 0.367 (0.367)\tData 0.232 (0.232)\tLoss 0.9728 (0.9728)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [87][50/391]\tTime 0.102 (0.102)\tData 0.005 (0.012)\tLoss 1.4045 (1.3442)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [87][100/391]\tTime 0.092 (0.109)\tData 0.007 (0.017)\tLoss 1.6935 (1.3554)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [87][150/391]\tTime 0.095 (0.105)\tData 0.000 (0.014)\tLoss 1.3700 (1.3614)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [87][200/391]\tTime 0.102 (0.103)\tData 0.011 (0.012)\tLoss 1.3439 (1.3644)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [87][250/391]\tTime 0.100 (0.106)\tData 0.000 (0.013)\tLoss 1.3355 (1.3620)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [87][300/391]\tTime 0.107 (0.105)\tData 0.007 (0.013)\tLoss 1.6370 (1.3595)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [87][350/391]\tTime 0.110 (0.107)\tData 0.020 (0.014)\tLoss 1.6771 (1.3617)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.163 (0.163)\tLoss 0.7820 (0.7820)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.029 (0.034)\tLoss 0.8109 (0.8270)\tPrec@1 92.188 (91.881)\n",
            " * Prec@1 92.150\n",
            "\n",
            "Epoch: 89/100\n",
            "Learning rate: 0.003608\n",
            "Epoch: [88][0/391]\tTime 0.329 (0.329)\tData 0.219 (0.219)\tLoss 1.0533 (1.0533)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [88][50/391]\tTime 0.095 (0.119)\tData 0.000 (0.024)\tLoss 1.0638 (1.3633)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [88][100/391]\tTime 0.084 (0.110)\tData 0.000 (0.018)\tLoss 1.6511 (1.3751)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [88][150/391]\tTime 0.181 (0.113)\tData 0.073 (0.020)\tLoss 1.4879 (1.3798)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [88][200/391]\tTime 0.120 (0.110)\tData 0.024 (0.018)\tLoss 0.7861 (1.3769)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [88][250/391]\tTime 0.091 (0.108)\tData 0.006 (0.016)\tLoss 1.0564 (1.3641)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [88][300/391]\tTime 0.152 (0.109)\tData 0.065 (0.018)\tLoss 1.1180 (1.3625)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [88][350/391]\tTime 0.075 (0.108)\tData 0.000 (0.019)\tLoss 0.9448 (1.3554)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.153 (0.153)\tLoss 0.7514 (0.7514)\tPrec@1 94.531 (94.531)\n",
            "Test: [50/79]\tTime 0.029 (0.033)\tLoss 0.7755 (0.7778)\tPrec@1 92.188 (92.479)\n",
            " * Prec@1 92.540\n",
            "\n",
            "Epoch: 90/100\n",
            "Learning rate: 0.003053\n",
            "Epoch: [89][0/391]\tTime 0.332 (0.332)\tData 0.216 (0.216)\tLoss 1.5899 (1.5899)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [89][50/391]\tTime 0.136 (0.105)\tData 0.051 (0.014)\tLoss 1.0929 (1.3903)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [89][100/391]\tTime 0.087 (0.111)\tData 0.000 (0.017)\tLoss 1.6251 (1.3742)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [89][150/391]\tTime 0.071 (0.107)\tData 0.000 (0.017)\tLoss 1.2897 (1.3563)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [89][200/391]\tTime 0.260 (0.110)\tData 0.175 (0.019)\tLoss 1.4691 (1.3626)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [89][250/391]\tTime 0.111 (0.108)\tData 0.015 (0.019)\tLoss 1.2793 (1.3562)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [89][300/391]\tTime 0.128 (0.108)\tData 0.018 (0.019)\tLoss 1.4993 (1.3487)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [89][350/391]\tTime 0.120 (0.109)\tData 0.031 (0.019)\tLoss 1.4358 (1.3446)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.156 (0.156)\tLoss 0.7414 (0.7414)\tPrec@1 94.531 (94.531)\n",
            "Test: [50/79]\tTime 0.025 (0.033)\tLoss 0.7505 (0.7743)\tPrec@1 93.750 (92.754)\n",
            " * Prec@1 92.910\n",
            "\n",
            "Epoch: 91/100\n",
            "Learning rate: 0.002545\n",
            "Epoch: [90][0/391]\tTime 0.440 (0.440)\tData 0.314 (0.314)\tLoss 1.5479 (1.5479)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [90][50/391]\tTime 0.082 (0.120)\tData 0.011 (0.025)\tLoss 1.6309 (1.3560)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [90][100/391]\tTime 0.119 (0.109)\tData 0.023 (0.016)\tLoss 1.0549 (1.3360)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [90][150/391]\tTime 0.096 (0.113)\tData 0.000 (0.018)\tLoss 1.5594 (1.3383)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [90][200/391]\tTime 0.099 (0.110)\tData 0.000 (0.016)\tLoss 1.6289 (1.3482)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [90][250/391]\tTime 0.093 (0.112)\tData 0.006 (0.018)\tLoss 1.1096 (1.3501)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [90][300/391]\tTime 0.104 (0.110)\tData 0.000 (0.016)\tLoss 1.6061 (1.3502)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [90][350/391]\tTime 0.181 (0.109)\tData 0.064 (0.016)\tLoss 1.1753 (1.3493)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.173 (0.173)\tLoss 0.7587 (0.7587)\tPrec@1 93.750 (93.750)\n",
            "Test: [50/79]\tTime 0.015 (0.034)\tLoss 0.7741 (0.7974)\tPrec@1 92.969 (92.678)\n",
            " * Prec@1 93.020\n",
            "\n",
            "Epoch: 92/100\n",
            "Learning rate: 0.002083\n",
            "Epoch: [91][0/391]\tTime 0.330 (0.330)\tData 0.219 (0.219)\tLoss 1.5189 (1.5189)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [91][50/391]\tTime 0.228 (0.109)\tData 0.142 (0.016)\tLoss 1.2749 (1.3195)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [91][100/391]\tTime 0.106 (0.112)\tData 0.005 (0.020)\tLoss 0.9499 (1.2987)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [91][150/391]\tTime 0.103 (0.107)\tData 0.006 (0.016)\tLoss 1.5717 (1.2876)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [91][200/391]\tTime 0.099 (0.110)\tData 0.028 (0.017)\tLoss 1.0667 (1.3019)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [91][250/391]\tTime 0.102 (0.108)\tData 0.005 (0.016)\tLoss 1.3145 (1.3253)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [91][300/391]\tTime 0.099 (0.110)\tData 0.000 (0.017)\tLoss 1.4916 (1.3169)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [91][350/391]\tTime 0.102 (0.109)\tData 0.000 (0.016)\tLoss 1.5755 (1.3196)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.157 (0.157)\tLoss 0.7658 (0.7658)\tPrec@1 94.531 (94.531)\n",
            "Test: [50/79]\tTime 0.053 (0.043)\tLoss 0.7795 (0.8013)\tPrec@1 93.750 (92.862)\n",
            " * Prec@1 93.070\n",
            "\n",
            "Epoch: 93/100\n",
            "Learning rate: 0.001669\n",
            "Epoch: [92][0/391]\tTime 0.340 (0.340)\tData 0.222 (0.222)\tLoss 1.4552 (1.4552)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [92][50/391]\tTime 0.074 (0.103)\tData 0.000 (0.013)\tLoss 1.0224 (1.3320)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [92][100/391]\tTime 0.212 (0.104)\tData 0.101 (0.014)\tLoss 1.5927 (1.3433)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [92][150/391]\tTime 0.117 (0.107)\tData 0.023 (0.016)\tLoss 0.7852 (1.3541)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [92][200/391]\tTime 0.100 (0.106)\tData 0.001 (0.019)\tLoss 1.5822 (1.3580)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [92][250/391]\tTime 0.097 (0.108)\tData 0.002 (0.020)\tLoss 1.4836 (1.3571)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [92][300/391]\tTime 0.097 (0.107)\tData 0.000 (0.019)\tLoss 1.3173 (1.3562)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [92][350/391]\tTime 0.084 (0.109)\tData 0.000 (0.019)\tLoss 1.5895 (1.3458)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.148 (0.148)\tLoss 0.7740 (0.7740)\tPrec@1 94.531 (94.531)\n",
            "Test: [50/79]\tTime 0.016 (0.034)\tLoss 0.7730 (0.7974)\tPrec@1 94.531 (92.494)\n",
            " * Prec@1 92.870\n",
            "\n",
            "Epoch: 94/100\n",
            "Learning rate: 0.001303\n",
            "Epoch: [93][0/391]\tTime 0.360 (0.360)\tData 0.237 (0.237)\tLoss 0.7829 (0.7829)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [93][50/391]\tTime 0.099 (0.120)\tData 0.000 (0.022)\tLoss 0.9922 (1.3536)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [93][100/391]\tTime 0.134 (0.111)\tData 0.064 (0.019)\tLoss 1.6249 (1.3817)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [93][150/391]\tTime 0.208 (0.109)\tData 0.123 (0.017)\tLoss 1.6436 (1.3804)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [93][200/391]\tTime 0.113 (0.110)\tData 0.005 (0.018)\tLoss 1.4194 (1.3666)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [93][250/391]\tTime 0.093 (0.108)\tData 0.000 (0.016)\tLoss 1.5340 (1.3604)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [93][300/391]\tTime 0.093 (0.110)\tData 0.000 (0.018)\tLoss 0.7321 (1.3570)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [93][350/391]\tTime 0.134 (0.108)\tData 0.060 (0.017)\tLoss 1.1955 (1.3639)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.196 (0.196)\tLoss 0.7198 (0.7198)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.049 (0.038)\tLoss 0.7280 (0.7549)\tPrec@1 94.531 (93.091)\n",
            " * Prec@1 93.260\n",
            "\n",
            "Epoch: 95/100\n",
            "Learning rate: 0.000985\n",
            "Epoch: [94][0/391]\tTime 0.333 (0.333)\tData 0.223 (0.223)\tLoss 1.0857 (1.0857)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [94][50/391]\tTime 0.073 (0.104)\tData 0.000 (0.013)\tLoss 1.0358 (1.3149)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [94][100/391]\tTime 0.112 (0.111)\tData 0.022 (0.019)\tLoss 0.9467 (1.3063)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [94][150/391]\tTime 0.120 (0.107)\tData 0.045 (0.016)\tLoss 0.9524 (1.3023)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [94][200/391]\tTime 0.126 (0.108)\tData 0.036 (0.018)\tLoss 1.0838 (1.3125)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [94][250/391]\tTime 0.076 (0.109)\tData 0.000 (0.020)\tLoss 1.1439 (1.3235)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [94][300/391]\tTime 0.137 (0.107)\tData 0.058 (0.019)\tLoss 1.2829 (1.3316)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [94][350/391]\tTime 0.130 (0.109)\tData 0.055 (0.020)\tLoss 1.6558 (1.3260)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.167 (0.167)\tLoss 0.7408 (0.7408)\tPrec@1 96.094 (96.094)\n",
            "Test: [50/79]\tTime 0.048 (0.033)\tLoss 0.7617 (0.7827)\tPrec@1 92.969 (93.061)\n",
            " * Prec@1 93.290\n",
            "\n",
            "Epoch: 96/100\n",
            "Learning rate: 0.000715\n",
            "Epoch: [95][0/391]\tTime 0.349 (0.349)\tData 0.235 (0.235)\tLoss 1.2500 (1.2500)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [95][50/391]\tTime 0.093 (0.123)\tData 0.000 (0.033)\tLoss 1.6332 (1.3045)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [95][100/391]\tTime 0.102 (0.111)\tData 0.005 (0.020)\tLoss 1.4188 (1.3196)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [95][150/391]\tTime 0.092 (0.113)\tData 0.006 (0.021)\tLoss 1.5296 (1.3012)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [95][200/391]\tTime 0.103 (0.109)\tData 0.006 (0.018)\tLoss 1.0847 (1.2912)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [95][250/391]\tTime 0.086 (0.109)\tData 0.000 (0.018)\tLoss 1.5407 (1.2995)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [95][300/391]\tTime 0.092 (0.109)\tData 0.000 (0.019)\tLoss 1.0274 (1.2967)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [95][350/391]\tTime 0.098 (0.108)\tData 0.000 (0.018)\tLoss 1.5050 (1.3042)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.147 (0.147)\tLoss 0.7221 (0.7221)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.036 (0.034)\tLoss 0.7228 (0.7570)\tPrec@1 96.094 (93.183)\n",
            " * Prec@1 93.420\n",
            "\n",
            "Epoch: 97/100\n",
            "Learning rate: 0.000494\n",
            "Epoch: [96][0/391]\tTime 0.329 (0.329)\tData 0.219 (0.219)\tLoss 0.8143 (0.8143)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [96][50/391]\tTime 0.100 (0.103)\tData 0.006 (0.010)\tLoss 1.3573 (1.3570)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [96][100/391]\tTime 0.094 (0.110)\tData 0.000 (0.013)\tLoss 1.4047 (1.3133)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [96][150/391]\tTime 0.102 (0.108)\tData 0.004 (0.016)\tLoss 1.4699 (1.3182)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [96][200/391]\tTime 0.131 (0.110)\tData 0.061 (0.019)\tLoss 1.0118 (1.3157)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [96][250/391]\tTime 0.134 (0.108)\tData 0.044 (0.017)\tLoss 1.3062 (1.3167)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [96][300/391]\tTime 0.103 (0.108)\tData 0.003 (0.018)\tLoss 0.9047 (1.3143)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [96][350/391]\tTime 0.128 (0.109)\tData 0.058 (0.019)\tLoss 0.8536 (1.3166)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.161 (0.161)\tLoss 0.6971 (0.6971)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.032 (0.033)\tLoss 0.7052 (0.7374)\tPrec@1 95.312 (93.382)\n",
            " * Prec@1 93.510\n",
            "\n",
            "Epoch: 98/100\n",
            "Learning rate: 0.000322\n",
            "Epoch: [97][0/391]\tTime 0.414 (0.414)\tData 0.289 (0.289)\tLoss 1.5332 (1.5332)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [97][50/391]\tTime 0.089 (0.112)\tData 0.000 (0.022)\tLoss 1.3958 (1.3549)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [97][100/391]\tTime 0.098 (0.106)\tData 0.000 (0.016)\tLoss 1.5154 (1.3354)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [97][150/391]\tTime 0.116 (0.109)\tData 0.028 (0.019)\tLoss 1.5994 (1.3303)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [97][200/391]\tTime 0.096 (0.108)\tData 0.005 (0.020)\tLoss 1.4546 (1.3282)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [97][250/391]\tTime 0.096 (0.109)\tData 0.000 (0.019)\tLoss 1.3431 (1.3162)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [97][300/391]\tTime 0.075 (0.107)\tData 0.000 (0.019)\tLoss 1.4199 (1.3244)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [97][350/391]\tTime 0.129 (0.108)\tData 0.002 (0.019)\tLoss 1.3867 (1.3255)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.151 (0.151)\tLoss 0.7597 (0.7597)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.038 (0.034)\tLoss 0.7667 (0.7905)\tPrec@1 94.531 (93.076)\n",
            " * Prec@1 93.340\n",
            "\n",
            "Epoch: 99/100\n",
            "Learning rate: 0.000199\n",
            "Epoch: [98][0/391]\tTime 0.346 (0.346)\tData 0.233 (0.233)\tLoss 1.1758 (1.1758)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [98][50/391]\tTime 0.178 (0.116)\tData 0.088 (0.022)\tLoss 1.5042 (1.3346)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [98][100/391]\tTime 0.097 (0.109)\tData 0.005 (0.015)\tLoss 1.0096 (1.3220)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [98][150/391]\tTime 0.074 (0.107)\tData 0.000 (0.015)\tLoss 1.2580 (1.3348)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [98][200/391]\tTime 0.103 (0.110)\tData 0.013 (0.019)\tLoss 1.0142 (1.3222)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [98][250/391]\tTime 0.115 (0.109)\tData 0.022 (0.019)\tLoss 1.7824 (1.3371)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [98][300/391]\tTime 0.094 (0.110)\tData 0.000 (0.020)\tLoss 1.5454 (1.3223)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [98][350/391]\tTime 0.094 (0.109)\tData 0.010 (0.019)\tLoss 1.4059 (1.3232)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.156 (0.156)\tLoss 0.7226 (0.7226)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.050 (0.047)\tLoss 0.7332 (0.7584)\tPrec@1 92.969 (93.107)\n",
            " * Prec@1 93.360\n",
            "\n",
            "Epoch: 100/100\n",
            "Learning rate: 0.000125\n",
            "Epoch: [99][0/391]\tTime 0.336 (0.336)\tData 0.219 (0.219)\tLoss 1.2673 (1.2673)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [99][50/391]\tTime 0.101 (0.103)\tData 0.000 (0.009)\tLoss 1.4045 (1.3577)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [99][100/391]\tTime 0.100 (0.109)\tData 0.005 (0.013)\tLoss 0.9602 (1.3435)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [99][150/391]\tTime 0.074 (0.108)\tData 0.000 (0.014)\tLoss 1.1236 (1.3262)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [99][200/391]\tTime 0.104 (0.106)\tData 0.011 (0.012)\tLoss 1.5834 (1.3228)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [99][250/391]\tTime 0.135 (0.108)\tData 0.060 (0.016)\tLoss 1.5399 (1.3209)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [99][300/391]\tTime 0.077 (0.107)\tData 0.000 (0.015)\tLoss 0.9790 (1.3271)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [99][350/391]\tTime 0.141 (0.109)\tData 0.055 (0.017)\tLoss 1.0543 (1.3295)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.159 (0.159)\tLoss 0.7534 (0.7534)\tPrec@1 94.531 (94.531)\n",
            "Test: [50/79]\tTime 0.036 (0.033)\tLoss 0.7642 (0.7858)\tPrec@1 93.750 (93.030)\n",
            " * Prec@1 93.280\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(HybridResNet(\n",
              "   (conv1): Conv2d(3, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "   (bn1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "   (layer1): Sequential(\n",
              "     (0): BasicBlock(\n",
              "       (conv1): Conv2d(22, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(22, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (dropout): Dropout(p=0.15, inplace=False)\n",
              "       (se): SELayer(\n",
              "         (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "         (fc): Sequential(\n",
              "           (0): Linear(in_features=22, out_features=1, bias=False)\n",
              "           (1): ReLU(inplace=True)\n",
              "           (2): Linear(in_features=1, out_features=22, bias=False)\n",
              "           (3): Sigmoid()\n",
              "         )\n",
              "       )\n",
              "       (shortcut): Sequential()\n",
              "     )\n",
              "     (1): BasicBlock(\n",
              "       (conv1): Conv2d(22, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(22, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (dropout): Dropout(p=0.15, inplace=False)\n",
              "       (se): SELayer(\n",
              "         (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "         (fc): Sequential(\n",
              "           (0): Linear(in_features=22, out_features=1, bias=False)\n",
              "           (1): ReLU(inplace=True)\n",
              "           (2): Linear(in_features=1, out_features=22, bias=False)\n",
              "           (3): Sigmoid()\n",
              "         )\n",
              "       )\n",
              "       (shortcut): Sequential()\n",
              "     )\n",
              "     (2): BasicBlock(\n",
              "       (conv1): Conv2d(22, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(22, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (dropout): Dropout(p=0.15, inplace=False)\n",
              "       (se): SELayer(\n",
              "         (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "         (fc): Sequential(\n",
              "           (0): Linear(in_features=22, out_features=1, bias=False)\n",
              "           (1): ReLU(inplace=True)\n",
              "           (2): Linear(in_features=1, out_features=22, bias=False)\n",
              "           (3): Sigmoid()\n",
              "         )\n",
              "       )\n",
              "       (shortcut): Sequential()\n",
              "     )\n",
              "   )\n",
              "   (layer2): Sequential(\n",
              "     (0): BasicBlock(\n",
              "       (conv1): Conv2d(22, 44, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(44, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (dropout): Dropout(p=0.15, inplace=False)\n",
              "       (se): SELayer(\n",
              "         (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "         (fc): Sequential(\n",
              "           (0): Linear(in_features=44, out_features=2, bias=False)\n",
              "           (1): ReLU(inplace=True)\n",
              "           (2): Linear(in_features=2, out_features=44, bias=False)\n",
              "           (3): Sigmoid()\n",
              "         )\n",
              "       )\n",
              "       (shortcut): Sequential(\n",
              "         (0): Conv2d(22, 44, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "         (1): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       )\n",
              "     )\n",
              "     (1): BasicBlock(\n",
              "       (conv1): Conv2d(44, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(44, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (dropout): Dropout(p=0.15, inplace=False)\n",
              "       (se): SELayer(\n",
              "         (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "         (fc): Sequential(\n",
              "           (0): Linear(in_features=44, out_features=2, bias=False)\n",
              "           (1): ReLU(inplace=True)\n",
              "           (2): Linear(in_features=2, out_features=44, bias=False)\n",
              "           (3): Sigmoid()\n",
              "         )\n",
              "       )\n",
              "       (shortcut): Sequential()\n",
              "     )\n",
              "     (2): BasicBlock(\n",
              "       (conv1): Conv2d(44, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(44, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (dropout): Dropout(p=0.15, inplace=False)\n",
              "       (se): SELayer(\n",
              "         (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "         (fc): Sequential(\n",
              "           (0): Linear(in_features=44, out_features=2, bias=False)\n",
              "           (1): ReLU(inplace=True)\n",
              "           (2): Linear(in_features=2, out_features=44, bias=False)\n",
              "           (3): Sigmoid()\n",
              "         )\n",
              "       )\n",
              "       (shortcut): Sequential()\n",
              "     )\n",
              "     (3): BasicBlock(\n",
              "       (conv1): Conv2d(44, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(44, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (dropout): Dropout(p=0.15, inplace=False)\n",
              "       (se): SELayer(\n",
              "         (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "         (fc): Sequential(\n",
              "           (0): Linear(in_features=44, out_features=2, bias=False)\n",
              "           (1): ReLU(inplace=True)\n",
              "           (2): Linear(in_features=2, out_features=44, bias=False)\n",
              "           (3): Sigmoid()\n",
              "         )\n",
              "       )\n",
              "       (shortcut): Sequential()\n",
              "     )\n",
              "   )\n",
              "   (layer3): Sequential(\n",
              "     (0): BasicBlock(\n",
              "       (conv1): Conv2d(44, 89, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(89, 89, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (dropout): Dropout(p=0.15, inplace=False)\n",
              "       (se): SELayer(\n",
              "         (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "         (fc): Sequential(\n",
              "           (0): Linear(in_features=89, out_features=5, bias=False)\n",
              "           (1): ReLU(inplace=True)\n",
              "           (2): Linear(in_features=5, out_features=89, bias=False)\n",
              "           (3): Sigmoid()\n",
              "         )\n",
              "       )\n",
              "       (shortcut): Sequential(\n",
              "         (0): Conv2d(44, 89, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "         (1): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       )\n",
              "     )\n",
              "     (1): BasicBlock(\n",
              "       (conv1): Conv2d(89, 89, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(89, 89, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (dropout): Dropout(p=0.15, inplace=False)\n",
              "       (se): SELayer(\n",
              "         (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "         (fc): Sequential(\n",
              "           (0): Linear(in_features=89, out_features=5, bias=False)\n",
              "           (1): ReLU(inplace=True)\n",
              "           (2): Linear(in_features=5, out_features=89, bias=False)\n",
              "           (3): Sigmoid()\n",
              "         )\n",
              "       )\n",
              "       (shortcut): Sequential()\n",
              "     )\n",
              "     (2): BasicBlock(\n",
              "       (conv1): Conv2d(89, 89, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(89, 89, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (dropout): Dropout(p=0.15, inplace=False)\n",
              "       (se): SELayer(\n",
              "         (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "         (fc): Sequential(\n",
              "           (0): Linear(in_features=89, out_features=5, bias=False)\n",
              "           (1): ReLU(inplace=True)\n",
              "           (2): Linear(in_features=5, out_features=89, bias=False)\n",
              "           (3): Sigmoid()\n",
              "         )\n",
              "       )\n",
              "       (shortcut): Sequential()\n",
              "     )\n",
              "     (3): BasicBlock(\n",
              "       (conv1): Conv2d(89, 89, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(89, 89, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (dropout): Dropout(p=0.15, inplace=False)\n",
              "       (se): SELayer(\n",
              "         (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "         (fc): Sequential(\n",
              "           (0): Linear(in_features=89, out_features=5, bias=False)\n",
              "           (1): ReLU(inplace=True)\n",
              "           (2): Linear(in_features=5, out_features=89, bias=False)\n",
              "           (3): Sigmoid()\n",
              "         )\n",
              "       )\n",
              "       (shortcut): Sequential()\n",
              "     )\n",
              "     (4): BasicBlock(\n",
              "       (conv1): Conv2d(89, 89, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(89, 89, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (dropout): Dropout(p=0.15, inplace=False)\n",
              "       (se): SELayer(\n",
              "         (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "         (fc): Sequential(\n",
              "           (0): Linear(in_features=89, out_features=5, bias=False)\n",
              "           (1): ReLU(inplace=True)\n",
              "           (2): Linear(in_features=5, out_features=89, bias=False)\n",
              "           (3): Sigmoid()\n",
              "         )\n",
              "       )\n",
              "       (shortcut): Sequential()\n",
              "     )\n",
              "     (5): BasicBlock(\n",
              "       (conv1): Conv2d(89, 89, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(89, 89, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (dropout): Dropout(p=0.15, inplace=False)\n",
              "       (se): SELayer(\n",
              "         (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "         (fc): Sequential(\n",
              "           (0): Linear(in_features=89, out_features=5, bias=False)\n",
              "           (1): ReLU(inplace=True)\n",
              "           (2): Linear(in_features=5, out_features=89, bias=False)\n",
              "           (3): Sigmoid()\n",
              "         )\n",
              "       )\n",
              "       (shortcut): Sequential()\n",
              "     )\n",
              "   )\n",
              "   (layer4): Sequential(\n",
              "     (0): BasicBlock(\n",
              "       (conv1): Conv2d(89, 179, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(179, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(179, 179, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(179, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (dropout): Dropout(p=0.15, inplace=False)\n",
              "       (se): SELayer(\n",
              "         (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "         (fc): Sequential(\n",
              "           (0): Linear(in_features=179, out_features=11, bias=False)\n",
              "           (1): ReLU(inplace=True)\n",
              "           (2): Linear(in_features=11, out_features=179, bias=False)\n",
              "           (3): Sigmoid()\n",
              "         )\n",
              "       )\n",
              "       (shortcut): Sequential(\n",
              "         (0): Conv2d(89, 179, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "         (1): BatchNorm2d(179, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       )\n",
              "     )\n",
              "     (1): BasicBlock(\n",
              "       (conv1): Conv2d(179, 179, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(179, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(179, 179, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(179, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (dropout): Dropout(p=0.15, inplace=False)\n",
              "       (se): SELayer(\n",
              "         (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "         (fc): Sequential(\n",
              "           (0): Linear(in_features=179, out_features=11, bias=False)\n",
              "           (1): ReLU(inplace=True)\n",
              "           (2): Linear(in_features=11, out_features=179, bias=False)\n",
              "           (3): Sigmoid()\n",
              "         )\n",
              "       )\n",
              "       (shortcut): Sequential()\n",
              "     )\n",
              "     (2): BasicBlock(\n",
              "       (conv1): Conv2d(179, 179, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(179, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(179, 179, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(179, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (dropout): Dropout(p=0.15, inplace=False)\n",
              "       (se): SELayer(\n",
              "         (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "         (fc): Sequential(\n",
              "           (0): Linear(in_features=179, out_features=11, bias=False)\n",
              "           (1): ReLU(inplace=True)\n",
              "           (2): Linear(in_features=11, out_features=179, bias=False)\n",
              "           (3): Sigmoid()\n",
              "         )\n",
              "       )\n",
              "       (shortcut): Sequential()\n",
              "     )\n",
              "   )\n",
              "   (dropout): Dropout(p=0.15, inplace=False)\n",
              "   (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "   (fc): Linear(in_features=179, out_features=10, bias=True)\n",
              " ),\n",
              " 93.51)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "#---------- MODEL DEFINITION ----------#\n",
        "\n",
        "class SELayer(nn.Module):\n",
        "    \"\"\"Squeeze-and-Excitation block for channel attention\"\"\"\n",
        "    def __init__(self, channel, reduction=16):\n",
        "        super(SELayer, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channel, channel // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channel // reduction, channel, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    \"\"\"Basic ResNet block with two 3x3 convolutions and a skip connection\"\"\"\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, dropout_rate=0.0, use_se=False):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.dropout = nn.Dropout(dropout_rate) if dropout_rate > 0 else None\n",
        "\n",
        "        # Optional SE block\n",
        "        self.use_se = use_se\n",
        "        if use_se:\n",
        "            self.se = SELayer(planes, reduction=16)\n",
        "\n",
        "        # Skip connection handling\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        if self.dropout is not None:\n",
        "            out = self.dropout(out)\n",
        "        out = self.bn2(self.conv2(out))\n",
        "\n",
        "        if self.use_se:\n",
        "            out = self.se(out)\n",
        "\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class BottleneckBlock(nn.Module):\n",
        "    \"\"\"Bottleneck block with 1x1, 3x3, 1x1 convolutions as shown in the tutorial\"\"\"\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, dropout_rate=0.0, use_se=False):\n",
        "        super(BottleneckBlock, self).__init__()\n",
        "        # First 1x1 conv to reduce channels\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        # 3x3 conv with potential stride\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        # Second 1x1 conv to increase channels (expansion)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_rate) if dropout_rate > 0 else None\n",
        "\n",
        "        # Optional SE block\n",
        "        self.use_se = use_se\n",
        "        if use_se:\n",
        "            self.se = SELayer(planes * self.expansion, reduction=16)\n",
        "\n",
        "        # Skip connection handling\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes * self.expansion:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, planes * self.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * self.expansion)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        if self.dropout is not None:\n",
        "            out = self.dropout(out)\n",
        "        out = self.bn3(self.conv3(out))\n",
        "\n",
        "        if self.use_se:\n",
        "            out = self.se(out)\n",
        "\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class HybridResNet(nn.Module):\n",
        "    \"\"\"Hybrid ResNet combining features from both implementations\"\"\"\n",
        "    def __init__(self, block, layers, num_classes=10, dropout_rate=0.0, use_se=False,\n",
        "                 initial_channels=32, width_multiplier=1.0):\n",
        "        super(HybridResNet, self).__init__()\n",
        "        self.in_planes = int(initial_channels * width_multiplier)\n",
        "\n",
        "        # Initial convolution\n",
        "        self.conv1 = nn.Conv2d(3, self.in_planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(self.in_planes)\n",
        "\n",
        "        # Create stages with progressively more channels\n",
        "        # Adjusted to use width_multiplier for controlled scaling\n",
        "        self.layer1 = self._make_layer(block, int(32 * width_multiplier), layers[0], stride=1,\n",
        "                                     dropout_rate=dropout_rate, use_se=use_se)\n",
        "        self.layer2 = self._make_layer(block, int(64 * width_multiplier), layers[1], stride=2,\n",
        "                                     dropout_rate=dropout_rate, use_se=use_se)\n",
        "        self.layer3 = self._make_layer(block, int(128 * width_multiplier), layers[2], stride=2,\n",
        "                                     dropout_rate=dropout_rate, use_se=use_se)\n",
        "        self.layer4 = self._make_layer(block, int(256 * width_multiplier), layers[3], stride=2,\n",
        "                                     dropout_rate=dropout_rate, use_se=use_se)\n",
        "\n",
        "        # Final classifier\n",
        "        self.dropout = nn.Dropout(dropout_rate) if dropout_rate > 0 else None\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        final_channels = int(256 * width_multiplier) * block.expansion\n",
        "        self.fc = nn.Linear(final_channels, num_classes)\n",
        "\n",
        "        # Initialize weights\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride, dropout_rate=0.0, use_se=False):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride, dropout_rate, use_se))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        if self.dropout is not None:\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Model configurations\n",
        "def ResNet18_Hybrid(num_classes=10, use_se=False, dropout_rate=0.0, width_multiplier=1.0):\n",
        "    \"\"\"ResNet-18 with hybrid features\"\"\"\n",
        "    return HybridResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes,\n",
        "                      dropout_rate=dropout_rate, use_se=use_se, width_multiplier=width_multiplier)\n",
        "\n",
        "def ResNet34_Hybrid(num_classes=10, use_se=False, dropout_rate=0.0, width_multiplier=1.0):\n",
        "    \"\"\"ResNet-34 with hybrid features\"\"\"\n",
        "    return HybridResNet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes,\n",
        "                      dropout_rate=dropout_rate, use_se=use_se, width_multiplier=width_multiplier)\n",
        "\n",
        "def ResNet50_Hybrid(num_classes=10, use_se=False, dropout_rate=0.0, width_multiplier=0.5):\n",
        "    \"\"\"ResNet-50 with hybrid features - scaled down for CIFAR-10\"\"\"\n",
        "    return HybridResNet(BottleneckBlock, [3, 4, 6, 3], num_classes=num_classes,\n",
        "                      dropout_rate=dropout_rate, use_se=use_se, width_multiplier=width_multiplier)\n",
        "\n",
        "# Optimized models specifically designed to be under 5M parameters\n",
        "def OptimizedResNet_A(num_classes=10):\n",
        "    \"\"\"Optimized ResNet with basic blocks and SE attention - around 2.5M parameters\"\"\"\n",
        "    return ResNet18_Hybrid(num_classes=num_classes, use_se=True, dropout_rate=0.1, width_multiplier=0.75)\n",
        "\n",
        "def OptimizedResNet_B(num_classes=10):\n",
        "    \"\"\"Optimized ResNet with basic blocks and focused hyperparameters - around 4.2M parameters\"\"\"\n",
        "    return ResNet34_Hybrid(num_classes=num_classes, use_se=True, dropout_rate=0.15, width_multiplier=0.7)\n",
        "\n",
        "def OptimizedResNet_C(num_classes=10):\n",
        "    \"\"\"Optimized ResNet with bottleneck blocks - most parameter efficient\"\"\"\n",
        "    return ResNet50_Hybrid(num_classes=num_classes, use_se=True, dropout_rate=0.2, width_multiplier=0.25)\n",
        "\n",
        "def project1_model():\n",
        "    \"\"\"Drop-in replacement for the original project1_model function\"\"\"\n",
        "    # Replace with one of the optimized models based on preference\n",
        "    return OptimizedResNet_B(num_classes=10)\n",
        "\n",
        "def count_parameters(model):\n",
        "    \"\"\"Count the total number of trainable parameters in a model\"\"\"\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "#---------- TRAINING UTILITIES ----------#\n",
        "\n",
        "class CutoutTransform:\n",
        "    \"\"\"Cutout data augmentation as shown in the tutorial\"\"\"\n",
        "    def __init__(self, n_holes=1, length=16):\n",
        "        self.n_holes = n_holes\n",
        "        self.length = length\n",
        "\n",
        "    def __call__(self, img):\n",
        "        h = img.size(1)\n",
        "        w = img.size(2)\n",
        "\n",
        "        mask = torch.ones((h, w))\n",
        "\n",
        "        for n in range(self.n_holes):\n",
        "            y = np.random.randint(h)\n",
        "            x = np.random.randint(w)\n",
        "\n",
        "            y1 = np.clip(y - self.length // 2, 0, h)\n",
        "            y2 = np.clip(y + self.length // 2, 0, h)\n",
        "            x1 = np.clip(x - self.length // 2, 0, w)\n",
        "            x2 = np.clip(x + self.length // 2, 0, w)\n",
        "\n",
        "            mask[y1:y2, x1:x2] = 0\n",
        "\n",
        "        mask = mask.expand_as(img)\n",
        "        img = img * mask\n",
        "\n",
        "        return img\n",
        "\n",
        "\n",
        "class MixupTransform:\n",
        "    \"\"\"Mixup augmentation as shown in the tutorial\"\"\"\n",
        "    def __init__(self, alpha=1.0):\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def __call__(self, batch, targets):\n",
        "        if self.alpha > 0:\n",
        "            lam = np.random.beta(self.alpha, self.alpha)\n",
        "        else:\n",
        "            lam = 1\n",
        "\n",
        "        batch_size = batch.size(0)\n",
        "        index = torch.randperm(batch_size).to(batch.device)\n",
        "\n",
        "        mixed_x = lam * batch + (1 - lam) * batch[index, :]\n",
        "        y_a, y_b = targets, targets[index]\n",
        "\n",
        "        return mixed_x, y_a, y_b, lam\n",
        "\n",
        "\n",
        "class LabelSmoothingLoss(nn.Module):\n",
        "    \"\"\"Label smoothing loss for better generalization\"\"\"\n",
        "    def __init__(self, classes, smoothing=0.1):\n",
        "        super(LabelSmoothingLoss, self).__init__()\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.classes = classes\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        pred = pred.log_softmax(-1)\n",
        "        with torch.no_grad():\n",
        "            true_dist = torch.zeros_like(pred)\n",
        "            true_dist.fill_(self.smoothing / (self.classes - 1))\n",
        "            true_dist.scatter_(1, target.unsqueeze(1), self.confidence)\n",
        "        return torch.mean(torch.sum(-true_dist * pred, dim=1))\n",
        "\n",
        "\n",
        "class KeepAverages(object):\n",
        "    \"\"\"Computes and stores the average along with the current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "class ResNetParams:\n",
        "    \"\"\"Hyperparameters for training the ResNet model\"\"\"\n",
        "    def __init__(self, arch='Hybrid-Model', epochs=200, start_epoch=0, batch_size=128,\n",
        "                 lr=0.1, min_lr=1e-4, momentum=0.9, weight_decay=5e-4, print_freq=50,\n",
        "                 save_dir='save_checkpoints', save_every=10, use_mixup=True,\n",
        "                 use_cutout=True, label_smoothing=0.1, model_type='B',\n",
        "                 optimizer='sgd', scheduler='cosine'):\n",
        "        self.save_every = save_every  # Saves checkpoints at specified epochs\n",
        "        self.save_dir = save_dir  # Directory for checkpoints\n",
        "        self.print_freq = print_freq  # Print frequency\n",
        "        self.weight_decay = weight_decay  # Weight decay for optimizer\n",
        "        self.momentum = momentum  # Momentum for SGD\n",
        "        self.lr = lr  # Initial learning rate\n",
        "        self.min_lr = min_lr  # Minimum learning rate for cosine annealing\n",
        "        self.batch_size = batch_size  # Batch size\n",
        "        self.start_epoch = start_epoch  # Starting epoch\n",
        "        self.epochs = epochs  # Total epochs\n",
        "        self.arch = arch  # Model name\n",
        "        self.use_mixup = use_mixup  # Whether to use mixup augmentation\n",
        "        self.use_cutout = use_cutout  # Whether to use cutout augmentation\n",
        "        self.label_smoothing = label_smoothing  # Label smoothing factor\n",
        "        self.model_type = model_type  # Model variant (A, B, or C)\n",
        "        self.optimizer = optimizer  # Optimizer type\n",
        "        self.scheduler = scheduler  # Learning rate scheduler\n",
        "\n",
        "\n",
        "def get_data_loaders(args):\n",
        "    \"\"\"Prepare and return data loaders with augmentations\"\"\"\n",
        "    # CIFAR10 mean and std\n",
        "    cifar_mean = [0.4914, 0.4822, 0.4465]\n",
        "    cifar_std = [0.2023, 0.1994, 0.2010]\n",
        "\n",
        "    # Base transforms for both train and test\n",
        "    transform_base = [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(cifar_mean, cifar_std)\n",
        "    ]\n",
        "\n",
        "    # Enhanced training transforms\n",
        "    transform_train = [\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    ]\n",
        "\n",
        "    # Combine transforms\n",
        "    transform_train.extend(transform_base)\n",
        "\n",
        "    # Add cutout augmentation if enabled\n",
        "    if args.use_cutout:\n",
        "        transform_train.append(CutoutTransform(n_holes=1, length=16))\n",
        "\n",
        "    # Create datasets and loaders\n",
        "    trainset = torchvision.datasets.CIFAR10(\n",
        "        root='./data', train=True, download=True,\n",
        "        transform=transforms.Compose(transform_train)\n",
        "    )\n",
        "\n",
        "    trainloader = DataLoader(\n",
        "        trainset, batch_size=args.batch_size, shuffle=True,\n",
        "        num_workers=2, pin_memory=True\n",
        "    )\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR10(\n",
        "        root='./data', train=False, download=True,\n",
        "        transform=transforms.Compose(transform_base)\n",
        "    )\n",
        "\n",
        "    testloader = DataLoader(\n",
        "        testset, batch_size=args.batch_size, shuffle=False,\n",
        "        num_workers=2, pin_memory=True\n",
        "    )\n",
        "\n",
        "    return trainloader, testloader\n",
        "\n",
        "\n",
        "def get_model(args):\n",
        "    \"\"\"Load the appropriate model based on configuration\"\"\"\n",
        "    if args.model_type == 'A':\n",
        "        model = OptimizedResNet_A()\n",
        "    elif args.model_type == 'B':\n",
        "        model = OptimizedResNet_B()\n",
        "    elif args.model_type == 'C':\n",
        "        model = OptimizedResNet_C()\n",
        "    else:\n",
        "        model = project1_model()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_optimizer(args, model):\n",
        "    \"\"\"Configure optimizer based on args\"\"\"\n",
        "    if args.optimizer == 'sgd':\n",
        "        optimizer = torch.optim.SGD(\n",
        "            model.parameters(),\n",
        "            lr=args.lr,\n",
        "            momentum=args.momentum,\n",
        "            weight_decay=args.weight_decay\n",
        "        )\n",
        "    elif args.optimizer == 'adam':\n",
        "        optimizer = torch.optim.Adam(\n",
        "            model.parameters(),\n",
        "            lr=args.lr,\n",
        "            weight_decay=args.weight_decay\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported optimizer: {args.optimizer}\")\n",
        "\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "def get_scheduler(args, optimizer):\n",
        "    \"\"\"Configure learning rate scheduler based on args\"\"\"\n",
        "    if args.scheduler == 'cosine':\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "            optimizer, T_max=args.epochs, eta_min=args.min_lr\n",
        "        )\n",
        "    elif args.scheduler == 'step':\n",
        "        scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
        "            optimizer, milestones=[int(args.epochs*0.5), int(args.epochs*0.75)], gamma=0.1\n",
        "        )\n",
        "    elif args.scheduler == 'onecycle':\n",
        "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "            optimizer, max_lr=args.lr, total_steps=args.epochs\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported scheduler: {args.scheduler}\")\n",
        "\n",
        "    return scheduler\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Compute the precision@k for the specified values of k\"\"\"\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "\n",
        "    return res\n",
        "\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    \"\"\"Mixup loss function\"\"\"\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, epoch, args, mixup_fn=None):\n",
        "    \"\"\"Training function with support for mixup\"\"\"\n",
        "    # Initialize metrics\n",
        "    batch_time = KeepAverages()\n",
        "    data_time = KeepAverages()\n",
        "    losses = KeepAverages()\n",
        "    top1 = KeepAverages()\n",
        "\n",
        "    # Switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "        # Measure data loading time\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        # Move data to device\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "        # Apply mixup if enabled\n",
        "        if args.use_mixup and mixup_fn is not None:\n",
        "            inputs, targets_a, targets_b, lam = mixup_fn(inputs, targets)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
        "        else:\n",
        "            # Regular forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Measure accuracy and record loss\n",
        "        if not args.use_mixup:  # Skip accuracy during mixup as it's not meaningful\n",
        "            prec1 = accuracy(outputs.data, targets)[0]\n",
        "            top1.update(prec1.item(), inputs.size(0))\n",
        "\n",
        "        losses.update(loss.item(), inputs.size(0))\n",
        "\n",
        "        # Measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        # Print progress\n",
        "        if i % args.print_freq == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
        "                      epoch, i, len(train_loader), batch_time=batch_time,\n",
        "                      data_time=data_time, loss=losses, top1=top1))\n",
        "\n",
        "    return losses.avg, top1.avg\n",
        "\n",
        "\n",
        "def validate(val_loader, model, criterion, args):\n",
        "    \"\"\"Validation function\"\"\"\n",
        "    batch_time = KeepAverages()\n",
        "    losses = KeepAverages()\n",
        "    top1 = KeepAverages()\n",
        "\n",
        "    # Switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        end = time.time()\n",
        "        for i, (inputs, targets) in enumerate(val_loader):\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Measure accuracy and record loss\n",
        "            prec1 = accuracy(outputs.data, targets)[0]\n",
        "            losses.update(loss.item(), inputs.size(0))\n",
        "            top1.update(prec1.item(), inputs.size(0))\n",
        "\n",
        "            # Measure elapsed time\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            # Print progress\n",
        "            if i % args.print_freq == 0:\n",
        "                print('Test: [{0}/{1}]\\t'\n",
        "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                      'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
        "                          i, len(val_loader), batch_time=batch_time,\n",
        "                          loss=losses, top1=top1))\n",
        "\n",
        "    print(' * Prec@1 {top1.avg:.3f}'.format(top1=top1))\n",
        "\n",
        "    return losses.avg, top1.avg\n",
        "\n",
        "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
        "    \"\"\"Save model checkpoint\"\"\"\n",
        "    # Create directory if it doesn't exist\n",
        "    directory = os.path.dirname(filename)\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "    # Save checkpoint\n",
        "    torch.save(state, filename)\n",
        "\n",
        "    if is_best:\n",
        "        best_filename = os.path.join(os.path.dirname(filename), 'model_best.pth.tar')\n",
        "        torch.save(state, best_filename)\n",
        "\n",
        "\n",
        "def run_training(args):\n",
        "    \"\"\"Main training loop with all components\"\"\"\n",
        "    # Create save directory if it doesn't exist\n",
        "    if not os.path.exists(args.save_dir):\n",
        "        os.makedirs(args.save_dir)\n",
        "\n",
        "    # Get data loaders\n",
        "    train_loader, val_loader = get_data_loaders(args)\n",
        "\n",
        "    # Get model\n",
        "    model = get_model(args)\n",
        "    model = model.cuda()\n",
        "\n",
        "    # Count parameters\n",
        "    num_params = count_parameters(model)\n",
        "    print(f'Model: {args.arch} - Type: {args.model_type}')\n",
        "    print(f'Total parameters: {num_params:,}')\n",
        "\n",
        "    # Ensure we're under the parameter limit\n",
        "    if num_params > 5000000:\n",
        "        print(f'WARNING: Model exceeds 5M parameter limit with {num_params:,} parameters!')\n",
        "\n",
        "    # Set up loss function\n",
        "    if args.label_smoothing > 0:\n",
        "        criterion = LabelSmoothingLoss(classes=10, smoothing=args.label_smoothing).cuda()\n",
        "    else:\n",
        "        criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "    # Set up optimizer and scheduler\n",
        "    optimizer = get_optimizer(args, model)\n",
        "    scheduler = get_scheduler(args, optimizer)\n",
        "\n",
        "    # Set up mixup function if needed\n",
        "    mixup_fn = MixupTransform(alpha=1.0) if args.use_mixup else None\n",
        "\n",
        "    # Track best accuracy\n",
        "    best_prec1 = 0\n",
        "\n",
        "    # Lists to track metrics\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(args.start_epoch, args.epochs):\n",
        "        # Train and validate\n",
        "        print(f'\\nEpoch: {epoch+1}/{args.epochs}')\n",
        "        print(f'Learning rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
        "\n",
        "        train_loss, train_acc = train(train_loader, model, criterion, optimizer, epoch, args, mixup_fn)\n",
        "        val_loss, val_acc = validate(val_loader, model, criterion, args)\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        # Record metrics\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        # Check if this is the best model\n",
        "        is_best = val_acc > best_prec1\n",
        "        best_prec1 = max(val_acc, best_prec1)\n",
        "\n",
        "        # Save checkpoint\n",
        "        if epoch > 0 and epoch % args.save_every == 0 or is_best:\n",
        "            save_checkpoint({\n",
        "                'epoch': epoch + 1,\n",
        "                'arch': args.arch,\n",
        "                'state_dict': model.state_dict(),\n",
        "                'best_prec1': best_prec1,\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "            }, is_best, filename=f'{args.save_dir}/checkpoint_{epoch+1}.pth.tar')\n",
        "\n",
        "    # Save final model\n",
        "    torch.save(model.state_dict(), f'{args.save_dir}/final_model.pth')\n",
        "\n",
        "    # Plot training curves\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Loss plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Training and Validation Loss')\n",
        "\n",
        "    # Accuracy plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_accs, label='Train Accuracy')\n",
        "    plt.plot(val_accs, label='Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{args.save_dir}/training_curves.png')\n",
        "\n",
        "    return model, best_prec1\n",
        "\n",
        "#---------- MAIN EXECUTION ----------#\n",
        "\n",
        "# Set up training parameters\n",
        "args = ResNetParams(\n",
        "    arch='Hybrid-ResNet',\n",
        "    epochs=100,            # Reduced for Colab\n",
        "    batch_size=128,\n",
        "    lr=0.1,\n",
        "    model_type='B',        # Model B (~4.2M parameters)\n",
        "    use_mixup=True,\n",
        "    use_cutout=True,\n",
        "    scheduler='cosine',\n",
        "    save_dir='checkpoints'\n",
        ")\n",
        "\n",
        "# Run the training\n",
        "run_training(args)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your model from checkpoint\n",
        "model = OptimizedResNet_B()\n",
        "checkpoint = torch.load('./checkpoints/model_best.pth.tar')\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "# Count parameters\n",
        "num_params = count_parameters(model)\n",
        "print(f'Total parameters: {num_params:,}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKrIOWMtQok-",
        "outputId": "e2811638-092a-4374-bd80-f793df05fbb3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 2,608,582\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-1275665a4e6b>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load('./checkpoints/model_best.pth.tar')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "#---------- MODEL DEFINITION ----------#\n",
        "\n",
        "class SELayer(nn.Module):\n",
        "    \"\"\"Squeeze-and-Excitation block for channel attention\"\"\"\n",
        "    def __init__(self, channel, reduction=16):\n",
        "        super(SELayer, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channel, channel // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channel // reduction, channel, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    \"\"\"Basic ResNet block with two 3x3 convolutions and a skip connection\"\"\"\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, dropout_rate=0.0, use_se=False):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.dropout = nn.Dropout(dropout_rate) if dropout_rate > 0 else None\n",
        "\n",
        "        # Optional SE block\n",
        "        self.use_se = use_se\n",
        "        if use_se:\n",
        "            self.se = SELayer(planes, reduction=16)\n",
        "\n",
        "        # Skip connection handling\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        if self.dropout is not None:\n",
        "            out = self.dropout(out)\n",
        "        out = self.bn2(self.conv2(out))\n",
        "\n",
        "        if self.use_se:\n",
        "            out = self.se(out)\n",
        "\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class BottleneckBlock(nn.Module):\n",
        "    \"\"\"Bottleneck block with 1x1, 3x3, 1x1 convolutions as shown in the tutorial\"\"\"\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, dropout_rate=0.0, use_se=False):\n",
        "        super(BottleneckBlock, self).__init__()\n",
        "        # First 1x1 conv to reduce channels\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        # 3x3 conv with potential stride\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        # Second 1x1 conv to increase channels (expansion)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_rate) if dropout_rate > 0 else None\n",
        "\n",
        "        # Optional SE block\n",
        "        self.use_se = use_se\n",
        "        if use_se:\n",
        "            self.se = SELayer(planes * self.expansion, reduction=16)\n",
        "\n",
        "        # Skip connection handling\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes * self.expansion:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, planes * self.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * self.expansion)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        if self.dropout is not None:\n",
        "            out = self.dropout(out)\n",
        "        out = self.bn3(self.conv3(out))\n",
        "\n",
        "        if self.use_se:\n",
        "            out = self.se(out)\n",
        "\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class HybridResNet(nn.Module):\n",
        "    \"\"\"Hybrid ResNet combining features from both implementations\"\"\"\n",
        "    def __init__(self, block, layers, num_classes=10, dropout_rate=0.0, use_se=False,\n",
        "                 initial_channels=32, width_multiplier=1.0):\n",
        "        super(HybridResNet, self).__init__()\n",
        "        self.in_planes = int(initial_channels * width_multiplier)\n",
        "\n",
        "        # Initial convolution\n",
        "        self.conv1 = nn.Conv2d(3, self.in_planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(self.in_planes)\n",
        "\n",
        "        # Create stages with progressively more channels\n",
        "        # Adjusted to use width_multiplier for controlled scaling\n",
        "        self.layer1 = self._make_layer(block, int(32 * width_multiplier), layers[0], stride=1,\n",
        "                                     dropout_rate=dropout_rate, use_se=use_se)\n",
        "        self.layer2 = self._make_layer(block, int(64 * width_multiplier), layers[1], stride=2,\n",
        "                                     dropout_rate=dropout_rate, use_se=use_se)\n",
        "        self.layer3 = self._make_layer(block, int(128 * width_multiplier), layers[2], stride=2,\n",
        "                                     dropout_rate=dropout_rate, use_se=use_se)\n",
        "        self.layer4 = self._make_layer(block, int(256 * width_multiplier), layers[3], stride=2,\n",
        "                                     dropout_rate=dropout_rate, use_se=use_se)\n",
        "\n",
        "        # Final classifier\n",
        "        self.dropout = nn.Dropout(dropout_rate) if dropout_rate > 0 else None\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        final_channels = int(256 * width_multiplier) * block.expansion\n",
        "        self.fc = nn.Linear(final_channels, num_classes)\n",
        "\n",
        "        # Initialize weights\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride, dropout_rate=0.0, use_se=False):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride, dropout_rate, use_se))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        if self.dropout is not None:\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Model configurations\n",
        "def ResNet18_Hybrid(num_classes=10, use_se=False, dropout_rate=0.0, width_multiplier=1.0):\n",
        "    \"\"\"ResNet-18 with hybrid features\"\"\"\n",
        "    return HybridResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes,\n",
        "                      dropout_rate=dropout_rate, use_se=use_se, width_multiplier=width_multiplier)\n",
        "\n",
        "def ResNet34_Hybrid(num_classes=10, use_se=False, dropout_rate=0.0, width_multiplier=1.0):\n",
        "    \"\"\"ResNet-34 with hybrid features\"\"\"\n",
        "    return HybridResNet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes,\n",
        "                      dropout_rate=dropout_rate, use_se=use_se, width_multiplier=width_multiplier)\n",
        "\n",
        "def ResNet50_Hybrid(num_classes=10, use_se=False, dropout_rate=0.0, width_multiplier=0.5):\n",
        "    \"\"\"ResNet-50 with hybrid features - scaled down for CIFAR-10\"\"\"\n",
        "    return HybridResNet(BottleneckBlock, [3, 4, 6, 3], num_classes=num_classes,\n",
        "                      dropout_rate=dropout_rate, use_se=use_se, width_multiplier=width_multiplier)\n",
        "\n",
        "# Optimized models specifically designed to be under 5M parameters\n",
        "def OptimizedResNet_A(num_classes=10):\n",
        "    \"\"\"Optimized ResNet with basic blocks and SE attention - around 2.5M parameters\"\"\"\n",
        "    return ResNet18_Hybrid(num_classes=num_classes, use_se=True, dropout_rate=0.1, width_multiplier=0.75)\n",
        "\n",
        "def OptimizedResNet_B(num_classes=10):\n",
        "    \"\"\"Optimized ResNet with basic blocks and focused hyperparameters - around 4.2M parameters\"\"\"\n",
        "    return ResNet34_Hybrid(num_classes=num_classes, use_se=True, dropout_rate=0.15, width_multiplier=0.7)\n",
        "\n",
        "def OptimizedResNet_C(num_classes=10):\n",
        "    \"\"\"Optimized ResNet with bottleneck blocks - most parameter efficient\"\"\"\n",
        "    return ResNet50_Hybrid(num_classes=num_classes, use_se=True, dropout_rate=0.2, width_multiplier=0.25)\n",
        "\n",
        "def project1_model():\n",
        "    \"\"\"Drop-in replacement for the original project1_model function\"\"\"\n",
        "    # Replace with one of the optimized models based on preference\n",
        "    return OptimizedResNet_B(num_classes=10)\n",
        "\n",
        "def count_parameters(model):\n",
        "    \"\"\"Count the total number of trainable parameters in a model\"\"\"\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "#---------- TRAINING UTILITIES ----------#\n",
        "\n",
        "class CutoutTransform:\n",
        "    \"\"\"Cutout data augmentation as shown in the tutorial\"\"\"\n",
        "    def __init__(self, n_holes=1, length=16):\n",
        "        self.n_holes = n_holes\n",
        "        self.length = length\n",
        "\n",
        "    def __call__(self, img):\n",
        "        h = img.size(1)\n",
        "        w = img.size(2)\n",
        "\n",
        "        mask = torch.ones((h, w))\n",
        "\n",
        "        for n in range(self.n_holes):\n",
        "            y = np.random.randint(h)\n",
        "            x = np.random.randint(w)\n",
        "\n",
        "            y1 = np.clip(y - self.length // 2, 0, h)\n",
        "            y2 = np.clip(y + self.length // 2, 0, h)\n",
        "            x1 = np.clip(x - self.length // 2, 0, w)\n",
        "            x2 = np.clip(x + self.length // 2, 0, w)\n",
        "\n",
        "            mask[y1:y2, x1:x2] = 0\n",
        "\n",
        "        mask = mask.expand_as(img)\n",
        "        img = img * mask\n",
        "\n",
        "        return img\n",
        "\n",
        "\n",
        "class MixupTransform:\n",
        "    \"\"\"Mixup augmentation as shown in the tutorial\"\"\"\n",
        "    def __init__(self, alpha=1.0):\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def __call__(self, batch, targets):\n",
        "        if self.alpha > 0:\n",
        "            lam = np.random.beta(self.alpha, self.alpha)\n",
        "        else:\n",
        "            lam = 1\n",
        "\n",
        "        batch_size = batch.size(0)\n",
        "        index = torch.randperm(batch_size).to(batch.device)\n",
        "\n",
        "        mixed_x = lam * batch + (1 - lam) * batch[index, :]\n",
        "        y_a, y_b = targets, targets[index]\n",
        "\n",
        "        return mixed_x, y_a, y_b, lam\n",
        "\n",
        "\n",
        "class LabelSmoothingLoss(nn.Module):\n",
        "    \"\"\"Label smoothing loss for better generalization\"\"\"\n",
        "    def __init__(self, classes, smoothing=0.1):\n",
        "        super(LabelSmoothingLoss, self).__init__()\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.classes = classes\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        pred = pred.log_softmax(-1)\n",
        "        with torch.no_grad():\n",
        "            true_dist = torch.zeros_like(pred)\n",
        "            true_dist.fill_(self.smoothing / (self.classes - 1))\n",
        "            true_dist.scatter_(1, target.unsqueeze(1), self.confidence)\n",
        "        return torch.mean(torch.sum(-true_dist * pred, dim=1))\n",
        "\n",
        "\n",
        "class KeepAverages(object):\n",
        "    \"\"\"Computes and stores the average along with the current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "class ResNetParams:\n",
        "    \"\"\"Hyperparameters for training the ResNet model\"\"\"\n",
        "    def __init__(self, arch='Hybrid-Model', epochs=200, start_epoch=0, batch_size=128,\n",
        "                 lr=0.1, min_lr=1e-4, momentum=0.9, weight_decay=5e-4, print_freq=50,\n",
        "                 save_dir='save_checkpoints', save_every=10, use_mixup=True,\n",
        "                 use_cutout=True, label_smoothing=0.1, model_type='B',\n",
        "                 optimizer='sgd', scheduler='cosine'):\n",
        "        self.save_every = save_every  # Saves checkpoints at specified epochs\n",
        "        self.save_dir = save_dir  # Directory for checkpoints\n",
        "        self.print_freq = print_freq  # Print frequency\n",
        "        self.weight_decay = weight_decay  # Weight decay for optimizer\n",
        "        self.momentum = momentum  # Momentum for SGD\n",
        "        self.lr = lr  # Initial learning rate\n",
        "        self.min_lr = min_lr  # Minimum learning rate for cosine annealing\n",
        "        self.batch_size = batch_size  # Batch size\n",
        "        self.start_epoch = start_epoch  # Starting epoch\n",
        "        self.epochs = epochs  # Total epochs\n",
        "        self.arch = arch  # Model name\n",
        "        self.use_mixup = use_mixup  # Whether to use mixup augmentation\n",
        "        self.use_cutout = use_cutout  # Whether to use cutout augmentation\n",
        "        self.label_smoothing = label_smoothing  # Label smoothing factor\n",
        "        self.model_type = model_type  # Model variant (A, B, or C)\n",
        "        self.optimizer = optimizer  # Optimizer type\n",
        "        self.scheduler = scheduler  # Learning rate scheduler\n",
        "\n",
        "\n",
        "def get_data_loaders(args):\n",
        "    \"\"\"Prepare and return data loaders with augmentations\"\"\"\n",
        "    # CIFAR10 mean and std\n",
        "    cifar_mean = [0.4914, 0.4822, 0.4465]\n",
        "    cifar_std = [0.2023, 0.1994, 0.2010]\n",
        "\n",
        "    # Base transforms for both train and test\n",
        "    transform_base = [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(cifar_mean, cifar_std)\n",
        "    ]\n",
        "\n",
        "    # Enhanced training transforms\n",
        "    transform_train = [\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    ]\n",
        "\n",
        "    # Combine transforms\n",
        "    transform_train.extend(transform_base)\n",
        "\n",
        "    # Add cutout augmentation if enabled\n",
        "    if args.use_cutout:\n",
        "        transform_train.append(CutoutTransform(n_holes=1, length=16))\n",
        "\n",
        "    # Create datasets and loaders\n",
        "    trainset = torchvision.datasets.CIFAR10(\n",
        "        root='./data', train=True, download=True,\n",
        "        transform=transforms.Compose(transform_train)\n",
        "    )\n",
        "\n",
        "    trainloader = DataLoader(\n",
        "        trainset, batch_size=args.batch_size, shuffle=True,\n",
        "        num_workers=2, pin_memory=True\n",
        "    )\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR10(\n",
        "        root='./data', train=False, download=True,\n",
        "        transform=transforms.Compose(transform_base)\n",
        "    )\n",
        "\n",
        "    testloader = DataLoader(\n",
        "        testset, batch_size=args.batch_size, shuffle=False,\n",
        "        num_workers=2, pin_memory=True\n",
        "    )\n",
        "\n",
        "    return trainloader, testloader\n",
        "\n",
        "\n",
        "def get_model(args):\n",
        "    \"\"\"Load the appropriate model based on configuration\"\"\"\n",
        "    if args.model_type == 'A':\n",
        "        model = OptimizedResNet_A()\n",
        "    elif args.model_type == 'B':\n",
        "        model = OptimizedResNet_B()\n",
        "    elif args.model_type == 'C':\n",
        "        model = OptimizedResNet_C()\n",
        "    else:\n",
        "        model = project1_model()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_optimizer(args, model):\n",
        "    \"\"\"Configure optimizer based on args\"\"\"\n",
        "    if args.optimizer == 'sgd':\n",
        "        optimizer = torch.optim.SGD(\n",
        "            model.parameters(),\n",
        "            lr=args.lr,\n",
        "            momentum=args.momentum,\n",
        "            weight_decay=args.weight_decay\n",
        "        )\n",
        "    elif args.optimizer == 'adam':\n",
        "        optimizer = torch.optim.Adam(\n",
        "            model.parameters(),\n",
        "            lr=args.lr,\n",
        "            weight_decay=args.weight_decay\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported optimizer: {args.optimizer}\")\n",
        "\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "def get_scheduler(args, optimizer):\n",
        "    \"\"\"Configure learning rate scheduler based on args\"\"\"\n",
        "    if args.scheduler == 'cosine':\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "            optimizer, T_max=args.epochs, eta_min=args.min_lr\n",
        "        )\n",
        "    elif args.scheduler == 'step':\n",
        "        scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
        "            optimizer, milestones=[int(args.epochs*0.5), int(args.epochs*0.75)], gamma=0.1\n",
        "        )\n",
        "    elif args.scheduler == 'onecycle':\n",
        "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "            optimizer, max_lr=args.lr, total_steps=args.epochs\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported scheduler: {args.scheduler}\")\n",
        "\n",
        "    return scheduler\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Compute the precision@k for the specified values of k\"\"\"\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "\n",
        "    return res\n",
        "\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    \"\"\"Mixup loss function\"\"\"\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, epoch, args, mixup_fn=None):\n",
        "    \"\"\"Training function with support for mixup\"\"\"\n",
        "    # Initialize metrics\n",
        "    batch_time = KeepAverages()\n",
        "    data_time = KeepAverages()\n",
        "    losses = KeepAverages()\n",
        "    top1 = KeepAverages()\n",
        "\n",
        "    # Switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "        # Measure data loading time\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        # Move data to device\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "        # Apply mixup if enabled\n",
        "        if args.use_mixup and mixup_fn is not None:\n",
        "            inputs, targets_a, targets_b, lam = mixup_fn(inputs, targets)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
        "        else:\n",
        "            # Regular forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Measure accuracy and record loss\n",
        "        if not args.use_mixup:  # Skip accuracy during mixup as it's not meaningful\n",
        "            prec1 = accuracy(outputs.data, targets)[0]\n",
        "            top1.update(prec1.item(), inputs.size(0))\n",
        "\n",
        "        losses.update(loss.item(), inputs.size(0))\n",
        "\n",
        "        # Measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        # Print progress\n",
        "        if i % args.print_freq == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
        "                      epoch, i, len(train_loader), batch_time=batch_time,\n",
        "                      data_time=data_time, loss=losses, top1=top1))\n",
        "\n",
        "    return losses.avg, top1.avg\n",
        "\n",
        "\n",
        "def validate(val_loader, model, criterion, args):\n",
        "    \"\"\"Validation function\"\"\"\n",
        "    batch_time = KeepAverages()\n",
        "    losses = KeepAverages()\n",
        "    top1 = KeepAverages()\n",
        "\n",
        "    # Switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        end = time.time()\n",
        "        for i, (inputs, targets) in enumerate(val_loader):\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Measure accuracy and record loss\n",
        "            prec1 = accuracy(outputs.data, targets)[0]\n",
        "            losses.update(loss.item(), inputs.size(0))\n",
        "            top1.update(prec1.item(), inputs.size(0))\n",
        "\n",
        "            # Measure elapsed time\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            # Print progress\n",
        "            if i % args.print_freq == 0:\n",
        "                print('Test: [{0}/{1}]\\t'\n",
        "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                      'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
        "                          i, len(val_loader), batch_time=batch_time,\n",
        "                          loss=losses, top1=top1))\n",
        "\n",
        "    print(' * Prec@1 {top1.avg:.3f}'.format(top1=top1))\n",
        "\n",
        "    return losses.avg, top1.avg\n",
        "\n",
        "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
        "    \"\"\"Save model checkpoint\"\"\"\n",
        "    # Create directory if it doesn't exist\n",
        "    directory = os.path.dirname(filename)\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "    # Save checkpoint\n",
        "    torch.save(state, filename)\n",
        "\n",
        "    if is_best:\n",
        "        best_filename = os.path.join(os.path.dirname(filename), 'model_best.pth.tar')\n",
        "        torch.save(state, best_filename)\n",
        "\n",
        "\n",
        "def run_training(args):\n",
        "    \"\"\"Main training loop with all components\"\"\"\n",
        "    # Create save directory if it doesn't exist\n",
        "    if not os.path.exists(args.save_dir):\n",
        "        os.makedirs(args.save_dir)\n",
        "\n",
        "    # Get data loaders\n",
        "    train_loader, val_loader = get_data_loaders(args)\n",
        "\n",
        "    # Get model\n",
        "    model = get_model(args)\n",
        "    model = model.cuda()\n",
        "\n",
        "    # Count parameters\n",
        "    num_params = count_parameters(model)\n",
        "    print(f'Model: {args.arch} - Type: {args.model_type}')\n",
        "    print(f'Total parameters: {num_params:,}')\n",
        "\n",
        "    # Ensure we're under the parameter limit\n",
        "    if num_params > 5000000:\n",
        "        print(f'WARNING: Model exceeds 5M parameter limit with {num_params:,} parameters!')\n",
        "\n",
        "    # Set up loss function\n",
        "    if args.label_smoothing > 0:\n",
        "        criterion = LabelSmoothingLoss(classes=10, smoothing=args.label_smoothing).cuda()\n",
        "    else:\n",
        "        criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "    # Set up optimizer and scheduler\n",
        "    optimizer = get_optimizer(args, model)\n",
        "    scheduler = get_scheduler(args, optimizer)\n",
        "\n",
        "    # Set up mixup function if needed\n",
        "    mixup_fn = MixupTransform(alpha=1.0) if args.use_mixup else None\n",
        "\n",
        "    # Track best accuracy\n",
        "    best_prec1 = 0\n",
        "\n",
        "    # Lists to track metrics\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(args.start_epoch, args.epochs):\n",
        "        # Train and validate\n",
        "        print(f'\\nEpoch: {epoch+1}/{args.epochs}')\n",
        "        print(f'Learning rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
        "\n",
        "        train_loss, train_acc = train(train_loader, model, criterion, optimizer, epoch, args, mixup_fn)\n",
        "        val_loss, val_acc = validate(val_loader, model, criterion, args)\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        # Record metrics\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        # Check if this is the best model\n",
        "        is_best = val_acc > best_prec1\n",
        "        best_prec1 = max(val_acc, best_prec1)\n",
        "\n",
        "        # Save checkpoint\n",
        "        if epoch > 0 and epoch % args.save_every == 0 or is_best:\n",
        "            save_checkpoint({\n",
        "                'epoch': epoch + 1,\n",
        "                'arch': args.arch,\n",
        "                'state_dict': model.state_dict(),\n",
        "                'best_prec1': best_prec1,\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "            }, is_best, filename=f'{args.save_dir}/checkpoint_{epoch+1}.pth.tar')\n",
        "\n",
        "    # Save final model\n",
        "    torch.save(model.state_dict(), f'{args.save_dir}/final_model.pth')\n",
        "\n",
        "    # Plot training curves\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Loss plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Training and Validation Loss')\n",
        "\n",
        "    # Accuracy plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_accs, label='Train Accuracy')\n",
        "    plt.plot(val_accs, label='Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{args.save_dir}/training_curves.png')\n",
        "\n",
        "    return model, best_prec1\n",
        "\n",
        "#---------- MAIN EXECUTION ----------#\n",
        "\n",
        "# Set up training parameters\n",
        "args = ResNetParams(\n",
        "    arch='Hybrid-ResNet',\n",
        "    epochs=100,            # Reduced for Colab\n",
        "    batch_size=128,\n",
        "    lr=0.1,\n",
        "    model_type='A',        # Model B (~4.2M parameters)\n",
        "    use_mixup=True,\n",
        "    use_cutout=True,\n",
        "    scheduler='cosine',\n",
        "    save_dir='checkpoints'\n",
        ")\n",
        "\n",
        "# Run the training\n",
        "run_training(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "T1hz1G37RUWW",
        "outputId": "105031a8-ce53-4fbd-d201-d6b9c273c25d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Model: Hybrid-ResNet - Type: A\n",
            "Total parameters: 1,587,394\n",
            "\n",
            "Epoch: 1/100\n",
            "Learning rate: 0.100000\n",
            "Epoch: [0][0/391]\tTime 0.445 (0.445)\tData 0.222 (0.222)\tLoss 2.3074 (2.3074)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [0][50/391]\tTime 0.113 (0.092)\tData 0.054 (0.036)\tLoss 2.1997 (2.2916)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [0][100/391]\tTime 0.179 (0.096)\tData 0.125 (0.042)\tLoss 2.0303 (2.2207)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [0][150/391]\tTime 0.127 (0.097)\tData 0.088 (0.043)\tLoss 1.9848 (2.1812)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [0][200/391]\tTime 0.051 (0.094)\tData 0.005 (0.043)\tLoss 2.1890 (2.1615)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [0][250/391]\tTime 0.054 (0.097)\tData 0.004 (0.046)\tLoss 2.0501 (2.1399)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [0][300/391]\tTime 0.122 (0.096)\tData 0.078 (0.044)\tLoss 1.8945 (2.1241)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [0][350/391]\tTime 0.143 (0.095)\tData 0.105 (0.044)\tLoss 1.9435 (2.1095)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.125 (0.125)\tLoss 1.7024 (1.7024)\tPrec@1 48.438 (48.438)\n",
            "Test: [50/79]\tTime 0.019 (0.030)\tLoss 1.7960 (1.7081)\tPrec@1 35.156 (44.240)\n",
            " * Prec@1 43.930\n",
            "\n",
            "Epoch: 2/100\n",
            "Learning rate: 0.099975\n",
            "Epoch: [1][0/391]\tTime 0.285 (0.285)\tData 0.216 (0.216)\tLoss 2.0919 (2.0919)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [1][50/391]\tTime 0.115 (0.093)\tData 0.076 (0.044)\tLoss 2.0076 (2.0142)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [1][100/391]\tTime 0.134 (0.099)\tData 0.095 (0.049)\tLoss 1.9401 (1.9964)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [1][150/391]\tTime 0.098 (0.095)\tData 0.053 (0.046)\tLoss 1.8734 (1.9911)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [1][200/391]\tTime 0.086 (0.093)\tData 0.028 (0.043)\tLoss 2.0963 (1.9865)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [1][250/391]\tTime 0.062 (0.096)\tData 0.006 (0.045)\tLoss 1.9965 (1.9832)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [1][300/391]\tTime 0.132 (0.094)\tData 0.093 (0.044)\tLoss 2.0597 (1.9764)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [1][350/391]\tTime 0.221 (0.095)\tData 0.173 (0.044)\tLoss 2.0895 (1.9679)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.139 (0.139)\tLoss 1.5696 (1.5696)\tPrec@1 51.562 (51.562)\n",
            "Test: [50/79]\tTime 0.009 (0.030)\tLoss 1.5659 (1.5291)\tPrec@1 53.906 (53.217)\n",
            " * Prec@1 52.720\n",
            "\n",
            "Epoch: 3/100\n",
            "Learning rate: 0.099901\n",
            "Epoch: [2][0/391]\tTime 0.307 (0.307)\tData 0.231 (0.231)\tLoss 1.9848 (1.9848)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [2][50/391]\tTime 0.061 (0.092)\tData 0.000 (0.038)\tLoss 1.7794 (1.9105)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [2][100/391]\tTime 0.062 (0.100)\tData 0.006 (0.046)\tLoss 1.8091 (1.8966)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [2][150/391]\tTime 0.072 (0.097)\tData 0.003 (0.044)\tLoss 1.9969 (1.8911)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [2][200/391]\tTime 0.091 (0.100)\tData 0.000 (0.046)\tLoss 1.9200 (1.8823)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [2][250/391]\tTime 0.043 (0.099)\tData 0.000 (0.047)\tLoss 1.6076 (1.8804)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [2][300/391]\tTime 0.081 (0.098)\tData 0.030 (0.046)\tLoss 1.7600 (1.8640)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [2][350/391]\tTime 0.051 (0.099)\tData 0.000 (0.047)\tLoss 1.6441 (1.8629)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.130 (0.130)\tLoss 1.4631 (1.4631)\tPrec@1 54.688 (54.688)\n",
            "Test: [50/79]\tTime 0.027 (0.030)\tLoss 1.5875 (1.4705)\tPrec@1 51.562 (58.379)\n",
            " * Prec@1 58.470\n",
            "\n",
            "Epoch: 4/100\n",
            "Learning rate: 0.099778\n",
            "Epoch: [3][0/391]\tTime 0.294 (0.294)\tData 0.223 (0.223)\tLoss 1.9981 (1.9981)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [3][50/391]\tTime 0.131 (0.111)\tData 0.090 (0.055)\tLoss 1.8224 (1.8250)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [3][100/391]\tTime 0.095 (0.100)\tData 0.044 (0.048)\tLoss 1.7206 (1.8413)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [3][150/391]\tTime 0.059 (0.096)\tData 0.005 (0.045)\tLoss 1.9184 (1.8316)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [3][200/391]\tTime 0.042 (0.098)\tData 0.000 (0.048)\tLoss 1.8698 (1.8307)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [3][250/391]\tTime 0.050 (0.096)\tData 0.006 (0.047)\tLoss 1.7498 (1.8167)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [3][300/391]\tTime 0.059 (0.097)\tData 0.005 (0.048)\tLoss 1.9224 (1.8162)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [3][350/391]\tTime 0.135 (0.097)\tData 0.091 (0.048)\tLoss 1.9652 (1.8166)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.136 (0.136)\tLoss 1.3814 (1.3814)\tPrec@1 60.938 (60.938)\n",
            "Test: [50/79]\tTime 0.068 (0.030)\tLoss 1.5597 (1.4672)\tPrec@1 55.469 (58.517)\n",
            " * Prec@1 58.360\n",
            "\n",
            "Epoch: 5/100\n",
            "Learning rate: 0.099606\n",
            "Epoch: [4][0/391]\tTime 0.295 (0.295)\tData 0.216 (0.216)\tLoss 1.6629 (1.6629)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [4][50/391]\tTime 0.139 (0.110)\tData 0.086 (0.053)\tLoss 2.0181 (1.7889)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [4][100/391]\tTime 0.041 (0.098)\tData 0.000 (0.044)\tLoss 1.7684 (1.7856)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [4][150/391]\tTime 0.075 (0.098)\tData 0.003 (0.044)\tLoss 1.5903 (1.7840)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [4][200/391]\tTime 0.134 (0.098)\tData 0.095 (0.046)\tLoss 1.8004 (1.7852)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [4][250/391]\tTime 0.061 (0.097)\tData 0.000 (0.045)\tLoss 1.6168 (1.7846)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [4][300/391]\tTime 0.092 (0.098)\tData 0.027 (0.046)\tLoss 1.8380 (1.7817)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [4][350/391]\tTime 0.143 (0.096)\tData 0.096 (0.046)\tLoss 1.6943 (1.7748)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.136 (0.136)\tLoss 1.2726 (1.2726)\tPrec@1 61.719 (61.719)\n",
            "Test: [50/79]\tTime 0.055 (0.029)\tLoss 1.3240 (1.2982)\tPrec@1 67.188 (65.288)\n",
            " * Prec@1 65.790\n",
            "\n",
            "Epoch: 6/100\n",
            "Learning rate: 0.099385\n",
            "Epoch: [5][0/391]\tTime 0.497 (0.497)\tData 0.418 (0.418)\tLoss 1.3802 (1.3802)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [5][50/391]\tTime 0.092 (0.099)\tData 0.051 (0.050)\tLoss 1.6226 (1.7107)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [5][100/391]\tTime 0.053 (0.093)\tData 0.007 (0.044)\tLoss 1.5669 (1.7344)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [5][150/391]\tTime 0.060 (0.098)\tData 0.004 (0.047)\tLoss 1.3841 (1.7263)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [5][200/391]\tTime 0.124 (0.095)\tData 0.081 (0.045)\tLoss 1.7279 (1.7214)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [5][250/391]\tTime 0.056 (0.093)\tData 0.000 (0.043)\tLoss 1.9323 (1.7172)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [5][300/391]\tTime 0.040 (0.095)\tData 0.000 (0.045)\tLoss 1.6118 (1.7231)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [5][350/391]\tTime 0.132 (0.094)\tData 0.076 (0.044)\tLoss 1.8864 (1.7208)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.127 (0.127)\tLoss 1.2471 (1.2471)\tPrec@1 71.094 (71.094)\n",
            "Test: [50/79]\tTime 0.028 (0.044)\tLoss 1.3001 (1.2663)\tPrec@1 66.406 (68.107)\n",
            " * Prec@1 68.510\n",
            "\n",
            "Epoch: 7/100\n",
            "Learning rate: 0.099115\n",
            "Epoch: [6][0/391]\tTime 0.292 (0.292)\tData 0.221 (0.221)\tLoss 1.6626 (1.6626)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [6][50/391]\tTime 0.135 (0.093)\tData 0.096 (0.046)\tLoss 1.5102 (1.6958)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [6][100/391]\tTime 0.127 (0.091)\tData 0.076 (0.046)\tLoss 1.4839 (1.6983)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [6][150/391]\tTime 0.062 (0.095)\tData 0.004 (0.047)\tLoss 1.9607 (1.7237)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [6][200/391]\tTime 0.122 (0.093)\tData 0.083 (0.045)\tLoss 1.9092 (1.7129)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [6][250/391]\tTime 0.127 (0.094)\tData 0.049 (0.046)\tLoss 1.7246 (1.7171)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [6][300/391]\tTime 0.139 (0.094)\tData 0.082 (0.045)\tLoss 1.5398 (1.7217)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [6][350/391]\tTime 0.119 (0.093)\tData 0.075 (0.044)\tLoss 1.6426 (1.7153)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.202 (0.202)\tLoss 1.2043 (1.2043)\tPrec@1 70.312 (70.312)\n",
            "Test: [50/79]\tTime 0.044 (0.031)\tLoss 1.2608 (1.2218)\tPrec@1 71.875 (70.558)\n",
            " * Prec@1 70.650\n",
            "\n",
            "Epoch: 8/100\n",
            "Learning rate: 0.098797\n",
            "Epoch: [7][0/391]\tTime 0.290 (0.290)\tData 0.215 (0.215)\tLoss 1.2333 (1.2333)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [7][50/391]\tTime 0.148 (0.090)\tData 0.096 (0.038)\tLoss 1.9181 (1.6701)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [7][100/391]\tTime 0.112 (0.093)\tData 0.067 (0.042)\tLoss 1.5854 (1.7022)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [7][150/391]\tTime 0.139 (0.094)\tData 0.099 (0.044)\tLoss 1.4172 (1.6716)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [7][200/391]\tTime 0.135 (0.092)\tData 0.076 (0.043)\tLoss 1.3724 (1.6790)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [7][250/391]\tTime 0.043 (0.095)\tData 0.000 (0.045)\tLoss 1.9747 (1.6658)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [7][300/391]\tTime 0.098 (0.094)\tData 0.049 (0.043)\tLoss 1.3460 (1.6670)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [7][350/391]\tTime 0.123 (0.093)\tData 0.077 (0.043)\tLoss 1.7002 (1.6722)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.137 (0.137)\tLoss 1.2528 (1.2528)\tPrec@1 71.094 (71.094)\n",
            "Test: [50/79]\tTime 0.021 (0.029)\tLoss 1.2956 (1.2812)\tPrec@1 68.750 (68.704)\n",
            " * Prec@1 69.310\n",
            "\n",
            "Epoch: 9/100\n",
            "Learning rate: 0.098431\n",
            "Epoch: [8][0/391]\tTime 0.276 (0.276)\tData 0.218 (0.218)\tLoss 1.7882 (1.7882)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [8][50/391]\tTime 0.053 (0.089)\tData 0.000 (0.038)\tLoss 1.9059 (1.6679)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [8][100/391]\tTime 0.102 (0.097)\tData 0.049 (0.045)\tLoss 1.9288 (1.6863)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [8][150/391]\tTime 0.145 (0.094)\tData 0.093 (0.043)\tLoss 1.9331 (1.6887)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [8][200/391]\tTime 0.054 (0.092)\tData 0.000 (0.041)\tLoss 1.7964 (1.6913)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [8][250/391]\tTime 0.040 (0.094)\tData 0.000 (0.044)\tLoss 1.8458 (1.6968)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [8][300/391]\tTime 0.137 (0.093)\tData 0.094 (0.044)\tLoss 1.2838 (1.6874)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [8][350/391]\tTime 0.217 (0.094)\tData 0.163 (0.045)\tLoss 1.4156 (1.6901)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.129 (0.129)\tLoss 1.1175 (1.1175)\tPrec@1 78.125 (78.125)\n",
            "Test: [50/79]\tTime 0.032 (0.029)\tLoss 1.1956 (1.1579)\tPrec@1 70.312 (72.044)\n",
            " * Prec@1 72.170\n",
            "\n",
            "Epoch: 10/100\n",
            "Learning rate: 0.098017\n",
            "Epoch: [9][0/391]\tTime 0.286 (0.286)\tData 0.216 (0.216)\tLoss 1.8556 (1.8556)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [9][50/391]\tTime 0.072 (0.089)\tData 0.019 (0.037)\tLoss 1.3837 (1.6692)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [9][100/391]\tTime 0.057 (0.097)\tData 0.016 (0.044)\tLoss 1.7866 (1.6958)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [9][150/391]\tTime 0.053 (0.094)\tData 0.000 (0.045)\tLoss 1.5637 (1.7027)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [9][200/391]\tTime 0.053 (0.094)\tData 0.000 (0.045)\tLoss 2.0052 (1.6949)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [9][250/391]\tTime 0.111 (0.095)\tData 0.060 (0.046)\tLoss 1.2619 (1.6852)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [9][300/391]\tTime 0.117 (0.094)\tData 0.068 (0.045)\tLoss 1.9401 (1.6823)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [9][350/391]\tTime 0.132 (0.095)\tData 0.090 (0.046)\tLoss 1.5635 (1.6825)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.141 (0.141)\tLoss 1.2425 (1.2425)\tPrec@1 71.875 (71.875)\n",
            "Test: [50/79]\tTime 0.039 (0.029)\tLoss 1.3505 (1.2586)\tPrec@1 67.188 (69.899)\n",
            " * Prec@1 70.120\n",
            "\n",
            "Epoch: 11/100\n",
            "Learning rate: 0.097555\n",
            "Epoch: [10][0/391]\tTime 0.290 (0.290)\tData 0.217 (0.217)\tLoss 1.8897 (1.8897)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [10][50/391]\tTime 0.315 (0.096)\tData 0.265 (0.044)\tLoss 1.6861 (1.6408)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [10][100/391]\tTime 0.040 (0.097)\tData 0.000 (0.045)\tLoss 1.3315 (1.6627)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [10][150/391]\tTime 0.041 (0.094)\tData 0.000 (0.043)\tLoss 1.6991 (1.6628)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [10][200/391]\tTime 0.151 (0.096)\tData 0.099 (0.045)\tLoss 1.8200 (1.6677)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [10][250/391]\tTime 0.115 (0.094)\tData 0.062 (0.044)\tLoss 1.6816 (1.6610)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [10][300/391]\tTime 0.116 (0.093)\tData 0.059 (0.042)\tLoss 1.3542 (1.6530)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [10][350/391]\tTime 0.087 (0.095)\tData 0.048 (0.044)\tLoss 1.7744 (1.6586)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.137 (0.137)\tLoss 1.0620 (1.0620)\tPrec@1 76.562 (76.562)\n",
            "Test: [50/79]\tTime 0.033 (0.030)\tLoss 1.1368 (1.1123)\tPrec@1 76.562 (75.797)\n",
            " * Prec@1 76.010\n",
            "\n",
            "Epoch: 12/100\n",
            "Learning rate: 0.097047\n",
            "Epoch: [11][0/391]\tTime 0.275 (0.275)\tData 0.209 (0.209)\tLoss 1.8375 (1.8375)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [11][50/391]\tTime 0.224 (0.108)\tData 0.183 (0.055)\tLoss 1.7827 (1.6535)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [11][100/391]\tTime 0.083 (0.096)\tData 0.031 (0.046)\tLoss 1.8825 (1.6195)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [11][150/391]\tTime 0.121 (0.093)\tData 0.082 (0.043)\tLoss 1.6957 (1.6382)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [11][200/391]\tTime 0.142 (0.096)\tData 0.101 (0.046)\tLoss 1.3353 (1.6398)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [11][250/391]\tTime 0.122 (0.094)\tData 0.070 (0.045)\tLoss 1.4007 (1.6435)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [11][300/391]\tTime 0.097 (0.093)\tData 0.051 (0.043)\tLoss 1.4947 (1.6377)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [11][350/391]\tTime 0.051 (0.095)\tData 0.005 (0.045)\tLoss 1.6509 (1.6411)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.135 (0.135)\tLoss 1.2210 (1.2210)\tPrec@1 69.531 (69.531)\n",
            "Test: [50/79]\tTime 0.041 (0.030)\tLoss 1.2655 (1.2788)\tPrec@1 64.062 (67.310)\n",
            " * Prec@1 67.710\n",
            "\n",
            "Epoch: 13/100\n",
            "Learning rate: 0.096492\n",
            "Epoch: [12][0/391]\tTime 0.287 (0.287)\tData 0.215 (0.215)\tLoss 1.8552 (1.8552)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [12][50/391]\tTime 0.089 (0.108)\tData 0.038 (0.052)\tLoss 1.7512 (1.6036)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [12][100/391]\tTime 0.073 (0.098)\tData 0.019 (0.045)\tLoss 1.8671 (1.6434)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [12][150/391]\tTime 0.134 (0.094)\tData 0.082 (0.042)\tLoss 1.6530 (1.6560)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [12][200/391]\tTime 0.075 (0.097)\tData 0.021 (0.044)\tLoss 1.7278 (1.6501)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [12][250/391]\tTime 0.056 (0.095)\tData 0.000 (0.043)\tLoss 1.2578 (1.6536)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [12][300/391]\tTime 0.053 (0.095)\tData 0.000 (0.044)\tLoss 1.8928 (1.6521)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [12][350/391]\tTime 0.040 (0.095)\tData 0.000 (0.044)\tLoss 1.8470 (1.6566)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.138 (0.138)\tLoss 1.1164 (1.1164)\tPrec@1 73.438 (73.438)\n",
            "Test: [50/79]\tTime 0.034 (0.030)\tLoss 1.2435 (1.1519)\tPrec@1 69.531 (74.326)\n",
            " * Prec@1 73.940\n",
            "\n",
            "Epoch: 14/100\n",
            "Learning rate: 0.095892\n",
            "Epoch: [13][0/391]\tTime 0.286 (0.286)\tData 0.219 (0.219)\tLoss 1.8976 (1.8976)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [13][50/391]\tTime 0.063 (0.108)\tData 0.012 (0.052)\tLoss 1.8269 (1.6900)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [13][100/391]\tTime 0.096 (0.097)\tData 0.037 (0.044)\tLoss 1.7110 (1.6665)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [13][150/391]\tTime 0.110 (0.099)\tData 0.054 (0.046)\tLoss 1.8319 (1.6467)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [13][200/391]\tTime 0.135 (0.097)\tData 0.077 (0.045)\tLoss 1.4709 (1.6370)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [13][250/391]\tTime 0.133 (0.096)\tData 0.092 (0.044)\tLoss 1.5582 (1.6492)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [13][300/391]\tTime 0.088 (0.098)\tData 0.048 (0.046)\tLoss 1.6526 (1.6553)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [13][350/391]\tTime 0.042 (0.096)\tData 0.000 (0.045)\tLoss 1.2795 (1.6567)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.134 (0.134)\tLoss 1.2493 (1.2493)\tPrec@1 71.875 (71.875)\n",
            "Test: [50/79]\tTime 0.078 (0.034)\tLoss 1.2496 (1.2625)\tPrec@1 73.438 (71.140)\n",
            " * Prec@1 71.540\n",
            "\n",
            "Epoch: 15/100\n",
            "Learning rate: 0.095246\n",
            "Epoch: [14][0/391]\tTime 0.430 (0.430)\tData 0.340 (0.340)\tLoss 1.6157 (1.6157)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [14][50/391]\tTime 0.144 (0.099)\tData 0.095 (0.048)\tLoss 1.7509 (1.6495)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [14][100/391]\tTime 0.119 (0.093)\tData 0.075 (0.042)\tLoss 1.8509 (1.6328)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [14][150/391]\tTime 0.145 (0.098)\tData 0.093 (0.047)\tLoss 1.2961 (1.6401)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [14][200/391]\tTime 0.157 (0.095)\tData 0.118 (0.045)\tLoss 1.4150 (1.6389)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [14][250/391]\tTime 0.133 (0.094)\tData 0.094 (0.044)\tLoss 1.2061 (1.6390)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [14][300/391]\tTime 0.140 (0.096)\tData 0.100 (0.047)\tLoss 1.6250 (1.6444)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [14][350/391]\tTime 0.130 (0.095)\tData 0.080 (0.047)\tLoss 1.8790 (1.6426)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.149 (0.149)\tLoss 1.1918 (1.1918)\tPrec@1 70.312 (70.312)\n",
            "Test: [50/79]\tTime 0.011 (0.042)\tLoss 1.1921 (1.1916)\tPrec@1 75.781 (72.273)\n",
            " * Prec@1 72.900\n",
            "\n",
            "Epoch: 16/100\n",
            "Learning rate: 0.094556\n",
            "Epoch: [15][0/391]\tTime 0.291 (0.291)\tData 0.213 (0.213)\tLoss 1.4223 (1.4223)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [15][50/391]\tTime 0.055 (0.089)\tData 0.000 (0.036)\tLoss 1.3759 (1.6296)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [15][100/391]\tTime 0.048 (0.088)\tData 0.000 (0.036)\tLoss 1.5460 (1.6202)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [15][150/391]\tTime 0.052 (0.094)\tData 0.000 (0.041)\tLoss 1.5843 (1.6275)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [15][200/391]\tTime 0.059 (0.093)\tData 0.000 (0.042)\tLoss 1.7982 (1.6314)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [15][250/391]\tTime 0.104 (0.094)\tData 0.054 (0.043)\tLoss 1.3415 (1.6248)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [15][300/391]\tTime 0.133 (0.094)\tData 0.094 (0.044)\tLoss 1.2974 (1.6300)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [15][350/391]\tTime 0.145 (0.093)\tData 0.092 (0.044)\tLoss 1.6874 (1.6298)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.125 (0.125)\tLoss 1.1813 (1.1813)\tPrec@1 75.781 (75.781)\n",
            "Test: [50/79]\tTime 0.025 (0.029)\tLoss 1.3160 (1.2324)\tPrec@1 70.312 (71.247)\n",
            " * Prec@1 71.460\n",
            "\n",
            "Epoch: 17/100\n",
            "Learning rate: 0.093822\n",
            "Epoch: [16][0/391]\tTime 0.296 (0.296)\tData 0.231 (0.231)\tLoss 1.3788 (1.3788)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [16][50/391]\tTime 0.090 (0.090)\tData 0.040 (0.040)\tLoss 1.5807 (1.6405)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [16][100/391]\tTime 0.074 (0.094)\tData 0.013 (0.042)\tLoss 1.9617 (1.6404)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [16][150/391]\tTime 0.058 (0.094)\tData 0.008 (0.044)\tLoss 1.3866 (1.6332)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [16][200/391]\tTime 0.135 (0.093)\tData 0.096 (0.044)\tLoss 1.2548 (1.6274)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [16][250/391]\tTime 0.112 (0.095)\tData 0.059 (0.046)\tLoss 1.7406 (1.6257)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [16][300/391]\tTime 0.149 (0.094)\tData 0.101 (0.044)\tLoss 1.7584 (1.6311)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [16][350/391]\tTime 0.041 (0.093)\tData 0.000 (0.044)\tLoss 1.8998 (1.6296)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.134 (0.134)\tLoss 1.0816 (1.0816)\tPrec@1 75.781 (75.781)\n",
            "Test: [50/79]\tTime 0.018 (0.029)\tLoss 1.2114 (1.1326)\tPrec@1 73.438 (74.433)\n",
            " * Prec@1 74.110\n",
            "\n",
            "Epoch: 18/100\n",
            "Learning rate: 0.093044\n",
            "Epoch: [17][0/391]\tTime 0.283 (0.283)\tData 0.217 (0.217)\tLoss 1.9222 (1.9222)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [17][50/391]\tTime 0.096 (0.089)\tData 0.042 (0.038)\tLoss 1.6373 (1.6267)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [17][100/391]\tTime 0.130 (0.097)\tData 0.073 (0.044)\tLoss 1.8683 (1.6410)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [17][150/391]\tTime 0.101 (0.093)\tData 0.039 (0.042)\tLoss 1.6867 (1.6412)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [17][200/391]\tTime 0.076 (0.091)\tData 0.028 (0.040)\tLoss 1.7402 (1.6216)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [17][250/391]\tTime 0.064 (0.095)\tData 0.000 (0.044)\tLoss 1.7815 (1.6309)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [17][300/391]\tTime 0.090 (0.093)\tData 0.043 (0.042)\tLoss 1.3076 (1.6245)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [17][350/391]\tTime 0.055 (0.094)\tData 0.004 (0.043)\tLoss 1.4640 (1.6271)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.131 (0.131)\tLoss 1.4517 (1.4517)\tPrec@1 59.375 (59.375)\n",
            "Test: [50/79]\tTime 0.037 (0.029)\tLoss 1.5736 (1.4439)\tPrec@1 51.562 (58.869)\n",
            " * Prec@1 59.030\n",
            "\n",
            "Epoch: 19/100\n",
            "Learning rate: 0.092224\n",
            "Epoch: [18][0/391]\tTime 0.288 (0.288)\tData 0.223 (0.223)\tLoss 1.2794 (1.2794)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [18][50/391]\tTime 0.110 (0.089)\tData 0.071 (0.038)\tLoss 1.4939 (1.6466)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [18][100/391]\tTime 0.091 (0.096)\tData 0.051 (0.044)\tLoss 1.5683 (1.6177)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [18][150/391]\tTime 0.062 (0.094)\tData 0.015 (0.043)\tLoss 1.6383 (1.6170)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [18][200/391]\tTime 0.069 (0.095)\tData 0.004 (0.044)\tLoss 1.7548 (1.6238)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [18][250/391]\tTime 0.040 (0.095)\tData 0.000 (0.045)\tLoss 1.7114 (1.6165)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [18][300/391]\tTime 0.040 (0.094)\tData 0.000 (0.045)\tLoss 1.7460 (1.6262)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [18][350/391]\tTime 0.099 (0.095)\tData 0.049 (0.046)\tLoss 1.4372 (1.6194)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.153 (0.153)\tLoss 1.0991 (1.0991)\tPrec@1 77.344 (77.344)\n",
            "Test: [50/79]\tTime 0.026 (0.030)\tLoss 1.1605 (1.1321)\tPrec@1 74.219 (74.249)\n",
            " * Prec@1 74.240\n",
            "\n",
            "Epoch: 20/100\n",
            "Learning rate: 0.091363\n",
            "Epoch: [19][0/391]\tTime 0.265 (0.265)\tData 0.211 (0.211)\tLoss 1.2917 (1.2917)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [19][50/391]\tTime 0.192 (0.101)\tData 0.121 (0.049)\tLoss 1.6615 (1.6685)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [19][100/391]\tTime 0.133 (0.098)\tData 0.094 (0.047)\tLoss 1.8264 (1.6728)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [19][150/391]\tTime 0.135 (0.095)\tData 0.096 (0.046)\tLoss 1.8618 (1.6444)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [19][200/391]\tTime 0.132 (0.098)\tData 0.094 (0.049)\tLoss 1.8046 (1.6359)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [19][250/391]\tTime 0.102 (0.095)\tData 0.058 (0.046)\tLoss 1.8153 (1.6409)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [19][300/391]\tTime 0.132 (0.094)\tData 0.091 (0.046)\tLoss 1.4668 (1.6401)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [19][350/391]\tTime 0.103 (0.095)\tData 0.058 (0.047)\tLoss 1.2777 (1.6334)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.134 (0.134)\tLoss 1.1468 (1.1468)\tPrec@1 74.219 (74.219)\n",
            "Test: [50/79]\tTime 0.043 (0.029)\tLoss 1.1599 (1.1394)\tPrec@1 73.438 (75.429)\n",
            " * Prec@1 75.740\n",
            "\n",
            "Epoch: 21/100\n",
            "Learning rate: 0.090460\n",
            "Epoch: [20][0/391]\tTime 0.276 (0.276)\tData 0.207 (0.207)\tLoss 1.7739 (1.7739)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [20][50/391]\tTime 0.129 (0.107)\tData 0.083 (0.055)\tLoss 1.4595 (1.6191)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [20][100/391]\tTime 0.116 (0.096)\tData 0.073 (0.045)\tLoss 1.3305 (1.5939)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [20][150/391]\tTime 0.108 (0.092)\tData 0.067 (0.043)\tLoss 1.8158 (1.5945)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [20][200/391]\tTime 0.058 (0.095)\tData 0.000 (0.045)\tLoss 1.4502 (1.5932)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [20][250/391]\tTime 0.053 (0.093)\tData 0.000 (0.044)\tLoss 1.5498 (1.5984)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [20][300/391]\tTime 0.068 (0.093)\tData 0.011 (0.044)\tLoss 1.9934 (1.5955)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [20][350/391]\tTime 0.057 (0.094)\tData 0.000 (0.045)\tLoss 1.8082 (1.6069)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.133 (0.133)\tLoss 1.0996 (1.0996)\tPrec@1 80.469 (80.469)\n",
            "Test: [50/79]\tTime 0.043 (0.030)\tLoss 1.2023 (1.1216)\tPrec@1 73.438 (77.053)\n",
            " * Prec@1 77.160\n",
            "\n",
            "Epoch: 22/100\n",
            "Learning rate: 0.089518\n",
            "Epoch: [21][0/391]\tTime 0.285 (0.285)\tData 0.216 (0.216)\tLoss 1.1956 (1.1956)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [21][50/391]\tTime 0.133 (0.110)\tData 0.091 (0.060)\tLoss 1.8495 (1.6246)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [21][100/391]\tTime 0.142 (0.099)\tData 0.093 (0.052)\tLoss 1.8438 (1.6532)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [21][150/391]\tTime 0.177 (0.096)\tData 0.123 (0.050)\tLoss 1.8588 (1.6332)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [21][200/391]\tTime 0.100 (0.098)\tData 0.051 (0.050)\tLoss 1.7485 (1.6278)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [21][250/391]\tTime 0.050 (0.095)\tData 0.000 (0.048)\tLoss 1.6296 (1.6192)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [21][300/391]\tTime 0.054 (0.097)\tData 0.000 (0.048)\tLoss 1.4437 (1.6167)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [21][350/391]\tTime 0.057 (0.095)\tData 0.003 (0.047)\tLoss 1.8201 (1.6201)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.134 (0.134)\tLoss 1.1204 (1.1204)\tPrec@1 76.562 (76.562)\n",
            "Test: [50/79]\tTime 0.038 (0.030)\tLoss 1.1553 (1.1330)\tPrec@1 74.219 (75.735)\n",
            " * Prec@1 75.990\n",
            "\n",
            "Epoch: 23/100\n",
            "Learning rate: 0.088537\n",
            "Epoch: [22][0/391]\tTime 0.447 (0.447)\tData 0.287 (0.287)\tLoss 1.4428 (1.4428)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [22][50/391]\tTime 0.062 (0.105)\tData 0.005 (0.052)\tLoss 1.8605 (1.6411)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [22][100/391]\tTime 0.124 (0.096)\tData 0.080 (0.045)\tLoss 1.8168 (1.6252)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [22][150/391]\tTime 0.122 (0.100)\tData 0.063 (0.049)\tLoss 1.8565 (1.6112)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [22][200/391]\tTime 0.096 (0.096)\tData 0.051 (0.045)\tLoss 1.5089 (1.6064)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [22][250/391]\tTime 0.040 (0.094)\tData 0.000 (0.044)\tLoss 1.5589 (1.6176)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [22][300/391]\tTime 0.113 (0.096)\tData 0.058 (0.045)\tLoss 1.3533 (1.6116)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [22][350/391]\tTime 0.054 (0.095)\tData 0.006 (0.044)\tLoss 1.7239 (1.6155)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.141 (0.141)\tLoss 1.1461 (1.1461)\tPrec@1 75.000 (75.000)\n",
            "Test: [50/79]\tTime 0.018 (0.037)\tLoss 1.2103 (1.2035)\tPrec@1 70.312 (71.446)\n",
            " * Prec@1 71.440\n",
            "\n",
            "Epoch: 24/100\n",
            "Learning rate: 0.087518\n",
            "Epoch: [23][0/391]\tTime 0.447 (0.447)\tData 0.314 (0.314)\tLoss 1.3771 (1.3771)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [23][50/391]\tTime 0.121 (0.094)\tData 0.075 (0.040)\tLoss 1.7775 (1.6560)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [23][100/391]\tTime 0.139 (0.090)\tData 0.100 (0.039)\tLoss 1.8861 (1.6532)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [23][150/391]\tTime 0.149 (0.096)\tData 0.110 (0.045)\tLoss 1.8385 (1.6425)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [23][200/391]\tTime 0.107 (0.094)\tData 0.056 (0.045)\tLoss 1.3264 (1.6535)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [23][250/391]\tTime 0.072 (0.092)\tData 0.020 (0.043)\tLoss 1.7118 (1.6475)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [23][300/391]\tTime 0.040 (0.095)\tData 0.000 (0.045)\tLoss 1.4474 (1.6441)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [23][350/391]\tTime 0.138 (0.094)\tData 0.087 (0.045)\tLoss 1.5937 (1.6458)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.201 (0.201)\tLoss 1.0909 (1.0909)\tPrec@1 77.344 (77.344)\n",
            "Test: [50/79]\tTime 0.028 (0.039)\tLoss 1.1497 (1.1260)\tPrec@1 74.219 (75.888)\n",
            " * Prec@1 76.380\n",
            "\n",
            "Epoch: 25/100\n",
            "Learning rate: 0.086462\n",
            "Epoch: [24][0/391]\tTime 0.291 (0.291)\tData 0.215 (0.215)\tLoss 1.5806 (1.5806)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [24][50/391]\tTime 0.144 (0.090)\tData 0.086 (0.043)\tLoss 1.5389 (1.6691)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [24][100/391]\tTime 0.149 (0.090)\tData 0.093 (0.043)\tLoss 1.7783 (1.6497)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [24][150/391]\tTime 0.040 (0.094)\tData 0.000 (0.047)\tLoss 1.6006 (1.6312)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [24][200/391]\tTime 0.042 (0.093)\tData 0.000 (0.046)\tLoss 1.5718 (1.6212)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [24][250/391]\tTime 0.063 (0.095)\tData 0.002 (0.047)\tLoss 1.5770 (1.6197)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [24][300/391]\tTime 0.056 (0.094)\tData 0.017 (0.047)\tLoss 1.3297 (1.6266)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [24][350/391]\tTime 0.041 (0.093)\tData 0.000 (0.046)\tLoss 1.7895 (1.6260)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.127 (0.127)\tLoss 1.1436 (1.1436)\tPrec@1 75.781 (75.781)\n",
            "Test: [50/79]\tTime 0.038 (0.029)\tLoss 1.1449 (1.1105)\tPrec@1 73.438 (76.440)\n",
            " * Prec@1 76.730\n",
            "\n",
            "Epoch: 26/100\n",
            "Learning rate: 0.085370\n",
            "Epoch: [25][0/391]\tTime 0.284 (0.284)\tData 0.215 (0.215)\tLoss 1.5704 (1.5704)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [25][50/391]\tTime 0.133 (0.091)\tData 0.095 (0.043)\tLoss 1.7488 (1.5759)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [25][100/391]\tTime 0.186 (0.099)\tData 0.121 (0.051)\tLoss 1.2130 (1.5754)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [25][150/391]\tTime 0.150 (0.095)\tData 0.094 (0.046)\tLoss 1.3826 (1.5830)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [25][200/391]\tTime 0.119 (0.093)\tData 0.071 (0.044)\tLoss 1.4982 (1.5956)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [25][250/391]\tTime 0.095 (0.095)\tData 0.057 (0.045)\tLoss 1.4156 (1.5976)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [25][300/391]\tTime 0.041 (0.094)\tData 0.000 (0.044)\tLoss 1.1594 (1.5935)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [25][350/391]\tTime 0.042 (0.093)\tData 0.000 (0.044)\tLoss 1.8183 (1.5984)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.149 (0.149)\tLoss 1.0895 (1.0895)\tPrec@1 76.562 (76.562)\n",
            "Test: [50/79]\tTime 0.022 (0.029)\tLoss 1.0656 (1.0643)\tPrec@1 82.031 (78.493)\n",
            " * Prec@1 78.830\n",
            "\n",
            "Epoch: 27/100\n",
            "Learning rate: 0.084243\n",
            "Epoch: [26][0/391]\tTime 0.281 (0.281)\tData 0.212 (0.212)\tLoss 1.3792 (1.3792)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [26][50/391]\tTime 0.125 (0.091)\tData 0.077 (0.041)\tLoss 1.4889 (1.6167)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [26][100/391]\tTime 0.045 (0.097)\tData 0.000 (0.044)\tLoss 1.5931 (1.5937)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [26][150/391]\tTime 0.133 (0.093)\tData 0.073 (0.042)\tLoss 1.8951 (1.5956)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [26][200/391]\tTime 0.150 (0.091)\tData 0.094 (0.042)\tLoss 1.8684 (1.6047)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [26][250/391]\tTime 0.042 (0.093)\tData 0.000 (0.043)\tLoss 1.4077 (1.6022)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [26][300/391]\tTime 0.047 (0.092)\tData 0.000 (0.043)\tLoss 1.9036 (1.5986)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [26][350/391]\tTime 0.157 (0.092)\tData 0.097 (0.043)\tLoss 1.8020 (1.5984)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.141 (0.141)\tLoss 1.1026 (1.1026)\tPrec@1 79.688 (79.688)\n",
            "Test: [50/79]\tTime 0.045 (0.029)\tLoss 1.1472 (1.0760)\tPrec@1 75.000 (79.029)\n",
            " * Prec@1 79.000\n",
            "\n",
            "Epoch: 28/100\n",
            "Learning rate: 0.083083\n",
            "Epoch: [27][0/391]\tTime 0.278 (0.278)\tData 0.213 (0.213)\tLoss 1.4424 (1.4424)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [27][50/391]\tTime 0.131 (0.088)\tData 0.075 (0.037)\tLoss 1.3463 (1.5595)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [27][100/391]\tTime 0.129 (0.096)\tData 0.091 (0.044)\tLoss 1.6106 (1.5761)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [27][150/391]\tTime 0.136 (0.093)\tData 0.094 (0.044)\tLoss 1.8586 (1.5916)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [27][200/391]\tTime 0.172 (0.092)\tData 0.109 (0.044)\tLoss 1.3779 (1.5859)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [27][250/391]\tTime 0.040 (0.094)\tData 0.000 (0.046)\tLoss 1.9356 (1.5906)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [27][300/391]\tTime 0.055 (0.093)\tData 0.007 (0.046)\tLoss 1.4214 (1.5947)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [27][350/391]\tTime 0.059 (0.094)\tData 0.006 (0.046)\tLoss 1.8219 (1.6019)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.120 (0.120)\tLoss 1.0239 (1.0239)\tPrec@1 82.812 (82.812)\n",
            "Test: [50/79]\tTime 0.008 (0.029)\tLoss 1.0568 (1.0423)\tPrec@1 79.688 (80.270)\n",
            " * Prec@1 80.740\n",
            "\n",
            "Epoch: 29/100\n",
            "Learning rate: 0.081889\n",
            "Epoch: [28][0/391]\tTime 0.284 (0.284)\tData 0.209 (0.209)\tLoss 1.8084 (1.8084)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [28][50/391]\tTime 0.201 (0.092)\tData 0.118 (0.045)\tLoss 1.7769 (1.6014)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [28][100/391]\tTime 0.095 (0.096)\tData 0.044 (0.046)\tLoss 1.8234 (1.6083)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [28][150/391]\tTime 0.089 (0.092)\tData 0.043 (0.043)\tLoss 1.8191 (1.6029)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [28][200/391]\tTime 0.160 (0.094)\tData 0.113 (0.046)\tLoss 1.4570 (1.5957)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [28][250/391]\tTime 0.141 (0.094)\tData 0.089 (0.046)\tLoss 1.1835 (1.5951)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [28][300/391]\tTime 0.122 (0.092)\tData 0.074 (0.044)\tLoss 1.1863 (1.6001)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [28][350/391]\tTime 0.067 (0.094)\tData 0.009 (0.045)\tLoss 1.4778 (1.5898)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.131 (0.131)\tLoss 1.0320 (1.0320)\tPrec@1 81.250 (81.250)\n",
            "Test: [50/79]\tTime 0.008 (0.028)\tLoss 1.1124 (1.0425)\tPrec@1 73.438 (79.366)\n",
            " * Prec@1 79.620\n",
            "\n",
            "Epoch: 30/100\n",
            "Learning rate: 0.080665\n",
            "Epoch: [29][0/391]\tTime 0.282 (0.282)\tData 0.215 (0.215)\tLoss 1.5853 (1.5853)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [29][50/391]\tTime 0.275 (0.096)\tData 0.201 (0.045)\tLoss 1.3638 (1.6523)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [29][100/391]\tTime 0.060 (0.097)\tData 0.000 (0.046)\tLoss 1.4717 (1.5960)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [29][150/391]\tTime 0.053 (0.093)\tData 0.000 (0.043)\tLoss 1.8378 (1.5980)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [29][200/391]\tTime 0.053 (0.096)\tData 0.002 (0.046)\tLoss 1.9306 (1.6010)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [29][250/391]\tTime 0.061 (0.094)\tData 0.006 (0.045)\tLoss 1.3212 (1.6102)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [29][300/391]\tTime 0.142 (0.093)\tData 0.082 (0.043)\tLoss 1.4440 (1.6056)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [29][350/391]\tTime 0.074 (0.094)\tData 0.028 (0.044)\tLoss 1.6087 (1.6009)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.142 (0.142)\tLoss 1.1432 (1.1432)\tPrec@1 73.438 (73.438)\n",
            "Test: [50/79]\tTime 0.013 (0.030)\tLoss 1.1266 (1.0911)\tPrec@1 74.219 (76.746)\n",
            " * Prec@1 77.200\n",
            "\n",
            "Epoch: 31/100\n",
            "Learning rate: 0.079410\n",
            "Epoch: [30][0/391]\tTime 0.285 (0.285)\tData 0.207 (0.207)\tLoss 1.3308 (1.3308)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [30][50/391]\tTime 0.044 (0.106)\tData 0.000 (0.055)\tLoss 1.8284 (1.6377)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [30][100/391]\tTime 0.135 (0.098)\tData 0.094 (0.050)\tLoss 1.2843 (1.6249)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [30][150/391]\tTime 0.132 (0.094)\tData 0.093 (0.047)\tLoss 1.6995 (1.5990)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [30][200/391]\tTime 0.122 (0.097)\tData 0.078 (0.049)\tLoss 1.6350 (1.6056)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [30][250/391]\tTime 0.132 (0.094)\tData 0.092 (0.048)\tLoss 1.7339 (1.6090)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [30][300/391]\tTime 0.248 (0.094)\tData 0.171 (0.048)\tLoss 1.8529 (1.6139)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [30][350/391]\tTime 0.082 (0.095)\tData 0.042 (0.047)\tLoss 1.6886 (1.6097)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.130 (0.130)\tLoss 1.0510 (1.0510)\tPrec@1 79.688 (79.688)\n",
            "Test: [50/79]\tTime 0.012 (0.028)\tLoss 1.2086 (1.1133)\tPrec@1 74.219 (75.414)\n",
            " * Prec@1 75.370\n",
            "\n",
            "Epoch: 32/100\n",
            "Learning rate: 0.078126\n",
            "Epoch: [31][0/391]\tTime 0.288 (0.288)\tData 0.210 (0.210)\tLoss 1.5231 (1.5231)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [31][50/391]\tTime 0.082 (0.106)\tData 0.037 (0.049)\tLoss 1.8970 (1.5993)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [31][100/391]\tTime 0.122 (0.096)\tData 0.069 (0.042)\tLoss 1.4965 (1.6083)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [31][150/391]\tTime 0.108 (0.092)\tData 0.058 (0.042)\tLoss 1.5617 (1.5982)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [31][200/391]\tTime 0.071 (0.095)\tData 0.032 (0.044)\tLoss 1.5293 (1.6032)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [31][250/391]\tTime 0.057 (0.093)\tData 0.005 (0.042)\tLoss 1.1515 (1.5929)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [31][300/391]\tTime 0.209 (0.094)\tData 0.119 (0.043)\tLoss 1.8562 (1.5921)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [31][350/391]\tTime 0.060 (0.093)\tData 0.008 (0.043)\tLoss 1.5532 (1.5961)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.136 (0.136)\tLoss 1.0243 (1.0243)\tPrec@1 78.125 (78.125)\n",
            "Test: [50/79]\tTime 0.036 (0.028)\tLoss 1.1605 (1.0663)\tPrec@1 71.094 (77.528)\n",
            " * Prec@1 77.390\n",
            "\n",
            "Epoch: 33/100\n",
            "Learning rate: 0.076815\n",
            "Epoch: [32][0/391]\tTime 0.284 (0.284)\tData 0.205 (0.205)\tLoss 1.8616 (1.8616)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [32][50/391]\tTime 0.130 (0.109)\tData 0.092 (0.057)\tLoss 1.8921 (1.5426)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [32][100/391]\tTime 0.051 (0.097)\tData 0.000 (0.048)\tLoss 1.2957 (1.5537)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [32][150/391]\tTime 0.070 (0.096)\tData 0.020 (0.047)\tLoss 1.8701 (1.5655)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [32][200/391]\tTime 0.076 (0.096)\tData 0.030 (0.047)\tLoss 1.5857 (1.5749)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [32][250/391]\tTime 0.114 (0.094)\tData 0.058 (0.045)\tLoss 1.8924 (1.5766)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [32][300/391]\tTime 0.060 (0.095)\tData 0.000 (0.045)\tLoss 1.7325 (1.5882)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [32][350/391]\tTime 0.040 (0.094)\tData 0.000 (0.045)\tLoss 1.8341 (1.5920)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.134 (0.134)\tLoss 1.0167 (1.0167)\tPrec@1 81.250 (81.250)\n",
            "Test: [50/79]\tTime 0.035 (0.029)\tLoss 1.1099 (1.0585)\tPrec@1 73.438 (78.676)\n",
            " * Prec@1 79.270\n",
            "\n",
            "Epoch: 34/100\n",
            "Learning rate: 0.075477\n",
            "Epoch: [33][0/391]\tTime 0.262 (0.262)\tData 0.207 (0.207)\tLoss 1.2921 (1.2921)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [33][50/391]\tTime 0.124 (0.111)\tData 0.080 (0.058)\tLoss 1.8842 (1.6088)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [33][100/391]\tTime 0.087 (0.098)\tData 0.042 (0.048)\tLoss 1.7411 (1.5844)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [33][150/391]\tTime 0.154 (0.098)\tData 0.093 (0.048)\tLoss 1.7081 (1.5746)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [33][200/391]\tTime 0.135 (0.097)\tData 0.096 (0.047)\tLoss 1.8218 (1.5897)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [33][250/391]\tTime 0.099 (0.095)\tData 0.053 (0.046)\tLoss 1.7545 (1.5836)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [33][300/391]\tTime 0.103 (0.097)\tData 0.044 (0.048)\tLoss 1.6139 (1.5832)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [33][350/391]\tTime 0.114 (0.095)\tData 0.072 (0.047)\tLoss 1.6847 (1.5942)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.153 (0.153)\tLoss 1.0444 (1.0444)\tPrec@1 80.469 (80.469)\n",
            "Test: [50/79]\tTime 0.032 (0.032)\tLoss 1.0638 (1.0410)\tPrec@1 77.344 (79.305)\n",
            " * Prec@1 79.570\n",
            "\n",
            "Epoch: 35/100\n",
            "Learning rate: 0.074114\n",
            "Epoch: [34][0/391]\tTime 0.413 (0.413)\tData 0.322 (0.322)\tLoss 1.7342 (1.7342)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [34][50/391]\tTime 0.051 (0.096)\tData 0.012 (0.046)\tLoss 1.7275 (1.5449)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [34][100/391]\tTime 0.042 (0.091)\tData 0.000 (0.044)\tLoss 1.6322 (1.5620)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [34][150/391]\tTime 0.132 (0.096)\tData 0.091 (0.047)\tLoss 1.2513 (1.5790)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [34][200/391]\tTime 0.099 (0.094)\tData 0.051 (0.046)\tLoss 1.8382 (1.5812)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [34][250/391]\tTime 0.111 (0.092)\tData 0.051 (0.044)\tLoss 1.3075 (1.5787)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [34][300/391]\tTime 0.053 (0.094)\tData 0.000 (0.046)\tLoss 1.3955 (1.5693)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [34][350/391]\tTime 0.049 (0.093)\tData 0.000 (0.044)\tLoss 1.8661 (1.5692)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.133 (0.133)\tLoss 1.0914 (1.0914)\tPrec@1 80.469 (80.469)\n",
            "Test: [50/79]\tTime 0.059 (0.039)\tLoss 1.1387 (1.1010)\tPrec@1 76.562 (79.335)\n",
            " * Prec@1 79.600\n",
            "\n",
            "Epoch: 36/100\n",
            "Learning rate: 0.072727\n",
            "Epoch: [35][0/391]\tTime 0.288 (0.288)\tData 0.207 (0.207)\tLoss 1.6775 (1.6775)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [35][50/391]\tTime 0.058 (0.091)\tData 0.000 (0.040)\tLoss 1.1761 (1.5683)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [35][100/391]\tTime 0.099 (0.088)\tData 0.054 (0.037)\tLoss 1.4888 (1.5729)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [35][150/391]\tTime 0.049 (0.093)\tData 0.000 (0.042)\tLoss 1.5086 (1.5663)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [35][200/391]\tTime 0.043 (0.092)\tData 0.000 (0.041)\tLoss 1.7713 (1.5691)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [35][250/391]\tTime 0.265 (0.092)\tData 0.175 (0.042)\tLoss 1.9005 (1.5767)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [35][300/391]\tTime 0.134 (0.093)\tData 0.091 (0.044)\tLoss 1.2300 (1.5866)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [35][350/391]\tTime 0.125 (0.092)\tData 0.060 (0.043)\tLoss 1.8286 (1.5838)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.163 (0.163)\tLoss 0.9806 (0.9806)\tPrec@1 80.469 (80.469)\n",
            "Test: [50/79]\tTime 0.016 (0.039)\tLoss 0.9823 (1.0385)\tPrec@1 82.812 (79.029)\n",
            " * Prec@1 79.340\n",
            "\n",
            "Epoch: 37/100\n",
            "Learning rate: 0.071318\n",
            "Epoch: [36][0/391]\tTime 0.282 (0.282)\tData 0.211 (0.211)\tLoss 1.1423 (1.1423)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [36][50/391]\tTime 0.126 (0.088)\tData 0.088 (0.039)\tLoss 1.4774 (1.5611)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [36][100/391]\tTime 0.065 (0.088)\tData 0.000 (0.038)\tLoss 1.3249 (1.5698)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [36][150/391]\tTime 0.041 (0.094)\tData 0.000 (0.045)\tLoss 1.4030 (1.5750)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [36][200/391]\tTime 0.057 (0.092)\tData 0.005 (0.043)\tLoss 1.8386 (1.5828)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [36][250/391]\tTime 0.063 (0.093)\tData 0.000 (0.044)\tLoss 1.8097 (1.5894)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [36][300/391]\tTime 0.101 (0.093)\tData 0.054 (0.044)\tLoss 1.7139 (1.5842)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [36][350/391]\tTime 0.079 (0.091)\tData 0.033 (0.043)\tLoss 1.6796 (1.5830)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.212 (0.212)\tLoss 1.1161 (1.1161)\tPrec@1 76.562 (76.562)\n",
            "Test: [50/79]\tTime 0.030 (0.033)\tLoss 1.1271 (1.1159)\tPrec@1 73.438 (76.608)\n",
            " * Prec@1 77.230\n",
            "\n",
            "Epoch: 38/100\n",
            "Learning rate: 0.069888\n",
            "Epoch: [37][0/391]\tTime 0.286 (0.286)\tData 0.212 (0.212)\tLoss 1.4704 (1.4704)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [37][50/391]\tTime 0.095 (0.089)\tData 0.047 (0.039)\tLoss 1.8496 (1.5633)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [37][100/391]\tTime 0.069 (0.091)\tData 0.000 (0.040)\tLoss 1.4084 (1.5809)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [37][150/391]\tTime 0.042 (0.093)\tData 0.000 (0.044)\tLoss 1.6388 (1.5878)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [37][200/391]\tTime 0.119 (0.092)\tData 0.067 (0.044)\tLoss 1.3336 (1.5799)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [37][250/391]\tTime 0.065 (0.094)\tData 0.026 (0.046)\tLoss 1.8571 (1.5765)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [37][300/391]\tTime 0.064 (0.093)\tData 0.015 (0.045)\tLoss 1.4314 (1.5809)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [37][350/391]\tTime 0.040 (0.092)\tData 0.000 (0.045)\tLoss 1.3380 (1.5859)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.127 (0.127)\tLoss 1.0577 (1.0577)\tPrec@1 80.469 (80.469)\n",
            "Test: [50/79]\tTime 0.040 (0.029)\tLoss 1.1063 (1.0816)\tPrec@1 75.781 (77.696)\n",
            " * Prec@1 77.980\n",
            "\n",
            "Epoch: 39/100\n",
            "Learning rate: 0.068438\n",
            "Epoch: [38][0/391]\tTime 0.281 (0.281)\tData 0.215 (0.215)\tLoss 1.8172 (1.8172)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [38][50/391]\tTime 0.098 (0.089)\tData 0.059 (0.041)\tLoss 1.7911 (1.6041)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [38][100/391]\tTime 0.070 (0.097)\tData 0.010 (0.048)\tLoss 1.4719 (1.6062)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [38][150/391]\tTime 0.092 (0.093)\tData 0.045 (0.044)\tLoss 1.7397 (1.6022)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [38][200/391]\tTime 0.163 (0.092)\tData 0.117 (0.044)\tLoss 1.3039 (1.5786)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [38][250/391]\tTime 0.137 (0.094)\tData 0.090 (0.046)\tLoss 1.5929 (1.5614)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [38][300/391]\tTime 0.120 (0.093)\tData 0.075 (0.045)\tLoss 1.2742 (1.5725)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [38][350/391]\tTime 0.131 (0.092)\tData 0.092 (0.044)\tLoss 1.5007 (1.5786)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.135 (0.135)\tLoss 1.1648 (1.1648)\tPrec@1 75.781 (75.781)\n",
            "Test: [50/79]\tTime 0.024 (0.028)\tLoss 1.1037 (1.1107)\tPrec@1 77.344 (75.720)\n",
            " * Prec@1 75.890\n",
            "\n",
            "Epoch: 40/100\n",
            "Learning rate: 0.066970\n",
            "Epoch: [39][0/391]\tTime 0.270 (0.270)\tData 0.204 (0.204)\tLoss 1.7878 (1.7878)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [39][50/391]\tTime 0.056 (0.087)\tData 0.000 (0.037)\tLoss 1.7644 (1.5380)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [39][100/391]\tTime 0.055 (0.096)\tData 0.007 (0.044)\tLoss 1.8389 (1.5267)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [39][150/391]\tTime 0.155 (0.093)\tData 0.117 (0.043)\tLoss 1.2582 (1.5338)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [39][200/391]\tTime 0.078 (0.091)\tData 0.039 (0.043)\tLoss 1.8467 (1.5495)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [39][250/391]\tTime 0.073 (0.094)\tData 0.031 (0.045)\tLoss 1.4045 (1.5483)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [39][300/391]\tTime 0.063 (0.093)\tData 0.008 (0.045)\tLoss 1.6346 (1.5490)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [39][350/391]\tTime 0.075 (0.093)\tData 0.000 (0.044)\tLoss 1.5915 (1.5522)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.142 (0.142)\tLoss 1.0218 (1.0218)\tPrec@1 84.375 (84.375)\n",
            "Test: [50/79]\tTime 0.043 (0.029)\tLoss 1.1140 (1.0482)\tPrec@1 76.562 (79.396)\n",
            " * Prec@1 79.690\n",
            "\n",
            "Epoch: 41/100\n",
            "Learning rate: 0.065485\n",
            "Epoch: [40][0/391]\tTime 0.280 (0.280)\tData 0.212 (0.212)\tLoss 1.6043 (1.6043)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [40][50/391]\tTime 0.114 (0.092)\tData 0.069 (0.043)\tLoss 1.7722 (1.6518)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [40][100/391]\tTime 0.110 (0.099)\tData 0.064 (0.047)\tLoss 1.9441 (1.6262)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [40][150/391]\tTime 0.065 (0.094)\tData 0.007 (0.043)\tLoss 1.1019 (1.6000)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [40][200/391]\tTime 0.072 (0.093)\tData 0.006 (0.042)\tLoss 1.3924 (1.5787)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [40][250/391]\tTime 0.136 (0.094)\tData 0.090 (0.044)\tLoss 1.6118 (1.5915)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [40][300/391]\tTime 0.136 (0.093)\tData 0.097 (0.044)\tLoss 1.7068 (1.5838)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [40][350/391]\tTime 0.269 (0.095)\tData 0.210 (0.046)\tLoss 1.8028 (1.5797)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.132 (0.132)\tLoss 1.0491 (1.0491)\tPrec@1 82.031 (82.031)\n",
            "Test: [50/79]\tTime 0.040 (0.029)\tLoss 1.0272 (1.0171)\tPrec@1 82.031 (81.985)\n",
            " * Prec@1 82.360\n",
            "\n",
            "Epoch: 42/100\n",
            "Learning rate: 0.063986\n",
            "Epoch: [41][0/391]\tTime 0.281 (0.281)\tData 0.212 (0.212)\tLoss 1.4930 (1.4930)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [41][50/391]\tTime 0.096 (0.087)\tData 0.045 (0.037)\tLoss 1.8300 (1.5052)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [41][100/391]\tTime 0.069 (0.095)\tData 0.026 (0.044)\tLoss 1.6847 (1.5271)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [41][150/391]\tTime 0.102 (0.092)\tData 0.057 (0.043)\tLoss 1.8163 (1.5546)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [41][200/391]\tTime 0.184 (0.094)\tData 0.131 (0.044)\tLoss 1.7639 (1.5527)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [41][250/391]\tTime 0.136 (0.093)\tData 0.082 (0.043)\tLoss 1.8384 (1.5563)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [41][300/391]\tTime 0.119 (0.092)\tData 0.076 (0.042)\tLoss 1.3171 (1.5525)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [41][350/391]\tTime 0.130 (0.094)\tData 0.083 (0.044)\tLoss 1.7596 (1.5550)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.134 (0.134)\tLoss 1.0656 (1.0656)\tPrec@1 81.250 (81.250)\n",
            "Test: [50/79]\tTime 0.043 (0.030)\tLoss 1.0746 (1.0611)\tPrec@1 78.906 (78.385)\n",
            " * Prec@1 78.430\n",
            "\n",
            "Epoch: 43/100\n",
            "Learning rate: 0.062472\n",
            "Epoch: [42][0/391]\tTime 0.286 (0.286)\tData 0.218 (0.218)\tLoss 1.5978 (1.5978)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [42][50/391]\tTime 0.048 (0.096)\tData 0.005 (0.046)\tLoss 1.8594 (1.5918)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [42][100/391]\tTime 0.043 (0.096)\tData 0.000 (0.047)\tLoss 1.4187 (1.5817)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [42][150/391]\tTime 0.062 (0.093)\tData 0.007 (0.045)\tLoss 1.7942 (1.5783)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [42][200/391]\tTime 0.041 (0.096)\tData 0.000 (0.047)\tLoss 1.7510 (1.5756)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [42][250/391]\tTime 0.050 (0.094)\tData 0.005 (0.046)\tLoss 1.6588 (1.5679)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [42][300/391]\tTime 0.051 (0.093)\tData 0.000 (0.044)\tLoss 1.9030 (1.5625)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [42][350/391]\tTime 0.062 (0.095)\tData 0.006 (0.045)\tLoss 1.2001 (1.5584)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.159 (0.159)\tLoss 1.0644 (1.0644)\tPrec@1 75.000 (75.000)\n",
            "Test: [50/79]\tTime 0.033 (0.030)\tLoss 1.1068 (1.0485)\tPrec@1 75.000 (78.937)\n",
            " * Prec@1 79.160\n",
            "\n",
            "Epoch: 44/100\n",
            "Learning rate: 0.060946\n",
            "Epoch: [43][0/391]\tTime 0.277 (0.277)\tData 0.209 (0.209)\tLoss 1.5478 (1.5478)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [43][50/391]\tTime 0.141 (0.109)\tData 0.089 (0.056)\tLoss 1.1646 (1.5233)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [43][100/391]\tTime 0.128 (0.099)\tData 0.078 (0.049)\tLoss 1.8558 (1.5189)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [43][150/391]\tTime 0.040 (0.094)\tData 0.000 (0.045)\tLoss 1.4173 (1.5374)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [43][200/391]\tTime 0.041 (0.097)\tData 0.000 (0.047)\tLoss 1.2606 (1.5453)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [43][250/391]\tTime 0.042 (0.095)\tData 0.000 (0.046)\tLoss 1.8484 (1.5536)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [43][300/391]\tTime 0.087 (0.095)\tData 0.007 (0.046)\tLoss 1.4979 (1.5620)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [43][350/391]\tTime 0.103 (0.096)\tData 0.055 (0.046)\tLoss 1.4607 (1.5552)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.142 (0.142)\tLoss 0.9950 (0.9950)\tPrec@1 83.594 (83.594)\n",
            "Test: [50/79]\tTime 0.009 (0.031)\tLoss 1.0073 (0.9955)\tPrec@1 80.469 (82.200)\n",
            " * Prec@1 82.210\n",
            "\n",
            "Epoch: 45/100\n",
            "Learning rate: 0.059410\n",
            "Epoch: [44][0/391]\tTime 0.282 (0.282)\tData 0.220 (0.220)\tLoss 1.3140 (1.3140)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [44][50/391]\tTime 0.106 (0.108)\tData 0.059 (0.056)\tLoss 1.7161 (1.5451)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [44][100/391]\tTime 0.129 (0.096)\tData 0.090 (0.046)\tLoss 1.5235 (1.5424)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [44][150/391]\tTime 0.060 (0.093)\tData 0.000 (0.044)\tLoss 1.2893 (1.5218)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [44][200/391]\tTime 0.052 (0.096)\tData 0.000 (0.047)\tLoss 1.7065 (1.5268)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [44][250/391]\tTime 0.040 (0.093)\tData 0.000 (0.045)\tLoss 1.1197 (1.5293)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [44][300/391]\tTime 0.066 (0.095)\tData 0.000 (0.046)\tLoss 1.8888 (1.5332)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [44][350/391]\tTime 0.045 (0.094)\tData 0.000 (0.045)\tLoss 1.3658 (1.5339)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.146 (0.146)\tLoss 0.9909 (0.9909)\tPrec@1 82.812 (82.812)\n",
            "Test: [50/79]\tTime 0.018 (0.029)\tLoss 1.0762 (1.0063)\tPrec@1 79.688 (80.760)\n",
            " * Prec@1 80.900\n",
            "\n",
            "Epoch: 46/100\n",
            "Learning rate: 0.057864\n",
            "Epoch: [45][0/391]\tTime 0.269 (0.269)\tData 0.203 (0.203)\tLoss 1.8637 (1.8637)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [45][50/391]\tTime 0.128 (0.107)\tData 0.090 (0.055)\tLoss 1.7209 (1.6172)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [45][100/391]\tTime 0.107 (0.096)\tData 0.055 (0.047)\tLoss 1.2458 (1.5909)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [45][150/391]\tTime 0.068 (0.095)\tData 0.000 (0.045)\tLoss 1.8225 (1.5535)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [45][200/391]\tTime 0.073 (0.096)\tData 0.000 (0.046)\tLoss 1.7590 (1.5562)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [45][250/391]\tTime 0.047 (0.094)\tData 0.000 (0.044)\tLoss 1.3523 (1.5557)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [45][300/391]\tTime 0.051 (0.096)\tData 0.000 (0.046)\tLoss 1.6790 (1.5524)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [45][350/391]\tTime 0.057 (0.095)\tData 0.000 (0.045)\tLoss 1.2351 (1.5453)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.130 (0.130)\tLoss 1.0473 (1.0473)\tPrec@1 80.469 (80.469)\n",
            "Test: [50/79]\tTime 0.043 (0.029)\tLoss 1.0781 (1.0328)\tPrec@1 75.000 (80.775)\n",
            " * Prec@1 80.880\n",
            "\n",
            "Epoch: 47/100\n",
            "Learning rate: 0.056310\n",
            "Epoch: [46][0/391]\tTime 0.430 (0.430)\tData 0.292 (0.292)\tLoss 1.3203 (1.3203)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [46][50/391]\tTime 0.132 (0.103)\tData 0.076 (0.048)\tLoss 1.5370 (1.5620)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [46][100/391]\tTime 0.106 (0.094)\tData 0.062 (0.044)\tLoss 1.8308 (1.5327)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [46][150/391]\tTime 0.137 (0.099)\tData 0.083 (0.049)\tLoss 1.6700 (1.5530)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [46][200/391]\tTime 0.049 (0.095)\tData 0.011 (0.045)\tLoss 1.2644 (1.5306)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [46][250/391]\tTime 0.087 (0.093)\tData 0.037 (0.044)\tLoss 1.3060 (1.5386)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [46][300/391]\tTime 0.144 (0.095)\tData 0.097 (0.045)\tLoss 1.6582 (1.5362)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [46][350/391]\tTime 0.138 (0.094)\tData 0.078 (0.045)\tLoss 1.7654 (1.5416)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.127 (0.127)\tLoss 1.0160 (1.0160)\tPrec@1 79.688 (79.688)\n",
            "Test: [50/79]\tTime 0.066 (0.035)\tLoss 1.0075 (0.9827)\tPrec@1 80.469 (82.016)\n",
            " * Prec@1 82.120\n",
            "\n",
            "Epoch: 48/100\n",
            "Learning rate: 0.054751\n",
            "Epoch: [47][0/391]\tTime 0.360 (0.360)\tData 0.284 (0.284)\tLoss 1.8719 (1.8719)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [47][50/391]\tTime 0.164 (0.095)\tData 0.123 (0.045)\tLoss 1.1753 (1.5121)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [47][100/391]\tTime 0.095 (0.090)\tData 0.042 (0.039)\tLoss 1.2285 (1.4924)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [47][150/391]\tTime 0.119 (0.095)\tData 0.067 (0.043)\tLoss 1.1530 (1.4989)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [47][200/391]\tTime 0.100 (0.093)\tData 0.061 (0.042)\tLoss 1.5609 (1.5206)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [47][250/391]\tTime 0.145 (0.092)\tData 0.069 (0.041)\tLoss 1.3292 (1.5240)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [47][300/391]\tTime 0.099 (0.094)\tData 0.054 (0.043)\tLoss 1.7282 (1.5251)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [47][350/391]\tTime 0.111 (0.093)\tData 0.060 (0.043)\tLoss 1.7902 (1.5278)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.157 (0.157)\tLoss 1.0259 (1.0259)\tPrec@1 82.812 (82.812)\n",
            "Test: [50/79]\tTime 0.015 (0.043)\tLoss 1.0343 (1.0000)\tPrec@1 78.125 (81.648)\n",
            " * Prec@1 81.710\n",
            "\n",
            "Epoch: 49/100\n",
            "Learning rate: 0.053186\n",
            "Epoch: [48][0/391]\tTime 0.274 (0.274)\tData 0.204 (0.204)\tLoss 1.7203 (1.7203)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [48][50/391]\tTime 0.083 (0.090)\tData 0.035 (0.038)\tLoss 1.5802 (1.4849)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [48][100/391]\tTime 0.043 (0.088)\tData 0.000 (0.037)\tLoss 1.3414 (1.5126)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [48][150/391]\tTime 0.118 (0.094)\tData 0.074 (0.043)\tLoss 1.8629 (1.5233)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [48][200/391]\tTime 0.125 (0.092)\tData 0.074 (0.042)\tLoss 1.2403 (1.5249)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [48][250/391]\tTime 0.278 (0.094)\tData 0.213 (0.044)\tLoss 1.1523 (1.5205)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [48][300/391]\tTime 0.131 (0.094)\tData 0.092 (0.044)\tLoss 1.2364 (1.5179)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [48][350/391]\tTime 0.114 (0.093)\tData 0.070 (0.044)\tLoss 1.4389 (1.5168)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.222 (0.222)\tLoss 1.0658 (1.0658)\tPrec@1 78.906 (78.906)\n",
            "Test: [50/79]\tTime 0.016 (0.030)\tLoss 1.0417 (1.0401)\tPrec@1 80.469 (80.224)\n",
            " * Prec@1 80.640\n",
            "\n",
            "Epoch: 50/100\n",
            "Learning rate: 0.051619\n",
            "Epoch: [49][0/391]\tTime 0.307 (0.307)\tData 0.233 (0.233)\tLoss 1.4134 (1.4134)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [49][50/391]\tTime 0.148 (0.090)\tData 0.100 (0.041)\tLoss 1.4383 (1.5599)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [49][100/391]\tTime 0.241 (0.094)\tData 0.186 (0.044)\tLoss 1.2043 (1.5454)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [49][150/391]\tTime 0.115 (0.093)\tData 0.067 (0.043)\tLoss 1.8652 (1.5303)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [49][200/391]\tTime 0.129 (0.092)\tData 0.091 (0.043)\tLoss 1.7600 (1.5293)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [49][250/391]\tTime 0.123 (0.095)\tData 0.069 (0.045)\tLoss 1.5639 (1.5318)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [49][300/391]\tTime 0.094 (0.093)\tData 0.045 (0.044)\tLoss 1.6746 (1.5319)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [49][350/391]\tTime 0.154 (0.092)\tData 0.101 (0.043)\tLoss 1.2083 (1.5249)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.131 (0.131)\tLoss 1.0225 (1.0225)\tPrec@1 82.812 (82.812)\n",
            "Test: [50/79]\tTime 0.043 (0.029)\tLoss 1.0787 (1.0264)\tPrec@1 77.344 (81.311)\n",
            " * Prec@1 81.610\n",
            "\n",
            "Epoch: 51/100\n",
            "Learning rate: 0.050050\n",
            "Epoch: [50][0/391]\tTime 0.267 (0.267)\tData 0.213 (0.213)\tLoss 1.6054 (1.6054)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [50][50/391]\tTime 0.057 (0.090)\tData 0.000 (0.038)\tLoss 1.4781 (1.5149)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [50][100/391]\tTime 0.052 (0.098)\tData 0.013 (0.045)\tLoss 1.3911 (1.4989)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [50][150/391]\tTime 0.051 (0.094)\tData 0.000 (0.042)\tLoss 1.5576 (1.4985)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [50][200/391]\tTime 0.041 (0.092)\tData 0.000 (0.042)\tLoss 1.6798 (1.5114)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [50][250/391]\tTime 0.064 (0.095)\tData 0.002 (0.045)\tLoss 1.3839 (1.5107)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [50][300/391]\tTime 0.091 (0.093)\tData 0.047 (0.044)\tLoss 1.8305 (1.5096)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [50][350/391]\tTime 0.171 (0.093)\tData 0.113 (0.043)\tLoss 1.7975 (1.5129)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.150 (0.150)\tLoss 0.9540 (0.9540)\tPrec@1 85.156 (85.156)\n",
            "Test: [50/79]\tTime 0.028 (0.029)\tLoss 0.9783 (0.9686)\tPrec@1 82.031 (82.690)\n",
            " * Prec@1 82.960\n",
            "\n",
            "Epoch: 52/100\n",
            "Learning rate: 0.048481\n",
            "Epoch: [51][0/391]\tTime 0.291 (0.291)\tData 0.220 (0.220)\tLoss 1.8396 (1.8396)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [51][50/391]\tTime 0.060 (0.088)\tData 0.000 (0.038)\tLoss 1.7522 (1.5468)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [51][100/391]\tTime 0.042 (0.096)\tData 0.000 (0.045)\tLoss 1.8030 (1.5516)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [51][150/391]\tTime 0.049 (0.093)\tData 0.000 (0.044)\tLoss 1.4243 (1.5219)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [51][200/391]\tTime 0.049 (0.092)\tData 0.000 (0.043)\tLoss 1.5017 (1.5317)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [51][250/391]\tTime 0.042 (0.095)\tData 0.000 (0.045)\tLoss 1.2286 (1.5255)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [51][300/391]\tTime 0.057 (0.094)\tData 0.018 (0.044)\tLoss 1.7983 (1.5218)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [51][350/391]\tTime 0.150 (0.096)\tData 0.098 (0.046)\tLoss 1.8320 (1.5276)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.156 (0.156)\tLoss 0.9552 (0.9552)\tPrec@1 84.375 (84.375)\n",
            "Test: [50/79]\tTime 0.025 (0.029)\tLoss 0.9314 (0.9620)\tPrec@1 85.156 (83.915)\n",
            " * Prec@1 84.060\n",
            "\n",
            "Epoch: 53/100\n",
            "Learning rate: 0.046914\n",
            "Epoch: [52][0/391]\tTime 0.300 (0.300)\tData 0.225 (0.225)\tLoss 1.2864 (1.2864)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [52][50/391]\tTime 0.149 (0.092)\tData 0.090 (0.043)\tLoss 1.1061 (1.5053)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [52][100/391]\tTime 0.123 (0.097)\tData 0.069 (0.045)\tLoss 1.4888 (1.5307)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [52][150/391]\tTime 0.093 (0.094)\tData 0.045 (0.043)\tLoss 1.7319 (1.5342)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [52][200/391]\tTime 0.257 (0.097)\tData 0.192 (0.046)\tLoss 1.7166 (1.5374)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [52][250/391]\tTime 0.152 (0.095)\tData 0.111 (0.045)\tLoss 1.8350 (1.5229)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [52][300/391]\tTime 0.133 (0.094)\tData 0.092 (0.045)\tLoss 1.6167 (1.5229)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [52][350/391]\tTime 0.134 (0.096)\tData 0.085 (0.046)\tLoss 1.6607 (1.5280)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.131 (0.131)\tLoss 1.0093 (1.0093)\tPrec@1 84.375 (84.375)\n",
            "Test: [50/79]\tTime 0.015 (0.028)\tLoss 1.1099 (1.0384)\tPrec@1 75.000 (80.086)\n",
            " * Prec@1 79.970\n",
            "\n",
            "Epoch: 54/100\n",
            "Learning rate: 0.045349\n",
            "Epoch: [53][0/391]\tTime 0.292 (0.292)\tData 0.221 (0.221)\tLoss 1.1829 (1.1829)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [53][50/391]\tTime 0.228 (0.108)\tData 0.175 (0.056)\tLoss 1.0692 (1.5011)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [53][100/391]\tTime 0.098 (0.098)\tData 0.049 (0.046)\tLoss 1.4505 (1.4971)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [53][150/391]\tTime 0.040 (0.094)\tData 0.000 (0.043)\tLoss 1.4480 (1.4850)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [53][200/391]\tTime 0.141 (0.096)\tData 0.095 (0.045)\tLoss 1.3883 (1.5035)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [53][250/391]\tTime 0.105 (0.094)\tData 0.061 (0.043)\tLoss 1.8007 (1.5097)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [53][300/391]\tTime 0.095 (0.093)\tData 0.047 (0.043)\tLoss 1.1680 (1.5037)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [53][350/391]\tTime 0.083 (0.095)\tData 0.032 (0.045)\tLoss 1.8619 (1.4953)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.152 (0.152)\tLoss 0.9885 (0.9885)\tPrec@1 82.031 (82.031)\n",
            "Test: [50/79]\tTime 0.036 (0.029)\tLoss 0.9696 (0.9984)\tPrec@1 84.375 (81.556)\n",
            " * Prec@1 81.700\n",
            "\n",
            "Epoch: 55/100\n",
            "Learning rate: 0.043790\n",
            "Epoch: [54][0/391]\tTime 0.279 (0.279)\tData 0.206 (0.206)\tLoss 1.1963 (1.1963)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [54][50/391]\tTime 0.040 (0.109)\tData 0.000 (0.058)\tLoss 1.7803 (1.4946)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [54][100/391]\tTime 0.061 (0.098)\tData 0.000 (0.050)\tLoss 1.8139 (1.5096)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [54][150/391]\tTime 0.049 (0.094)\tData 0.000 (0.047)\tLoss 1.1200 (1.5103)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [54][200/391]\tTime 0.133 (0.097)\tData 0.092 (0.049)\tLoss 1.4387 (1.5025)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [54][250/391]\tTime 0.141 (0.095)\tData 0.097 (0.047)\tLoss 1.2802 (1.4983)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [54][300/391]\tTime 0.049 (0.095)\tData 0.000 (0.047)\tLoss 1.8025 (1.4946)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [54][350/391]\tTime 0.051 (0.095)\tData 0.000 (0.046)\tLoss 1.1111 (1.5009)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.142 (0.142)\tLoss 0.9593 (0.9593)\tPrec@1 83.594 (83.594)\n",
            "Test: [50/79]\tTime 0.020 (0.029)\tLoss 0.9856 (0.9785)\tPrec@1 82.812 (84.314)\n",
            " * Prec@1 84.360\n",
            "\n",
            "Epoch: 56/100\n",
            "Learning rate: 0.042236\n",
            "Epoch: [55][0/391]\tTime 0.290 (0.290)\tData 0.219 (0.219)\tLoss 1.1004 (1.1004)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [55][50/391]\tTime 0.115 (0.108)\tData 0.070 (0.054)\tLoss 1.7547 (1.5664)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [55][100/391]\tTime 0.141 (0.098)\tData 0.087 (0.047)\tLoss 1.7615 (1.5570)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [55][150/391]\tTime 0.245 (0.099)\tData 0.193 (0.047)\tLoss 1.5205 (1.5601)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [55][200/391]\tTime 0.136 (0.097)\tData 0.097 (0.048)\tLoss 1.7740 (1.5597)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [55][250/391]\tTime 0.136 (0.096)\tData 0.097 (0.047)\tLoss 1.4421 (1.5479)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [55][300/391]\tTime 0.137 (0.098)\tData 0.097 (0.049)\tLoss 1.4696 (1.5307)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [55][350/391]\tTime 0.124 (0.097)\tData 0.074 (0.048)\tLoss 1.8131 (1.5188)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.138 (0.138)\tLoss 0.9565 (0.9565)\tPrec@1 85.938 (85.938)\n",
            "Test: [50/79]\tTime 0.032 (0.033)\tLoss 0.9458 (0.9676)\tPrec@1 83.594 (82.659)\n",
            " * Prec@1 83.250\n",
            "\n",
            "Epoch: 57/100\n",
            "Learning rate: 0.040690\n",
            "Epoch: [56][0/391]\tTime 0.365 (0.365)\tData 0.287 (0.287)\tLoss 1.7604 (1.7604)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [56][50/391]\tTime 0.119 (0.095)\tData 0.069 (0.044)\tLoss 1.5170 (1.5248)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [56][100/391]\tTime 0.137 (0.092)\tData 0.090 (0.045)\tLoss 1.1392 (1.5121)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [56][150/391]\tTime 0.050 (0.097)\tData 0.000 (0.050)\tLoss 1.1410 (1.5007)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [56][200/391]\tTime 0.100 (0.095)\tData 0.054 (0.048)\tLoss 1.8560 (1.5025)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [56][250/391]\tTime 0.041 (0.093)\tData 0.000 (0.045)\tLoss 1.8309 (1.5108)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [56][300/391]\tTime 0.050 (0.095)\tData 0.000 (0.047)\tLoss 1.7650 (1.5135)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [56][350/391]\tTime 0.142 (0.094)\tData 0.090 (0.045)\tLoss 1.1956 (1.5072)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.154 (0.154)\tLoss 0.9412 (0.9412)\tPrec@1 85.156 (85.156)\n",
            "Test: [50/79]\tTime 0.020 (0.044)\tLoss 0.9564 (0.9401)\tPrec@1 85.938 (85.432)\n",
            " * Prec@1 85.500\n",
            "\n",
            "Epoch: 58/100\n",
            "Learning rate: 0.039154\n",
            "Epoch: [57][0/391]\tTime 0.298 (0.298)\tData 0.217 (0.217)\tLoss 1.6685 (1.6685)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [57][50/391]\tTime 0.126 (0.092)\tData 0.087 (0.043)\tLoss 1.1245 (1.4696)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [57][100/391]\tTime 0.119 (0.089)\tData 0.063 (0.039)\tLoss 1.7517 (1.4981)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [57][150/391]\tTime 0.078 (0.094)\tData 0.038 (0.042)\tLoss 1.8145 (1.4900)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [57][200/391]\tTime 0.123 (0.092)\tData 0.075 (0.042)\tLoss 1.7415 (1.4940)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [57][250/391]\tTime 0.152 (0.092)\tData 0.103 (0.042)\tLoss 1.6475 (1.4993)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [57][300/391]\tTime 0.069 (0.093)\tData 0.018 (0.043)\tLoss 1.7226 (1.4947)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [57][350/391]\tTime 0.139 (0.092)\tData 0.092 (0.043)\tLoss 1.7527 (1.4979)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.164 (0.164)\tLoss 0.9907 (0.9907)\tPrec@1 83.594 (83.594)\n",
            "Test: [50/79]\tTime 0.039 (0.033)\tLoss 1.0301 (0.9904)\tPrec@1 81.250 (84.651)\n",
            " * Prec@1 84.870\n",
            "\n",
            "Epoch: 59/100\n",
            "Learning rate: 0.037628\n",
            "Epoch: [58][0/391]\tTime 0.300 (0.300)\tData 0.233 (0.233)\tLoss 1.5150 (1.5150)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [58][50/391]\tTime 0.040 (0.088)\tData 0.000 (0.040)\tLoss 1.8023 (1.4823)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [58][100/391]\tTime 0.227 (0.093)\tData 0.165 (0.044)\tLoss 1.7570 (1.5106)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [58][150/391]\tTime 0.061 (0.094)\tData 0.015 (0.044)\tLoss 1.1622 (1.5085)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [58][200/391]\tTime 0.105 (0.092)\tData 0.056 (0.042)\tLoss 1.7607 (1.5117)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [58][250/391]\tTime 0.163 (0.095)\tData 0.112 (0.045)\tLoss 1.6407 (1.5099)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [58][300/391]\tTime 0.160 (0.094)\tData 0.108 (0.044)\tLoss 1.8165 (1.5141)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [58][350/391]\tTime 0.041 (0.093)\tData 0.000 (0.043)\tLoss 1.1286 (1.5073)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.125 (0.125)\tLoss 0.9493 (0.9493)\tPrec@1 85.938 (85.938)\n",
            "Test: [50/79]\tTime 0.041 (0.029)\tLoss 1.0813 (0.9946)\tPrec@1 78.125 (82.108)\n",
            " * Prec@1 82.360\n",
            "\n",
            "Epoch: 60/100\n",
            "Learning rate: 0.036114\n",
            "Epoch: [59][0/391]\tTime 0.291 (0.291)\tData 0.213 (0.213)\tLoss 1.7442 (1.7442)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [59][50/391]\tTime 0.083 (0.090)\tData 0.037 (0.039)\tLoss 1.3982 (1.5332)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [59][100/391]\tTime 0.104 (0.097)\tData 0.048 (0.045)\tLoss 1.5250 (1.5207)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [59][150/391]\tTime 0.070 (0.093)\tData 0.028 (0.043)\tLoss 1.1599 (1.5075)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [59][200/391]\tTime 0.044 (0.092)\tData 0.004 (0.042)\tLoss 1.7842 (1.5007)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [59][250/391]\tTime 0.056 (0.094)\tData 0.005 (0.045)\tLoss 1.0305 (1.4940)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [59][300/391]\tTime 0.142 (0.093)\tData 0.096 (0.044)\tLoss 1.6548 (1.4943)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [59][350/391]\tTime 0.063 (0.093)\tData 0.005 (0.044)\tLoss 1.5813 (1.4917)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.146 (0.146)\tLoss 1.0475 (1.0475)\tPrec@1 81.250 (81.250)\n",
            "Test: [50/79]\tTime 0.023 (0.029)\tLoss 1.0971 (1.0212)\tPrec@1 74.219 (81.112)\n",
            " * Prec@1 81.680\n",
            "\n",
            "Epoch: 61/100\n",
            "Learning rate: 0.034615\n",
            "Epoch: [60][0/391]\tTime 0.277 (0.277)\tData 0.204 (0.204)\tLoss 1.0342 (1.0342)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [60][50/391]\tTime 0.039 (0.089)\tData 0.000 (0.039)\tLoss 1.6964 (1.5011)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [60][100/391]\tTime 0.041 (0.097)\tData 0.000 (0.046)\tLoss 1.0045 (1.4853)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [60][150/391]\tTime 0.052 (0.094)\tData 0.000 (0.044)\tLoss 1.5866 (1.4877)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [60][200/391]\tTime 0.127 (0.092)\tData 0.078 (0.042)\tLoss 1.3933 (1.4984)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [60][250/391]\tTime 0.054 (0.094)\tData 0.007 (0.043)\tLoss 1.7242 (1.4941)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [60][300/391]\tTime 0.057 (0.093)\tData 0.000 (0.042)\tLoss 1.4123 (1.4942)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [60][350/391]\tTime 0.052 (0.094)\tData 0.001 (0.043)\tLoss 1.6759 (1.5037)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.136 (0.136)\tLoss 0.8959 (0.8959)\tPrec@1 88.281 (88.281)\n",
            "Test: [50/79]\tTime 0.016 (0.030)\tLoss 0.9523 (0.9315)\tPrec@1 83.594 (85.539)\n",
            " * Prec@1 85.710\n",
            "\n",
            "Epoch: 62/100\n",
            "Learning rate: 0.033130\n",
            "Epoch: [61][0/391]\tTime 0.279 (0.279)\tData 0.211 (0.211)\tLoss 1.7633 (1.7633)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [61][50/391]\tTime 0.084 (0.089)\tData 0.039 (0.040)\tLoss 1.6987 (1.4886)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [61][100/391]\tTime 0.113 (0.096)\tData 0.068 (0.044)\tLoss 1.5755 (1.4800)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [61][150/391]\tTime 0.133 (0.093)\tData 0.087 (0.042)\tLoss 1.7613 (1.4661)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [61][200/391]\tTime 0.071 (0.094)\tData 0.002 (0.044)\tLoss 1.4555 (1.4604)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [61][250/391]\tTime 0.086 (0.094)\tData 0.033 (0.044)\tLoss 1.3567 (1.4769)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [61][300/391]\tTime 0.124 (0.093)\tData 0.072 (0.043)\tLoss 1.8364 (1.4737)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [61][350/391]\tTime 0.136 (0.095)\tData 0.097 (0.045)\tLoss 1.4543 (1.4810)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.148 (0.148)\tLoss 0.9753 (0.9753)\tPrec@1 85.156 (85.156)\n",
            "Test: [50/79]\tTime 0.023 (0.029)\tLoss 1.0207 (0.9582)\tPrec@1 78.125 (85.600)\n",
            " * Prec@1 85.780\n",
            "\n",
            "Epoch: 63/100\n",
            "Learning rate: 0.031662\n",
            "Epoch: [62][0/391]\tTime 0.300 (0.300)\tData 0.221 (0.221)\tLoss 1.7095 (1.7095)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [62][50/391]\tTime 0.062 (0.102)\tData 0.000 (0.047)\tLoss 1.4798 (1.5311)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [62][100/391]\tTime 0.082 (0.097)\tData 0.030 (0.044)\tLoss 1.2833 (1.5117)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [62][150/391]\tTime 0.059 (0.094)\tData 0.005 (0.042)\tLoss 1.5485 (1.5119)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [62][200/391]\tTime 0.092 (0.097)\tData 0.007 (0.045)\tLoss 1.5980 (1.5169)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [62][250/391]\tTime 0.051 (0.095)\tData 0.000 (0.044)\tLoss 1.1716 (1.5274)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [62][300/391]\tTime 0.082 (0.094)\tData 0.031 (0.043)\tLoss 1.5730 (1.5147)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [62][350/391]\tTime 0.065 (0.096)\tData 0.000 (0.045)\tLoss 1.8175 (1.5098)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.140 (0.140)\tLoss 0.9478 (0.9478)\tPrec@1 85.156 (85.156)\n",
            "Test: [50/79]\tTime 0.008 (0.029)\tLoss 0.9797 (0.9528)\tPrec@1 83.594 (86.152)\n",
            " * Prec@1 86.240\n",
            "\n",
            "Epoch: 64/100\n",
            "Learning rate: 0.030212\n",
            "Epoch: [63][0/391]\tTime 0.307 (0.307)\tData 0.234 (0.234)\tLoss 1.1736 (1.1736)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [63][50/391]\tTime 0.040 (0.110)\tData 0.000 (0.055)\tLoss 1.5532 (1.4534)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [63][100/391]\tTime 0.041 (0.100)\tData 0.000 (0.051)\tLoss 1.4445 (1.4531)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [63][150/391]\tTime 0.098 (0.096)\tData 0.041 (0.047)\tLoss 1.6163 (1.4798)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [63][200/391]\tTime 0.051 (0.099)\tData 0.000 (0.049)\tLoss 1.7002 (1.4851)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [63][250/391]\tTime 0.059 (0.096)\tData 0.007 (0.046)\tLoss 1.7055 (1.4845)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [63][300/391]\tTime 0.055 (0.096)\tData 0.000 (0.046)\tLoss 1.7198 (1.4918)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [63][350/391]\tTime 0.130 (0.096)\tData 0.090 (0.046)\tLoss 1.5551 (1.4895)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.147 (0.147)\tLoss 0.9517 (0.9517)\tPrec@1 84.375 (84.375)\n",
            "Test: [50/79]\tTime 0.046 (0.030)\tLoss 0.9519 (0.9604)\tPrec@1 85.156 (85.141)\n",
            " * Prec@1 85.350\n",
            "\n",
            "Epoch: 65/100\n",
            "Learning rate: 0.028782\n",
            "Epoch: [64][0/391]\tTime 0.278 (0.278)\tData 0.208 (0.208)\tLoss 1.6782 (1.6782)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [64][50/391]\tTime 0.102 (0.107)\tData 0.058 (0.051)\tLoss 1.6628 (1.4843)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [64][100/391]\tTime 0.130 (0.096)\tData 0.087 (0.043)\tLoss 1.1025 (1.4971)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [64][150/391]\tTime 0.265 (0.097)\tData 0.218 (0.044)\tLoss 1.5734 (1.4851)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [64][200/391]\tTime 0.153 (0.097)\tData 0.095 (0.043)\tLoss 1.7125 (1.4812)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [64][250/391]\tTime 0.120 (0.095)\tData 0.066 (0.043)\tLoss 1.2649 (1.4743)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [64][300/391]\tTime 0.048 (0.097)\tData 0.000 (0.044)\tLoss 1.0293 (1.4771)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [64][350/391]\tTime 0.101 (0.095)\tData 0.055 (0.044)\tLoss 1.0991 (1.4822)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.158 (0.158)\tLoss 0.9712 (0.9712)\tPrec@1 86.719 (86.719)\n",
            "Test: [50/79]\tTime 0.009 (0.029)\tLoss 0.9753 (0.9778)\tPrec@1 82.812 (84.620)\n",
            " * Prec@1 84.860\n",
            "\n",
            "Epoch: 66/100\n",
            "Learning rate: 0.027373\n",
            "Epoch: [65][0/391]\tTime 0.354 (0.354)\tData 0.257 (0.257)\tLoss 1.7827 (1.7827)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [65][50/391]\tTime 0.044 (0.103)\tData 0.000 (0.051)\tLoss 1.6474 (1.5032)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [65][100/391]\tTime 0.059 (0.096)\tData 0.000 (0.046)\tLoss 1.2933 (1.4920)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [65][150/391]\tTime 0.042 (0.099)\tData 0.000 (0.048)\tLoss 1.6558 (1.4769)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [65][200/391]\tTime 0.058 (0.096)\tData 0.005 (0.045)\tLoss 1.2748 (1.4624)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [65][250/391]\tTime 0.046 (0.094)\tData 0.000 (0.045)\tLoss 1.6684 (1.4564)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [65][300/391]\tTime 0.092 (0.096)\tData 0.038 (0.046)\tLoss 1.1616 (1.4612)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [65][350/391]\tTime 0.101 (0.095)\tData 0.044 (0.044)\tLoss 1.5696 (1.4571)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.129 (0.129)\tLoss 0.9388 (0.9388)\tPrec@1 86.719 (86.719)\n",
            "Test: [50/79]\tTime 0.045 (0.040)\tLoss 0.9413 (0.9318)\tPrec@1 84.375 (86.305)\n",
            " * Prec@1 86.410\n",
            "\n",
            "Epoch: 67/100\n",
            "Learning rate: 0.025986\n",
            "Epoch: [66][0/391]\tTime 0.289 (0.289)\tData 0.216 (0.216)\tLoss 1.3181 (1.3181)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [66][50/391]\tTime 0.058 (0.090)\tData 0.000 (0.040)\tLoss 1.8341 (1.4307)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [66][100/391]\tTime 0.055 (0.088)\tData 0.007 (0.039)\tLoss 1.7180 (1.5016)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [66][150/391]\tTime 0.040 (0.094)\tData 0.000 (0.043)\tLoss 1.2310 (1.4816)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [66][200/391]\tTime 0.058 (0.092)\tData 0.000 (0.042)\tLoss 1.4365 (1.4840)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [66][250/391]\tTime 0.065 (0.092)\tData 0.000 (0.041)\tLoss 1.4405 (1.4913)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [66][300/391]\tTime 0.056 (0.093)\tData 0.000 (0.042)\tLoss 1.7914 (1.4888)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [66][350/391]\tTime 0.060 (0.093)\tData 0.006 (0.041)\tLoss 1.3278 (1.4884)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.243 (0.243)\tLoss 0.9239 (0.9239)\tPrec@1 87.500 (87.500)\n",
            "Test: [50/79]\tTime 0.050 (0.039)\tLoss 0.9807 (0.9372)\tPrec@1 85.938 (86.152)\n",
            " * Prec@1 86.560\n",
            "\n",
            "Epoch: 68/100\n",
            "Learning rate: 0.024623\n",
            "Epoch: [67][0/391]\tTime 0.284 (0.284)\tData 0.220 (0.220)\tLoss 1.0285 (1.0285)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [67][50/391]\tTime 0.054 (0.090)\tData 0.000 (0.038)\tLoss 1.6908 (1.5197)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [67][100/391]\tTime 0.052 (0.089)\tData 0.000 (0.039)\tLoss 1.3795 (1.4879)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [67][150/391]\tTime 0.131 (0.094)\tData 0.093 (0.043)\tLoss 1.7130 (1.4848)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [67][200/391]\tTime 0.082 (0.092)\tData 0.031 (0.043)\tLoss 1.7522 (1.4796)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [67][250/391]\tTime 0.142 (0.095)\tData 0.100 (0.046)\tLoss 1.7175 (1.4665)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [67][300/391]\tTime 0.137 (0.094)\tData 0.098 (0.046)\tLoss 1.6929 (1.4595)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [67][350/391]\tTime 0.108 (0.094)\tData 0.060 (0.046)\tLoss 1.7594 (1.4623)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.135 (0.135)\tLoss 0.8838 (0.8838)\tPrec@1 85.938 (85.938)\n",
            "Test: [50/79]\tTime 0.016 (0.028)\tLoss 0.9106 (0.8993)\tPrec@1 84.375 (87.469)\n",
            " * Prec@1 87.890\n",
            "\n",
            "Epoch: 69/100\n",
            "Learning rate: 0.023285\n",
            "Epoch: [68][0/391]\tTime 0.278 (0.278)\tData 0.209 (0.209)\tLoss 1.4747 (1.4747)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [68][50/391]\tTime 0.110 (0.091)\tData 0.064 (0.042)\tLoss 0.9935 (1.5253)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [68][100/391]\tTime 0.061 (0.098)\tData 0.005 (0.046)\tLoss 1.4128 (1.5126)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [68][150/391]\tTime 0.056 (0.094)\tData 0.005 (0.045)\tLoss 1.4275 (1.4873)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [68][200/391]\tTime 0.134 (0.092)\tData 0.083 (0.044)\tLoss 1.7957 (1.4776)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [68][250/391]\tTime 0.127 (0.095)\tData 0.078 (0.046)\tLoss 1.7619 (1.4629)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [68][300/391]\tTime 0.146 (0.094)\tData 0.102 (0.045)\tLoss 1.7605 (1.4620)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [68][350/391]\tTime 0.112 (0.093)\tData 0.065 (0.044)\tLoss 1.6614 (1.4576)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.142 (0.142)\tLoss 0.9296 (0.9296)\tPrec@1 86.719 (86.719)\n",
            "Test: [50/79]\tTime 0.046 (0.028)\tLoss 0.8919 (0.9241)\tPrec@1 90.625 (86.489)\n",
            " * Prec@1 86.670\n",
            "\n",
            "Epoch: 70/100\n",
            "Learning rate: 0.021974\n",
            "Epoch: [69][0/391]\tTime 0.292 (0.292)\tData 0.218 (0.218)\tLoss 1.2378 (1.2378)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [69][50/391]\tTime 0.119 (0.090)\tData 0.074 (0.040)\tLoss 0.9857 (1.4172)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [69][100/391]\tTime 0.113 (0.096)\tData 0.074 (0.045)\tLoss 1.8119 (1.4280)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [69][150/391]\tTime 0.139 (0.093)\tData 0.100 (0.044)\tLoss 0.9922 (1.4560)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [69][200/391]\tTime 0.089 (0.092)\tData 0.041 (0.043)\tLoss 1.6125 (1.4624)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [69][250/391]\tTime 0.126 (0.094)\tData 0.077 (0.046)\tLoss 1.2066 (1.4521)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [69][300/391]\tTime 0.078 (0.093)\tData 0.033 (0.045)\tLoss 1.7628 (1.4579)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [69][350/391]\tTime 0.172 (0.094)\tData 0.113 (0.046)\tLoss 1.6870 (1.4597)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.145 (0.145)\tLoss 0.9177 (0.9177)\tPrec@1 89.844 (89.844)\n",
            "Test: [50/79]\tTime 0.036 (0.028)\tLoss 0.8904 (0.8903)\tPrec@1 87.500 (87.822)\n",
            " * Prec@1 87.930\n",
            "\n",
            "Epoch: 71/100\n",
            "Learning rate: 0.020690\n",
            "Epoch: [70][0/391]\tTime 0.303 (0.303)\tData 0.229 (0.229)\tLoss 1.5817 (1.5817)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [70][50/391]\tTime 0.063 (0.089)\tData 0.018 (0.036)\tLoss 1.0132 (1.4639)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [70][100/391]\tTime 0.063 (0.097)\tData 0.016 (0.044)\tLoss 1.5087 (1.4232)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [70][150/391]\tTime 0.054 (0.094)\tData 0.002 (0.042)\tLoss 1.0056 (1.4348)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [70][200/391]\tTime 0.209 (0.094)\tData 0.158 (0.043)\tLoss 1.3762 (1.4463)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [70][250/391]\tTime 0.064 (0.095)\tData 0.015 (0.044)\tLoss 1.5278 (1.4505)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [70][300/391]\tTime 0.111 (0.094)\tData 0.068 (0.043)\tLoss 1.3342 (1.4459)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [70][350/391]\tTime 0.115 (0.096)\tData 0.061 (0.045)\tLoss 1.6516 (1.4511)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.140 (0.140)\tLoss 0.9529 (0.9529)\tPrec@1 88.281 (88.281)\n",
            "Test: [50/79]\tTime 0.038 (0.031)\tLoss 0.9427 (0.9486)\tPrec@1 86.719 (85.034)\n",
            " * Prec@1 85.300\n",
            "\n",
            "Epoch: 72/100\n",
            "Learning rate: 0.019435\n",
            "Epoch: [71][0/391]\tTime 0.284 (0.284)\tData 0.217 (0.217)\tLoss 1.8033 (1.8033)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [71][50/391]\tTime 0.069 (0.099)\tData 0.000 (0.044)\tLoss 1.1734 (1.4450)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [71][100/391]\tTime 0.049 (0.097)\tData 0.000 (0.045)\tLoss 1.8100 (1.4669)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [71][150/391]\tTime 0.042 (0.094)\tData 0.000 (0.044)\tLoss 1.6281 (1.4605)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [71][200/391]\tTime 0.055 (0.096)\tData 0.000 (0.046)\tLoss 1.4231 (1.4540)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [71][250/391]\tTime 0.047 (0.094)\tData 0.006 (0.044)\tLoss 1.4194 (1.4616)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [71][300/391]\tTime 0.072 (0.093)\tData 0.019 (0.043)\tLoss 1.6404 (1.4527)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [71][350/391]\tTime 0.121 (0.095)\tData 0.079 (0.045)\tLoss 0.9641 (1.4607)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.128 (0.128)\tLoss 0.9287 (0.9287)\tPrec@1 85.938 (85.938)\n",
            "Test: [50/79]\tTime 0.035 (0.028)\tLoss 0.9025 (0.9222)\tPrec@1 87.500 (87.316)\n",
            " * Prec@1 87.400\n",
            "\n",
            "Epoch: 73/100\n",
            "Learning rate: 0.018211\n",
            "Epoch: [72][0/391]\tTime 0.281 (0.281)\tData 0.216 (0.216)\tLoss 1.7373 (1.7373)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [72][50/391]\tTime 0.055 (0.107)\tData 0.000 (0.053)\tLoss 1.4416 (1.4423)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [72][100/391]\tTime 0.059 (0.097)\tData 0.010 (0.047)\tLoss 1.7234 (1.4327)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [72][150/391]\tTime 0.105 (0.093)\tData 0.059 (0.043)\tLoss 1.6438 (1.4445)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [72][200/391]\tTime 0.055 (0.096)\tData 0.000 (0.045)\tLoss 1.1174 (1.4404)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [72][250/391]\tTime 0.042 (0.094)\tData 0.000 (0.043)\tLoss 1.1388 (1.4425)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [72][300/391]\tTime 0.078 (0.093)\tData 0.000 (0.043)\tLoss 1.6982 (1.4355)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [72][350/391]\tTime 0.124 (0.094)\tData 0.085 (0.044)\tLoss 0.9697 (1.4375)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.139 (0.139)\tLoss 0.8282 (0.8282)\tPrec@1 90.625 (90.625)\n",
            "Test: [50/79]\tTime 0.040 (0.029)\tLoss 0.8547 (0.8651)\tPrec@1 87.500 (88.634)\n",
            " * Prec@1 88.660\n",
            "\n",
            "Epoch: 74/100\n",
            "Learning rate: 0.017017\n",
            "Epoch: [73][0/391]\tTime 0.276 (0.276)\tData 0.207 (0.207)\tLoss 1.4550 (1.4550)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [73][50/391]\tTime 0.132 (0.108)\tData 0.093 (0.057)\tLoss 1.7095 (1.4570)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [73][100/391]\tTime 0.054 (0.096)\tData 0.000 (0.045)\tLoss 1.6984 (1.4449)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [73][150/391]\tTime 0.043 (0.094)\tData 0.000 (0.044)\tLoss 1.2091 (1.4411)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [73][200/391]\tTime 0.041 (0.096)\tData 0.000 (0.046)\tLoss 1.4112 (1.4375)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [73][250/391]\tTime 0.059 (0.094)\tData 0.003 (0.044)\tLoss 1.3246 (1.4354)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [73][300/391]\tTime 0.056 (0.095)\tData 0.000 (0.045)\tLoss 1.6319 (1.4306)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [73][350/391]\tTime 0.068 (0.095)\tData 0.020 (0.045)\tLoss 1.2508 (1.4239)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.148 (0.148)\tLoss 0.9111 (0.9111)\tPrec@1 89.844 (89.844)\n",
            "Test: [50/79]\tTime 0.040 (0.029)\tLoss 0.9051 (0.9068)\tPrec@1 85.938 (86.581)\n",
            " * Prec@1 86.830\n",
            "\n",
            "Epoch: 75/100\n",
            "Learning rate: 0.015857\n",
            "Epoch: [74][0/391]\tTime 0.280 (0.280)\tData 0.211 (0.211)\tLoss 1.7841 (1.7841)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [74][50/391]\tTime 0.051 (0.110)\tData 0.000 (0.054)\tLoss 1.3894 (1.4611)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [74][100/391]\tTime 0.040 (0.099)\tData 0.000 (0.050)\tLoss 1.6953 (1.4848)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [74][150/391]\tTime 0.254 (0.099)\tData 0.201 (0.051)\tLoss 1.2072 (1.4734)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [74][200/391]\tTime 0.053 (0.098)\tData 0.002 (0.049)\tLoss 1.6727 (1.4636)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [74][250/391]\tTime 0.082 (0.095)\tData 0.026 (0.047)\tLoss 1.6768 (1.4613)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [74][300/391]\tTime 0.040 (0.097)\tData 0.000 (0.048)\tLoss 1.7797 (1.4464)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [74][350/391]\tTime 0.055 (0.096)\tData 0.000 (0.048)\tLoss 1.6525 (1.4465)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.138 (0.138)\tLoss 0.8818 (0.8818)\tPrec@1 87.500 (87.500)\n",
            "Test: [50/79]\tTime 0.041 (0.030)\tLoss 0.8730 (0.8896)\tPrec@1 89.844 (87.546)\n",
            " * Prec@1 87.520\n",
            "\n",
            "Epoch: 76/100\n",
            "Learning rate: 0.014730\n",
            "Epoch: [75][0/391]\tTime 0.475 (0.475)\tData 0.402 (0.402)\tLoss 0.9915 (0.9915)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [75][50/391]\tTime 0.057 (0.099)\tData 0.003 (0.047)\tLoss 1.4950 (1.3993)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [75][100/391]\tTime 0.127 (0.093)\tData 0.076 (0.041)\tLoss 1.4206 (1.4173)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [75][150/391]\tTime 0.052 (0.096)\tData 0.005 (0.045)\tLoss 1.6340 (1.4268)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [75][200/391]\tTime 0.068 (0.094)\tData 0.021 (0.044)\tLoss 1.3797 (1.4199)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [75][250/391]\tTime 0.049 (0.092)\tData 0.000 (0.043)\tLoss 1.1302 (1.4180)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [75][300/391]\tTime 0.101 (0.095)\tData 0.060 (0.045)\tLoss 1.1965 (1.4137)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [75][350/391]\tTime 0.043 (0.093)\tData 0.000 (0.044)\tLoss 1.7747 (1.4126)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.130 (0.130)\tLoss 0.8108 (0.8108)\tPrec@1 89.062 (89.062)\n",
            "Test: [50/79]\tTime 0.068 (0.038)\tLoss 0.8172 (0.8399)\tPrec@1 89.844 (88.557)\n",
            " * Prec@1 88.740\n",
            "\n",
            "Epoch: 77/100\n",
            "Learning rate: 0.013638\n",
            "Epoch: [76][0/391]\tTime 0.287 (0.287)\tData 0.215 (0.215)\tLoss 1.6901 (1.6901)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [76][50/391]\tTime 0.098 (0.089)\tData 0.053 (0.039)\tLoss 1.5581 (1.4265)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [76][100/391]\tTime 0.129 (0.087)\tData 0.074 (0.037)\tLoss 1.7386 (1.4363)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [76][150/391]\tTime 0.070 (0.093)\tData 0.001 (0.041)\tLoss 1.0558 (1.4380)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [76][200/391]\tTime 0.099 (0.091)\tData 0.055 (0.041)\tLoss 1.1867 (1.4590)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [76][250/391]\tTime 0.204 (0.091)\tData 0.162 (0.041)\tLoss 1.2994 (1.4648)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [76][300/391]\tTime 0.151 (0.093)\tData 0.111 (0.043)\tLoss 1.6559 (1.4655)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [76][350/391]\tTime 0.084 (0.092)\tData 0.045 (0.043)\tLoss 1.0872 (1.4584)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.235 (0.235)\tLoss 0.8255 (0.8255)\tPrec@1 92.188 (92.188)\n",
            "Test: [50/79]\tTime 0.032 (0.038)\tLoss 0.8775 (0.8582)\tPrec@1 86.719 (89.568)\n",
            " * Prec@1 89.570\n",
            "\n",
            "Epoch: 78/100\n",
            "Learning rate: 0.012582\n",
            "Epoch: [77][0/391]\tTime 0.281 (0.281)\tData 0.210 (0.210)\tLoss 1.6294 (1.6294)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [77][50/391]\tTime 0.122 (0.089)\tData 0.076 (0.039)\tLoss 1.2872 (1.3633)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [77][100/391]\tTime 0.067 (0.090)\tData 0.000 (0.041)\tLoss 1.1117 (1.3853)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [77][150/391]\tTime 0.039 (0.094)\tData 0.000 (0.045)\tLoss 1.7624 (1.3872)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [77][200/391]\tTime 0.044 (0.092)\tData 0.000 (0.044)\tLoss 1.5379 (1.3888)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [77][250/391]\tTime 0.164 (0.094)\tData 0.111 (0.044)\tLoss 1.6811 (1.3914)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [77][300/391]\tTime 0.136 (0.093)\tData 0.090 (0.044)\tLoss 1.1403 (1.3958)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [77][350/391]\tTime 0.110 (0.092)\tData 0.070 (0.043)\tLoss 1.0341 (1.3958)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.142 (0.142)\tLoss 0.8540 (0.8540)\tPrec@1 88.281 (88.281)\n",
            "Test: [50/79]\tTime 0.015 (0.029)\tLoss 0.8511 (0.8381)\tPrec@1 86.719 (89.124)\n",
            " * Prec@1 89.420\n",
            "\n",
            "Epoch: 79/100\n",
            "Learning rate: 0.011563\n",
            "Epoch: [78][0/391]\tTime 0.285 (0.285)\tData 0.211 (0.211)\tLoss 1.6924 (1.6924)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [78][50/391]\tTime 0.107 (0.090)\tData 0.053 (0.041)\tLoss 1.6892 (1.4401)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [78][100/391]\tTime 0.225 (0.093)\tData 0.173 (0.044)\tLoss 1.0617 (1.4377)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [78][150/391]\tTime 0.118 (0.093)\tData 0.067 (0.044)\tLoss 1.7214 (1.4225)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [78][200/391]\tTime 0.050 (0.091)\tData 0.000 (0.041)\tLoss 1.7987 (1.4256)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [78][250/391]\tTime 0.062 (0.093)\tData 0.002 (0.043)\tLoss 1.2517 (1.4259)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [78][300/391]\tTime 0.128 (0.093)\tData 0.089 (0.043)\tLoss 1.5965 (1.4199)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [78][350/391]\tTime 0.147 (0.092)\tData 0.098 (0.043)\tLoss 1.6339 (1.4048)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.146 (0.146)\tLoss 0.8543 (0.8543)\tPrec@1 89.062 (89.062)\n",
            "Test: [50/79]\tTime 0.047 (0.029)\tLoss 0.8421 (0.8652)\tPrec@1 88.281 (88.557)\n",
            " * Prec@1 88.720\n",
            "\n",
            "Epoch: 80/100\n",
            "Learning rate: 0.010582\n",
            "Epoch: [79][0/391]\tTime 0.277 (0.277)\tData 0.211 (0.211)\tLoss 0.9676 (0.9676)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [79][50/391]\tTime 0.124 (0.089)\tData 0.079 (0.038)\tLoss 1.2611 (1.4489)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [79][100/391]\tTime 0.106 (0.096)\tData 0.059 (0.043)\tLoss 1.7208 (1.3965)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [79][150/391]\tTime 0.112 (0.093)\tData 0.058 (0.042)\tLoss 1.4001 (1.4034)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [79][200/391]\tTime 0.111 (0.091)\tData 0.073 (0.041)\tLoss 0.8555 (1.4045)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [79][250/391]\tTime 0.128 (0.094)\tData 0.090 (0.043)\tLoss 1.1477 (1.4204)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [79][300/391]\tTime 0.061 (0.092)\tData 0.002 (0.042)\tLoss 1.0557 (1.4296)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [79][350/391]\tTime 0.057 (0.091)\tData 0.000 (0.042)\tLoss 1.2027 (1.4197)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.130 (0.130)\tLoss 0.8842 (0.8842)\tPrec@1 89.844 (89.844)\n",
            "Test: [50/79]\tTime 0.048 (0.028)\tLoss 0.8580 (0.8744)\tPrec@1 89.844 (88.680)\n",
            " * Prec@1 88.900\n",
            "\n",
            "Epoch: 81/100\n",
            "Learning rate: 0.009640\n",
            "Epoch: [80][0/391]\tTime 0.276 (0.276)\tData 0.211 (0.211)\tLoss 0.9228 (0.9228)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [80][50/391]\tTime 0.054 (0.091)\tData 0.000 (0.041)\tLoss 1.3550 (1.3578)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [80][100/391]\tTime 0.135 (0.098)\tData 0.094 (0.046)\tLoss 1.4097 (1.3967)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [80][150/391]\tTime 0.065 (0.094)\tData 0.011 (0.044)\tLoss 1.6675 (1.3876)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [80][200/391]\tTime 0.108 (0.092)\tData 0.050 (0.042)\tLoss 1.4557 (1.4116)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [80][250/391]\tTime 0.068 (0.094)\tData 0.020 (0.044)\tLoss 1.6898 (1.4219)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [80][300/391]\tTime 0.058 (0.093)\tData 0.008 (0.044)\tLoss 0.9024 (1.4117)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [80][350/391]\tTime 0.062 (0.095)\tData 0.000 (0.045)\tLoss 0.8246 (1.4056)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.158 (0.158)\tLoss 0.8882 (0.8882)\tPrec@1 85.938 (85.938)\n",
            "Test: [50/79]\tTime 0.045 (0.029)\tLoss 0.8817 (0.8707)\tPrec@1 88.281 (89.185)\n",
            " * Prec@1 89.340\n",
            "\n",
            "Epoch: 82/100\n",
            "Learning rate: 0.008737\n",
            "Epoch: [81][0/391]\tTime 0.299 (0.299)\tData 0.219 (0.219)\tLoss 0.8951 (0.8951)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [81][50/391]\tTime 0.059 (0.089)\tData 0.000 (0.038)\tLoss 1.1216 (1.3142)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [81][100/391]\tTime 0.094 (0.097)\tData 0.044 (0.046)\tLoss 1.2182 (1.3655)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [81][150/391]\tTime 0.040 (0.093)\tData 0.000 (0.044)\tLoss 1.6611 (1.3581)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [81][200/391]\tTime 0.058 (0.096)\tData 0.000 (0.046)\tLoss 1.8251 (1.3715)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [81][250/391]\tTime 0.123 (0.094)\tData 0.065 (0.044)\tLoss 1.4265 (1.3840)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [81][300/391]\tTime 0.147 (0.093)\tData 0.105 (0.043)\tLoss 1.4965 (1.3746)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [81][350/391]\tTime 0.104 (0.095)\tData 0.064 (0.045)\tLoss 1.7026 (1.3760)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.131 (0.131)\tLoss 0.8444 (0.8444)\tPrec@1 89.844 (89.844)\n",
            "Test: [50/79]\tTime 0.009 (0.029)\tLoss 0.8191 (0.8408)\tPrec@1 93.750 (90.104)\n",
            " * Prec@1 90.020\n",
            "\n",
            "Epoch: 83/100\n",
            "Learning rate: 0.007876\n",
            "Epoch: [82][0/391]\tTime 0.300 (0.300)\tData 0.227 (0.227)\tLoss 1.3307 (1.3307)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [82][50/391]\tTime 0.144 (0.106)\tData 0.081 (0.054)\tLoss 1.6851 (1.3582)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [82][100/391]\tTime 0.060 (0.098)\tData 0.011 (0.049)\tLoss 1.6851 (1.3539)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [82][150/391]\tTime 0.053 (0.095)\tData 0.005 (0.045)\tLoss 0.8946 (1.3703)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [82][200/391]\tTime 0.138 (0.097)\tData 0.091 (0.048)\tLoss 1.0034 (1.3694)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [82][250/391]\tTime 0.124 (0.096)\tData 0.066 (0.046)\tLoss 1.3507 (1.3677)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [82][300/391]\tTime 0.107 (0.094)\tData 0.049 (0.045)\tLoss 1.0454 (1.3721)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [82][350/391]\tTime 0.099 (0.096)\tData 0.050 (0.046)\tLoss 1.5897 (1.3792)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.143 (0.143)\tLoss 0.8034 (0.8034)\tPrec@1 89.844 (89.844)\n",
            "Test: [50/79]\tTime 0.036 (0.029)\tLoss 0.8077 (0.8132)\tPrec@1 91.406 (90.809)\n",
            " * Prec@1 90.660\n",
            "\n",
            "Epoch: 84/100\n",
            "Learning rate: 0.007056\n",
            "Epoch: [83][0/391]\tTime 0.301 (0.301)\tData 0.227 (0.227)\tLoss 1.5763 (1.5763)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [83][50/391]\tTime 0.058 (0.111)\tData 0.006 (0.056)\tLoss 0.8786 (1.4011)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [83][100/391]\tTime 0.105 (0.098)\tData 0.057 (0.046)\tLoss 1.2618 (1.4115)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [83][150/391]\tTime 0.072 (0.095)\tData 0.019 (0.045)\tLoss 1.1785 (1.4024)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [83][200/391]\tTime 0.138 (0.099)\tData 0.098 (0.048)\tLoss 1.5847 (1.4061)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [83][250/391]\tTime 0.139 (0.097)\tData 0.084 (0.047)\tLoss 1.6462 (1.3988)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [83][300/391]\tTime 0.263 (0.099)\tData 0.208 (0.049)\tLoss 1.5081 (1.4036)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [83][350/391]\tTime 0.139 (0.098)\tData 0.099 (0.048)\tLoss 1.5313 (1.4040)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.128 (0.128)\tLoss 0.8262 (0.8262)\tPrec@1 90.625 (90.625)\n",
            "Test: [50/79]\tTime 0.045 (0.029)\tLoss 0.8157 (0.8324)\tPrec@1 90.625 (90.165)\n",
            " * Prec@1 90.350\n",
            "\n",
            "Epoch: 85/100\n",
            "Learning rate: 0.006278\n",
            "Epoch: [84][0/391]\tTime 0.502 (0.502)\tData 0.400 (0.400)\tLoss 0.9032 (0.9032)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [84][50/391]\tTime 0.066 (0.108)\tData 0.011 (0.055)\tLoss 1.4227 (1.3486)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [84][100/391]\tTime 0.053 (0.098)\tData 0.007 (0.048)\tLoss 1.6429 (1.3984)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [84][150/391]\tTime 0.042 (0.101)\tData 0.000 (0.050)\tLoss 0.9189 (1.4012)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [84][200/391]\tTime 0.055 (0.097)\tData 0.000 (0.047)\tLoss 1.6764 (1.4109)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [84][250/391]\tTime 0.065 (0.096)\tData 0.007 (0.046)\tLoss 0.8797 (1.4116)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [84][300/391]\tTime 0.042 (0.097)\tData 0.003 (0.047)\tLoss 1.6247 (1.4149)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [84][350/391]\tTime 0.113 (0.096)\tData 0.061 (0.047)\tLoss 1.5038 (1.4122)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.135 (0.135)\tLoss 0.8167 (0.8167)\tPrec@1 91.406 (91.406)\n",
            "Test: [50/79]\tTime 0.013 (0.039)\tLoss 0.8016 (0.8333)\tPrec@1 92.188 (90.303)\n",
            " * Prec@1 90.570\n",
            "\n",
            "Epoch: 86/100\n",
            "Learning rate: 0.005544\n",
            "Epoch: [85][0/391]\tTime 0.293 (0.293)\tData 0.218 (0.218)\tLoss 1.5420 (1.5420)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [85][50/391]\tTime 0.073 (0.090)\tData 0.026 (0.041)\tLoss 0.8774 (1.3487)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [85][100/391]\tTime 0.046 (0.089)\tData 0.000 (0.039)\tLoss 1.5288 (1.3332)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [85][150/391]\tTime 0.056 (0.095)\tData 0.000 (0.043)\tLoss 1.3760 (1.3692)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [85][200/391]\tTime 0.114 (0.093)\tData 0.054 (0.042)\tLoss 1.3771 (1.3728)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [85][250/391]\tTime 0.169 (0.093)\tData 0.064 (0.041)\tLoss 1.6125 (1.3764)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [85][300/391]\tTime 0.054 (0.095)\tData 0.012 (0.043)\tLoss 1.6827 (1.3918)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [85][350/391]\tTime 0.091 (0.094)\tData 0.039 (0.043)\tLoss 1.0851 (1.3774)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.173 (0.173)\tLoss 0.8010 (0.8010)\tPrec@1 93.750 (93.750)\n",
            "Test: [50/79]\tTime 0.020 (0.037)\tLoss 0.7983 (0.8263)\tPrec@1 92.969 (90.656)\n",
            " * Prec@1 90.940\n",
            "\n",
            "Epoch: 87/100\n",
            "Learning rate: 0.004854\n",
            "Epoch: [86][0/391]\tTime 0.333 (0.333)\tData 0.259 (0.259)\tLoss 1.6939 (1.6939)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [86][50/391]\tTime 0.042 (0.091)\tData 0.000 (0.042)\tLoss 1.6368 (1.4011)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [86][100/391]\tTime 0.251 (0.096)\tData 0.209 (0.045)\tLoss 1.6779 (1.4065)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [86][150/391]\tTime 0.143 (0.096)\tData 0.079 (0.046)\tLoss 1.2114 (1.3842)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [86][200/391]\tTime 0.121 (0.094)\tData 0.076 (0.044)\tLoss 0.9604 (1.3911)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [86][250/391]\tTime 0.043 (0.097)\tData 0.000 (0.046)\tLoss 1.0289 (1.3880)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [86][300/391]\tTime 0.133 (0.096)\tData 0.095 (0.046)\tLoss 1.5369 (1.3824)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [86][350/391]\tTime 0.148 (0.095)\tData 0.086 (0.046)\tLoss 0.8710 (1.3774)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.135 (0.135)\tLoss 0.8663 (0.8663)\tPrec@1 90.625 (90.625)\n",
            "Test: [50/79]\tTime 0.015 (0.030)\tLoss 0.8068 (0.8455)\tPrec@1 92.969 (90.441)\n",
            " * Prec@1 90.660\n",
            "\n",
            "Epoch: 88/100\n",
            "Learning rate: 0.004208\n",
            "Epoch: [87][0/391]\tTime 0.286 (0.286)\tData 0.215 (0.215)\tLoss 1.7064 (1.7064)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [87][50/391]\tTime 0.148 (0.093)\tData 0.101 (0.045)\tLoss 0.8877 (1.4165)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [87][100/391]\tTime 0.042 (0.100)\tData 0.000 (0.049)\tLoss 1.5985 (1.4158)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [87][150/391]\tTime 0.099 (0.097)\tData 0.059 (0.047)\tLoss 1.6984 (1.4093)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [87][200/391]\tTime 0.105 (0.094)\tData 0.052 (0.044)\tLoss 1.5616 (1.4084)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [87][250/391]\tTime 0.065 (0.097)\tData 0.000 (0.047)\tLoss 1.4984 (1.3968)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [87][300/391]\tTime 0.049 (0.096)\tData 0.000 (0.046)\tLoss 1.4594 (1.3892)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [87][350/391]\tTime 0.107 (0.097)\tData 0.047 (0.047)\tLoss 1.6668 (1.3925)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.146 (0.146)\tLoss 0.7984 (0.7984)\tPrec@1 91.406 (91.406)\n",
            "Test: [50/79]\tTime 0.009 (0.029)\tLoss 0.8009 (0.8149)\tPrec@1 91.406 (91.146)\n",
            " * Prec@1 91.420\n",
            "\n",
            "Epoch: 89/100\n",
            "Learning rate: 0.003608\n",
            "Epoch: [88][0/391]\tTime 0.286 (0.286)\tData 0.217 (0.217)\tLoss 1.1418 (1.1418)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [88][50/391]\tTime 0.234 (0.095)\tData 0.161 (0.044)\tLoss 1.6435 (1.4274)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [88][100/391]\tTime 0.134 (0.098)\tData 0.096 (0.048)\tLoss 1.4136 (1.3872)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [88][150/391]\tTime 0.083 (0.095)\tData 0.035 (0.045)\tLoss 1.6329 (1.3875)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [88][200/391]\tTime 0.252 (0.097)\tData 0.191 (0.047)\tLoss 0.8043 (1.3916)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [88][250/391]\tTime 0.113 (0.095)\tData 0.075 (0.045)\tLoss 1.7835 (1.3913)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [88][300/391]\tTime 0.136 (0.095)\tData 0.096 (0.045)\tLoss 1.6341 (1.3875)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [88][350/391]\tTime 0.135 (0.096)\tData 0.085 (0.047)\tLoss 0.8738 (1.3875)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.139 (0.139)\tLoss 0.8150 (0.8150)\tPrec@1 91.406 (91.406)\n",
            "Test: [50/79]\tTime 0.036 (0.030)\tLoss 0.7967 (0.8201)\tPrec@1 91.406 (91.268)\n",
            " * Prec@1 91.360\n",
            "\n",
            "Epoch: 90/100\n",
            "Learning rate: 0.003053\n",
            "Epoch: [89][0/391]\tTime 0.283 (0.283)\tData 0.214 (0.214)\tLoss 1.6326 (1.6326)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [89][50/391]\tTime 0.131 (0.111)\tData 0.090 (0.056)\tLoss 1.0506 (1.3508)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [89][100/391]\tTime 0.149 (0.100)\tData 0.101 (0.048)\tLoss 1.4280 (1.3352)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [89][150/391]\tTime 0.104 (0.096)\tData 0.048 (0.044)\tLoss 1.1431 (1.3448)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [89][200/391]\tTime 0.131 (0.099)\tData 0.092 (0.047)\tLoss 1.4459 (1.3516)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [89][250/391]\tTime 0.136 (0.097)\tData 0.097 (0.046)\tLoss 1.2726 (1.3541)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [89][300/391]\tTime 0.221 (0.097)\tData 0.145 (0.047)\tLoss 0.8841 (1.3572)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [89][350/391]\tTime 0.153 (0.097)\tData 0.092 (0.047)\tLoss 1.1394 (1.3610)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.144 (0.144)\tLoss 0.7937 (0.7937)\tPrec@1 92.969 (92.969)\n",
            "Test: [50/79]\tTime 0.026 (0.030)\tLoss 0.7754 (0.8048)\tPrec@1 94.531 (91.513)\n",
            " * Prec@1 91.650\n",
            "\n",
            "Epoch: 91/100\n",
            "Learning rate: 0.002545\n",
            "Epoch: [90][0/391]\tTime 0.313 (0.313)\tData 0.241 (0.241)\tLoss 1.2082 (1.2082)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [90][50/391]\tTime 0.040 (0.111)\tData 0.000 (0.059)\tLoss 1.4558 (1.3281)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [90][100/391]\tTime 0.040 (0.100)\tData 0.000 (0.049)\tLoss 1.4930 (1.3318)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [90][150/391]\tTime 0.231 (0.100)\tData 0.168 (0.049)\tLoss 1.6455 (1.3564)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [90][200/391]\tTime 0.100 (0.099)\tData 0.055 (0.048)\tLoss 1.1958 (1.3341)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [90][250/391]\tTime 0.158 (0.097)\tData 0.102 (0.046)\tLoss 1.1504 (1.3465)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [90][300/391]\tTime 0.041 (0.099)\tData 0.000 (0.048)\tLoss 0.8727 (1.3455)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [90][350/391]\tTime 0.041 (0.097)\tData 0.000 (0.047)\tLoss 1.0502 (1.3634)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.137 (0.137)\tLoss 0.7857 (0.7857)\tPrec@1 89.844 (89.844)\n",
            "Test: [50/79]\tTime 0.015 (0.032)\tLoss 0.7699 (0.7886)\tPrec@1 92.188 (91.713)\n",
            " * Prec@1 91.790\n",
            "\n",
            "Epoch: 92/100\n",
            "Learning rate: 0.002083\n",
            "Epoch: [91][0/391]\tTime 0.449 (0.449)\tData 0.351 (0.351)\tLoss 1.0475 (1.0475)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [91][50/391]\tTime 0.110 (0.100)\tData 0.066 (0.047)\tLoss 1.2012 (1.3481)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [91][100/391]\tTime 0.081 (0.095)\tData 0.032 (0.045)\tLoss 1.5022 (1.3420)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [91][150/391]\tTime 0.056 (0.100)\tData 0.006 (0.049)\tLoss 0.9274 (1.3378)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [91][200/391]\tTime 0.041 (0.097)\tData 0.000 (0.048)\tLoss 1.6101 (1.3435)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [91][250/391]\tTime 0.084 (0.096)\tData 0.009 (0.047)\tLoss 1.6855 (1.3549)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [91][300/391]\tTime 0.137 (0.098)\tData 0.093 (0.049)\tLoss 0.9111 (1.3434)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [91][350/391]\tTime 0.105 (0.096)\tData 0.057 (0.047)\tLoss 0.9431 (1.3491)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.169 (0.169)\tLoss 0.8215 (0.8215)\tPrec@1 90.625 (90.625)\n",
            "Test: [50/79]\tTime 0.015 (0.042)\tLoss 0.7862 (0.8090)\tPrec@1 92.188 (91.559)\n",
            " * Prec@1 91.690\n",
            "\n",
            "Epoch: 93/100\n",
            "Learning rate: 0.001669\n",
            "Epoch: [92][0/391]\tTime 0.289 (0.289)\tData 0.211 (0.211)\tLoss 1.6089 (1.6089)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [92][50/391]\tTime 0.056 (0.090)\tData 0.003 (0.042)\tLoss 1.5221 (1.3518)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [92][100/391]\tTime 0.072 (0.089)\tData 0.005 (0.041)\tLoss 1.6253 (1.3454)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [92][150/391]\tTime 0.127 (0.094)\tData 0.076 (0.044)\tLoss 1.0203 (1.3706)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [92][200/391]\tTime 0.043 (0.093)\tData 0.000 (0.043)\tLoss 1.5933 (1.3678)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [92][250/391]\tTime 0.057 (0.096)\tData 0.006 (0.046)\tLoss 1.4888 (1.3577)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [92][300/391]\tTime 0.055 (0.095)\tData 0.002 (0.045)\tLoss 1.4785 (1.3497)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [92][350/391]\tTime 0.054 (0.094)\tData 0.000 (0.044)\tLoss 1.0108 (1.3436)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.142 (0.142)\tLoss 0.8178 (0.8178)\tPrec@1 92.188 (92.188)\n",
            "Test: [50/79]\tTime 0.042 (0.030)\tLoss 0.7830 (0.8112)\tPrec@1 92.969 (91.789)\n",
            " * Prec@1 91.990\n",
            "\n",
            "Epoch: 94/100\n",
            "Learning rate: 0.001303\n",
            "Epoch: [93][0/391]\tTime 0.301 (0.301)\tData 0.227 (0.227)\tLoss 1.1815 (1.1815)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [93][50/391]\tTime 0.041 (0.091)\tData 0.000 (0.042)\tLoss 1.1202 (1.2580)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [93][100/391]\tTime 0.131 (0.101)\tData 0.074 (0.049)\tLoss 1.1787 (1.2930)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [93][150/391]\tTime 0.056 (0.096)\tData 0.005 (0.045)\tLoss 1.5264 (1.3078)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [93][200/391]\tTime 0.050 (0.095)\tData 0.011 (0.045)\tLoss 1.5212 (1.3313)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [93][250/391]\tTime 0.046 (0.097)\tData 0.000 (0.047)\tLoss 1.4931 (1.3373)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [93][300/391]\tTime 0.117 (0.095)\tData 0.071 (0.046)\tLoss 1.0018 (1.3446)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [93][350/391]\tTime 0.057 (0.096)\tData 0.000 (0.046)\tLoss 1.6836 (1.3365)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.136 (0.136)\tLoss 0.7732 (0.7732)\tPrec@1 91.406 (91.406)\n",
            "Test: [50/79]\tTime 0.017 (0.029)\tLoss 0.7434 (0.7736)\tPrec@1 93.750 (91.820)\n",
            " * Prec@1 92.100\n",
            "\n",
            "Epoch: 95/100\n",
            "Learning rate: 0.000985\n",
            "Epoch: [94][0/391]\tTime 0.303 (0.303)\tData 0.227 (0.227)\tLoss 1.2500 (1.2500)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [94][50/391]\tTime 0.136 (0.091)\tData 0.095 (0.044)\tLoss 1.1812 (1.3351)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [94][100/391]\tTime 0.096 (0.098)\tData 0.057 (0.050)\tLoss 1.5346 (1.3437)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [94][150/391]\tTime 0.140 (0.095)\tData 0.101 (0.048)\tLoss 1.5881 (1.3510)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [94][200/391]\tTime 0.158 (0.097)\tData 0.112 (0.051)\tLoss 1.5811 (1.3627)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [94][250/391]\tTime 0.127 (0.096)\tData 0.088 (0.049)\tLoss 1.3433 (1.3573)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [94][300/391]\tTime 0.150 (0.095)\tData 0.099 (0.048)\tLoss 1.6246 (1.3503)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [94][350/391]\tTime 0.135 (0.096)\tData 0.095 (0.049)\tLoss 1.4688 (1.3481)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.143 (0.143)\tLoss 0.8212 (0.8212)\tPrec@1 91.406 (91.406)\n",
            "Test: [50/79]\tTime 0.040 (0.030)\tLoss 0.7749 (0.8089)\tPrec@1 93.750 (91.942)\n",
            " * Prec@1 92.080\n",
            "\n",
            "Epoch: 96/100\n",
            "Learning rate: 0.000715\n",
            "Epoch: [95][0/391]\tTime 0.302 (0.302)\tData 0.232 (0.232)\tLoss 1.5201 (1.5201)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [95][50/391]\tTime 0.147 (0.107)\tData 0.091 (0.052)\tLoss 1.1455 (1.2748)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [95][100/391]\tTime 0.127 (0.100)\tData 0.077 (0.048)\tLoss 1.2887 (1.3002)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [95][150/391]\tTime 0.041 (0.096)\tData 0.000 (0.046)\tLoss 1.1614 (1.3407)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [95][200/391]\tTime 0.135 (0.099)\tData 0.093 (0.049)\tLoss 1.3858 (1.3514)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [95][250/391]\tTime 0.051 (0.097)\tData 0.000 (0.047)\tLoss 1.4751 (1.3409)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [95][300/391]\tTime 0.129 (0.096)\tData 0.089 (0.046)\tLoss 1.6671 (1.3364)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [95][350/391]\tTime 0.139 (0.097)\tData 0.092 (0.047)\tLoss 1.3938 (1.3406)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.142 (0.142)\tLoss 0.7804 (0.7804)\tPrec@1 92.188 (92.188)\n",
            "Test: [50/79]\tTime 0.017 (0.029)\tLoss 0.7365 (0.7742)\tPrec@1 94.531 (92.188)\n",
            " * Prec@1 92.240\n",
            "\n",
            "Epoch: 97/100\n",
            "Learning rate: 0.000494\n",
            "Epoch: [96][0/391]\tTime 0.303 (0.303)\tData 0.227 (0.227)\tLoss 1.6271 (1.6271)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [96][50/391]\tTime 0.046 (0.108)\tData 0.000 (0.053)\tLoss 1.6406 (1.3688)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [96][100/391]\tTime 0.056 (0.098)\tData 0.000 (0.048)\tLoss 1.3571 (1.3461)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [96][150/391]\tTime 0.203 (0.096)\tData 0.132 (0.046)\tLoss 1.0810 (1.3389)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [96][200/391]\tTime 0.072 (0.097)\tData 0.024 (0.047)\tLoss 0.9069 (1.3368)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [96][250/391]\tTime 0.135 (0.095)\tData 0.085 (0.045)\tLoss 1.3618 (1.3290)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [96][300/391]\tTime 0.063 (0.097)\tData 0.003 (0.046)\tLoss 1.5960 (1.3353)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [96][350/391]\tTime 0.040 (0.096)\tData 0.000 (0.046)\tLoss 1.4462 (1.3391)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.129 (0.129)\tLoss 0.7757 (0.7757)\tPrec@1 91.406 (91.406)\n",
            "Test: [50/79]\tTime 0.046 (0.031)\tLoss 0.7272 (0.7656)\tPrec@1 95.312 (92.494)\n",
            " * Prec@1 92.470\n",
            "\n",
            "Epoch: 98/100\n",
            "Learning rate: 0.000322\n",
            "Epoch: [97][0/391]\tTime 0.357 (0.357)\tData 0.260 (0.260)\tLoss 1.6211 (1.6211)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [97][50/391]\tTime 0.075 (0.104)\tData 0.036 (0.054)\tLoss 1.4193 (1.3961)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [97][100/391]\tTime 0.055 (0.097)\tData 0.000 (0.049)\tLoss 1.6571 (1.3763)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [97][150/391]\tTime 0.061 (0.100)\tData 0.000 (0.051)\tLoss 1.3682 (1.3658)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [97][200/391]\tTime 0.100 (0.098)\tData 0.053 (0.049)\tLoss 1.6063 (1.3598)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [97][250/391]\tTime 0.136 (0.096)\tData 0.096 (0.047)\tLoss 1.2578 (1.3488)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [97][300/391]\tTime 0.088 (0.098)\tData 0.036 (0.049)\tLoss 1.4997 (1.3446)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [97][350/391]\tTime 0.054 (0.097)\tData 0.004 (0.048)\tLoss 1.3161 (1.3414)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.175 (0.175)\tLoss 0.8018 (0.8018)\tPrec@1 90.625 (90.625)\n",
            "Test: [50/79]\tTime 0.053 (0.047)\tLoss 0.7547 (0.7915)\tPrec@1 95.312 (92.050)\n",
            " * Prec@1 92.180\n",
            "\n",
            "Epoch: 99/100\n",
            "Learning rate: 0.000199\n",
            "Epoch: [98][0/391]\tTime 0.288 (0.288)\tData 0.212 (0.212)\tLoss 1.6159 (1.6159)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [98][50/391]\tTime 0.052 (0.090)\tData 0.000 (0.038)\tLoss 1.6716 (1.3539)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [98][100/391]\tTime 0.051 (0.088)\tData 0.006 (0.038)\tLoss 0.8912 (1.3155)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [98][150/391]\tTime 0.053 (0.094)\tData 0.000 (0.045)\tLoss 1.0496 (1.3394)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [98][200/391]\tTime 0.046 (0.093)\tData 0.000 (0.044)\tLoss 1.1543 (1.3443)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [98][250/391]\tTime 0.056 (0.094)\tData 0.000 (0.045)\tLoss 0.9103 (1.3436)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [98][300/391]\tTime 0.113 (0.094)\tData 0.068 (0.044)\tLoss 1.1587 (1.3513)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [98][350/391]\tTime 0.055 (0.093)\tData 0.001 (0.044)\tLoss 1.0899 (1.3453)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.169 (0.169)\tLoss 0.7801 (0.7801)\tPrec@1 91.406 (91.406)\n",
            "Test: [50/79]\tTime 0.035 (0.030)\tLoss 0.7337 (0.7714)\tPrec@1 95.312 (92.509)\n",
            " * Prec@1 92.480\n",
            "\n",
            "Epoch: 100/100\n",
            "Learning rate: 0.000125\n",
            "Epoch: [99][0/391]\tTime 0.284 (0.284)\tData 0.215 (0.215)\tLoss 1.1207 (1.1207)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [99][50/391]\tTime 0.138 (0.095)\tData 0.076 (0.043)\tLoss 1.3664 (1.3892)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [99][100/391]\tTime 0.067 (0.100)\tData 0.005 (0.045)\tLoss 1.1234 (1.3673)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [99][150/391]\tTime 0.118 (0.097)\tData 0.072 (0.044)\tLoss 1.7332 (1.3511)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [99][200/391]\tTime 0.083 (0.095)\tData 0.035 (0.043)\tLoss 1.1475 (1.3451)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [99][250/391]\tTime 0.072 (0.097)\tData 0.027 (0.045)\tLoss 0.7881 (1.3459)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [99][300/391]\tTime 0.055 (0.095)\tData 0.007 (0.044)\tLoss 1.5832 (1.3563)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [99][350/391]\tTime 0.050 (0.094)\tData 0.000 (0.043)\tLoss 1.1646 (1.3577)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.126 (0.126)\tLoss 0.7945 (0.7945)\tPrec@1 91.406 (91.406)\n",
            "Test: [50/79]\tTime 0.042 (0.030)\tLoss 0.7528 (0.7874)\tPrec@1 95.312 (92.371)\n",
            " * Prec@1 92.360\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(HybridResNet(\n",
              "   (conv1): Conv2d(3, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "   (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "   (layer1): Sequential(\n",
              "     (0): BasicBlock(\n",
              "       (conv1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (dropout): Dropout(p=0.1, inplace=False)\n",
              "       (se): SELayer(\n",
              "         (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "         (fc): Sequential(\n",
              "           (0): Linear(in_features=24, out_features=1, bias=False)\n",
              "           (1): ReLU(inplace=True)\n",
              "           (2): Linear(in_features=1, out_features=24, bias=False)\n",
              "           (3): Sigmoid()\n",
              "         )\n",
              "       )\n",
              "       (shortcut): Sequential()\n",
              "     )\n",
              "     (1): BasicBlock(\n",
              "       (conv1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (dropout): Dropout(p=0.1, inplace=False)\n",
              "       (se): SELayer(\n",
              "         (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "         (fc): Sequential(\n",
              "           (0): Linear(in_features=24, out_features=1, bias=False)\n",
              "           (1): ReLU(inplace=True)\n",
              "           (2): Linear(in_features=1, out_features=24, bias=False)\n",
              "           (3): Sigmoid()\n",
              "         )\n",
              "       )\n",
              "       (shortcut): Sequential()\n",
              "     )\n",
              "   )\n",
              "   (layer2): Sequential(\n",
              "     (0): BasicBlock(\n",
              "       (conv1): Conv2d(24, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (dropout): Dropout(p=0.1, inplace=False)\n",
              "       (se): SELayer(\n",
              "         (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "         (fc): Sequential(\n",
              "           (0): Linear(in_features=48, out_features=3, bias=False)\n",
              "           (1): ReLU(inplace=True)\n",
              "           (2): Linear(in_features=3, out_features=48, bias=False)\n",
              "           (3): Sigmoid()\n",
              "         )\n",
              "       )\n",
              "       (shortcut): Sequential(\n",
              "         (0): Conv2d(24, 48, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "         (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       )\n",
              "     )\n",
              "     (1): BasicBlock(\n",
              "       (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (dropout): Dropout(p=0.1, inplace=False)\n",
              "       (se): SELayer(\n",
              "         (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "         (fc): Sequential(\n",
              "           (0): Linear(in_features=48, out_features=3, bias=False)\n",
              "           (1): ReLU(inplace=True)\n",
              "           (2): Linear(in_features=3, out_features=48, bias=False)\n",
              "           (3): Sigmoid()\n",
              "         )\n",
              "       )\n",
              "       (shortcut): Sequential()\n",
              "     )\n",
              "   )\n",
              "   (layer3): Sequential(\n",
              "     (0): BasicBlock(\n",
              "       (conv1): Conv2d(48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (dropout): Dropout(p=0.1, inplace=False)\n",
              "       (se): SELayer(\n",
              "         (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "         (fc): Sequential(\n",
              "           (0): Linear(in_features=96, out_features=6, bias=False)\n",
              "           (1): ReLU(inplace=True)\n",
              "           (2): Linear(in_features=6, out_features=96, bias=False)\n",
              "           (3): Sigmoid()\n",
              "         )\n",
              "       )\n",
              "       (shortcut): Sequential(\n",
              "         (0): Conv2d(48, 96, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "         (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       )\n",
              "     )\n",
              "     (1): BasicBlock(\n",
              "       (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (dropout): Dropout(p=0.1, inplace=False)\n",
              "       (se): SELayer(\n",
              "         (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "         (fc): Sequential(\n",
              "           (0): Linear(in_features=96, out_features=6, bias=False)\n",
              "           (1): ReLU(inplace=True)\n",
              "           (2): Linear(in_features=6, out_features=96, bias=False)\n",
              "           (3): Sigmoid()\n",
              "         )\n",
              "       )\n",
              "       (shortcut): Sequential()\n",
              "     )\n",
              "   )\n",
              "   (layer4): Sequential(\n",
              "     (0): BasicBlock(\n",
              "       (conv1): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (dropout): Dropout(p=0.1, inplace=False)\n",
              "       (se): SELayer(\n",
              "         (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "         (fc): Sequential(\n",
              "           (0): Linear(in_features=192, out_features=12, bias=False)\n",
              "           (1): ReLU(inplace=True)\n",
              "           (2): Linear(in_features=12, out_features=192, bias=False)\n",
              "           (3): Sigmoid()\n",
              "         )\n",
              "       )\n",
              "       (shortcut): Sequential(\n",
              "         (0): Conv2d(96, 192, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "         (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       )\n",
              "     )\n",
              "     (1): BasicBlock(\n",
              "       (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (dropout): Dropout(p=0.1, inplace=False)\n",
              "       (se): SELayer(\n",
              "         (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "         (fc): Sequential(\n",
              "           (0): Linear(in_features=192, out_features=12, bias=False)\n",
              "           (1): ReLU(inplace=True)\n",
              "           (2): Linear(in_features=12, out_features=192, bias=False)\n",
              "           (3): Sigmoid()\n",
              "         )\n",
              "       )\n",
              "       (shortcut): Sequential()\n",
              "     )\n",
              "   )\n",
              "   (dropout): Dropout(p=0.1, inplace=False)\n",
              "   (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "   (fc): Linear(in_features=192, out_features=10, bias=True)\n",
              " ),\n",
              " 92.48)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}