# ğŸš€ ResNet Training Notebooks

## ğŸŒŸ Overview
This repository contains **Jupyter notebooks** for training a **ResNet** model on a dataset over different numbers of epochs. The models were trained for **50, 100, and 600 epochs**, respectively, to analyze performance improvements with increased training time. The goal was to evaluate how **training duration impacted accuracy, loss, and generalization ability**.

---

## ğŸ“‚ Files

1. ğŸ“Œ **resnet_train_test(50 epochs).ipynb**  
   - ğŸƒâ€â™‚ï¸ Trained a **ResNet model for 50 epochs**.  
   - âš¡ Quick experimentation and establishing a **baseline performance**.
   - ğŸ” Recommended for **early-stage model evaluation**.

2. ğŸ“Œ **resnet_train_test(100 epochs).ipynb**  
   - ğŸ”¬ Trained a **ResNet model for 100 epochs**.  
   - âš–ï¸ Balanced **training time vs model performance**.
   - ğŸ“ˆ Helped analyze improvements over the **50-epoch model**.

3. ğŸ“Œ **resnet_train_test(600 epochs).ipynb**  
   - ğŸ”¥ Trained a **ResNet model for 600 epochs**.  
   - ğŸ¯ Focused on **extensive training for optimal accuracy**.
  

---

## âš™ï¸ Dependencies
Before running the notebooks, make sure the following dependencies were installed:

```bash
pip install torch torchvision numpy matplotlib
```

---

## ğŸ› ï¸ Usage
1. **Open the desired notebook** in Jupyter Notebook or Jupyter Lab:
   ```bash
   jupyter notebook resnet_train_test(50 epochs).ipynb
   ```
2. **Run all cells** sequentially to train the **ResNet model**.
3. **Modify hyperparameters** as needed for experimentation:
   - ğŸ¯ **Batch Size**: Adjust based on **GPU memory** availability.
   - âš¡ **Learning Rate**: Default was typically `0.001`, but could be fine-tuned.
   - ğŸ‹ï¸ **Optimizer**: Used **Adam, SGD**, or other optimizers as needed.
   - ğŸ“Š **Data Augmentation**: Could be added for **better generalization**.

---

## ğŸ“Š Results & Observations
### ğŸ”¹ **Key Insights from Running the Notebooks**:

1. **â³ Training Time vs Accuracy**  
   - âœ… The **50-epoch model** reached a moderate accuracy but may have **underfitted**.
   - âš–ï¸ The **100-epoch model** provided **more stable performance**.
   - ğŸš€ The **600-epoch model** achieved the **best accuracy** but risked **overfitting** if not regularized.

2. **ğŸ“‰ Loss Reduction**  
   - Loss **progressively decreased** over epochs.
   - If loss **stagnated or increased**, hyperparameters needed **adjustments**.

3. **ğŸ›¡ï¸ Overfitting Analysis**
   - ğŸš¨ Higher epochs improved training accuracy, but **validation accuracy needed monitoring**.
   - ğŸ› ï¸ Techniques like **dropout, batch normalization, and learning rate scheduling** helped reduce **overfitting**.

---



