{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0Nv9m0Kc7qLK",
        "outputId": "1e8d2eaa-6fe6-49c1-da47-521441537957"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:05<00:00, 29.3MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Test: [50/79]\tTime 0.017 (0.016)\tLoss 0.9562 (0.9803)\tPrec@1 85.156 (84.053)\n",
            " * Prec@1 84.000\n",
            "\n",
            "Epoch: 244/600\n",
            "Learning rate: 0.064737\n",
            "Epoch: [243][0/391]\tTime 0.199 (0.199)\tData 0.152 (0.152)\tLoss 1.3778 (1.3778)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [243][50/391]\tTime 0.067 (0.051)\tData 0.035 (0.018)\tLoss 1.2999 (1.4243)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [243][100/391]\tTime 0.064 (0.050)\tData 0.032 (0.017)\tLoss 1.7082 (1.4839)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [243][150/391]\tTime 0.062 (0.050)\tData 0.030 (0.017)\tLoss 1.6869 (1.4806)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [243][200/391]\tTime 0.062 (0.049)\tData 0.030 (0.017)\tLoss 1.7781 (1.4914)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [243][250/391]\tTime 0.062 (0.049)\tData 0.030 (0.016)\tLoss 1.1329 (1.4968)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [243][300/391]\tTime 0.063 (0.049)\tData 0.031 (0.016)\tLoss 1.6670 (1.5020)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [243][350/391]\tTime 0.064 (0.049)\tData 0.032 (0.016)\tLoss 1.6798 (1.5061)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 1.0180 (1.0180)\tPrec@1 78.906 (78.906)\n",
            "Test: [50/79]\tTime 0.017 (0.016)\tLoss 0.9704 (1.0134)\tPrec@1 82.812 (81.373)\n",
            " * Prec@1 81.500\n",
            "\n",
            "Epoch: 245/600\n",
            "Learning rate: 0.064487\n",
            "Epoch: [244][0/391]\tTime 0.203 (0.203)\tData 0.156 (0.156)\tLoss 1.7460 (1.7460)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [244][50/391]\tTime 0.041 (0.053)\tData 0.000 (0.018)\tLoss 1.2462 (1.4823)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [244][100/391]\tTime 0.031 (0.051)\tData 0.000 (0.017)\tLoss 1.8553 (1.4872)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [244][150/391]\tTime 0.069 (0.051)\tData 0.037 (0.017)\tLoss 1.7358 (1.4860)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [244][200/391]\tTime 0.069 (0.051)\tData 0.037 (0.018)\tLoss 1.8227 (1.4946)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [244][250/391]\tTime 0.066 (0.051)\tData 0.034 (0.018)\tLoss 1.3655 (1.4905)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [244][300/391]\tTime 0.069 (0.050)\tData 0.036 (0.018)\tLoss 1.6178 (1.5060)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [244][350/391]\tTime 0.069 (0.050)\tData 0.036 (0.017)\tLoss 1.4401 (1.5084)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.9859 (0.9859)\tPrec@1 81.250 (81.250)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.9120 (0.9603)\tPrec@1 85.156 (83.257)\n",
            " * Prec@1 83.330\n",
            "\n",
            "Epoch: 246/600\n",
            "Learning rate: 0.064237\n",
            "Epoch: [245][0/391]\tTime 0.199 (0.199)\tData 0.152 (0.152)\tLoss 1.6361 (1.6361)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [245][50/391]\tTime 0.048 (0.051)\tData 0.016 (0.018)\tLoss 1.2603 (1.4775)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [245][100/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.7178 (1.4804)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [245][150/391]\tTime 0.033 (0.050)\tData 0.000 (0.017)\tLoss 1.8324 (1.5004)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [245][200/391]\tTime 0.063 (0.050)\tData 0.021 (0.017)\tLoss 1.1890 (1.5031)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [245][250/391]\tTime 0.032 (0.050)\tData 0.001 (0.017)\tLoss 1.7946 (1.5039)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [245][300/391]\tTime 0.031 (0.050)\tData 0.000 (0.017)\tLoss 1.8009 (1.5031)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [245][350/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 1.2886 (1.5092)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.104 (0.104)\tLoss 1.0568 (1.0568)\tPrec@1 78.125 (78.125)\n",
            "Test: [50/79]\tTime 0.010 (0.016)\tLoss 1.0354 (1.0276)\tPrec@1 80.469 (82.031)\n",
            " * Prec@1 82.300\n",
            "\n",
            "Epoch: 247/600\n",
            "Learning rate: 0.063986\n",
            "Epoch: [246][0/391]\tTime 0.198 (0.198)\tData 0.153 (0.153)\tLoss 1.7954 (1.7954)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [246][50/391]\tTime 0.064 (0.051)\tData 0.031 (0.018)\tLoss 1.4730 (1.4598)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [246][100/391]\tTime 0.063 (0.050)\tData 0.032 (0.017)\tLoss 1.2485 (1.4797)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [246][150/391]\tTime 0.065 (0.050)\tData 0.033 (0.017)\tLoss 1.3702 (1.4865)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [246][200/391]\tTime 0.067 (0.050)\tData 0.036 (0.017)\tLoss 1.6586 (1.4961)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [246][250/391]\tTime 0.064 (0.050)\tData 0.032 (0.017)\tLoss 1.4913 (1.5084)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [246][300/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.4961 (1.5148)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [246][350/391]\tTime 0.063 (0.049)\tData 0.031 (0.017)\tLoss 1.7156 (1.5104)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 1.0488 (1.0488)\tPrec@1 79.688 (79.688)\n",
            "Test: [50/79]\tTime 0.017 (0.016)\tLoss 1.0424 (1.0419)\tPrec@1 78.125 (79.305)\n",
            " * Prec@1 79.400\n",
            "\n",
            "Epoch: 248/600\n",
            "Learning rate: 0.063734\n",
            "Epoch: [247][0/391]\tTime 0.199 (0.199)\tData 0.153 (0.153)\tLoss 1.6375 (1.6375)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [247][50/391]\tTime 0.067 (0.052)\tData 0.034 (0.018)\tLoss 1.4109 (1.4983)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [247][100/391]\tTime 0.060 (0.051)\tData 0.028 (0.018)\tLoss 1.2098 (1.5128)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [247][150/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.1724 (1.5135)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [247][200/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.7204 (1.5264)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [247][250/391]\tTime 0.062 (0.049)\tData 0.030 (0.016)\tLoss 1.8199 (1.5341)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [247][300/391]\tTime 0.065 (0.049)\tData 0.034 (0.016)\tLoss 1.7861 (1.5352)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [247][350/391]\tTime 0.063 (0.049)\tData 0.032 (0.016)\tLoss 1.5761 (1.5242)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.094 (0.094)\tLoss 0.9218 (0.9218)\tPrec@1 89.844 (89.844)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.9708 (0.9597)\tPrec@1 82.031 (84.835)\n",
            " * Prec@1 85.030\n",
            "\n",
            "Epoch: 249/600\n",
            "Learning rate: 0.063483\n",
            "Epoch: [248][0/391]\tTime 0.198 (0.198)\tData 0.152 (0.152)\tLoss 1.6721 (1.6721)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [248][50/391]\tTime 0.064 (0.051)\tData 0.032 (0.018)\tLoss 1.7306 (1.5246)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [248][100/391]\tTime 0.064 (0.050)\tData 0.033 (0.017)\tLoss 1.7785 (1.5191)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [248][150/391]\tTime 0.064 (0.049)\tData 0.031 (0.017)\tLoss 1.1143 (1.5075)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [248][200/391]\tTime 0.063 (0.049)\tData 0.030 (0.016)\tLoss 1.7144 (1.5171)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [248][250/391]\tTime 0.065 (0.049)\tData 0.032 (0.016)\tLoss 1.3100 (1.5133)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [248][300/391]\tTime 0.035 (0.049)\tData 0.000 (0.016)\tLoss 1.7949 (1.5125)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [248][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 0.9812 (1.5093)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 0.9913 (0.9913)\tPrec@1 86.719 (86.719)\n",
            "Test: [50/79]\tTime 0.012 (0.016)\tLoss 1.0451 (1.0121)\tPrec@1 77.344 (81.081)\n",
            " * Prec@1 81.090\n",
            "\n",
            "Epoch: 250/600\n",
            "Learning rate: 0.063230\n",
            "Epoch: [249][0/391]\tTime 0.208 (0.208)\tData 0.151 (0.151)\tLoss 1.8682 (1.8682)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [249][50/391]\tTime 0.063 (0.050)\tData 0.031 (0.018)\tLoss 1.4248 (1.5018)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [249][100/391]\tTime 0.067 (0.049)\tData 0.034 (0.017)\tLoss 1.0814 (1.5155)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [249][150/391]\tTime 0.066 (0.049)\tData 0.034 (0.017)\tLoss 1.5889 (1.5140)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [249][200/391]\tTime 0.061 (0.049)\tData 0.028 (0.016)\tLoss 1.8484 (1.5195)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [249][250/391]\tTime 0.064 (0.049)\tData 0.033 (0.016)\tLoss 1.7594 (1.5152)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [249][300/391]\tTime 0.065 (0.049)\tData 0.033 (0.016)\tLoss 1.8282 (1.5122)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [249][350/391]\tTime 0.055 (0.049)\tData 0.022 (0.016)\tLoss 1.3067 (1.5158)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.105 (0.105)\tLoss 1.0925 (1.0925)\tPrec@1 78.125 (78.125)\n",
            "Test: [50/79]\tTime 0.010 (0.016)\tLoss 1.0512 (1.0236)\tPrec@1 77.344 (80.162)\n",
            " * Prec@1 80.380\n",
            "\n",
            "Epoch: 251/600\n",
            "Learning rate: 0.062978\n",
            "Epoch: [250][0/391]\tTime 0.199 (0.199)\tData 0.151 (0.151)\tLoss 1.6828 (1.6828)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [250][50/391]\tTime 0.064 (0.051)\tData 0.032 (0.018)\tLoss 1.6771 (1.5022)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [250][100/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.1332 (1.4697)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [250][150/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.5462 (1.4934)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [250][200/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.1545 (1.4925)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [250][250/391]\tTime 0.031 (0.049)\tData 0.000 (0.016)\tLoss 1.4938 (1.5074)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [250][300/391]\tTime 0.069 (0.049)\tData 0.036 (0.016)\tLoss 1.5717 (1.5185)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [250][350/391]\tTime 0.065 (0.049)\tData 0.032 (0.016)\tLoss 1.8048 (1.5215)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.109 (0.109)\tLoss 1.0514 (1.0514)\tPrec@1 81.250 (81.250)\n",
            "Test: [50/79]\tTime 0.010 (0.017)\tLoss 0.9951 (1.0660)\tPrec@1 82.812 (78.922)\n",
            " * Prec@1 78.410\n",
            "\n",
            "Epoch: 252/600\n",
            "Learning rate: 0.062725\n",
            "Epoch: [251][0/391]\tTime 0.211 (0.211)\tData 0.154 (0.154)\tLoss 1.7831 (1.7831)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [251][50/391]\tTime 0.074 (0.060)\tData 0.032 (0.018)\tLoss 1.8413 (1.4856)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [251][100/391]\tTime 0.032 (0.058)\tData 0.000 (0.016)\tLoss 1.7881 (1.5243)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [251][150/391]\tTime 0.032 (0.055)\tData 0.000 (0.016)\tLoss 1.0486 (1.5107)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [251][200/391]\tTime 0.065 (0.054)\tData 0.032 (0.016)\tLoss 0.9738 (1.5119)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [251][250/391]\tTime 0.058 (0.053)\tData 0.026 (0.016)\tLoss 1.5977 (1.5102)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [251][300/391]\tTime 0.031 (0.052)\tData 0.000 (0.016)\tLoss 1.5190 (1.5143)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [251][350/391]\tTime 0.032 (0.052)\tData 0.000 (0.017)\tLoss 1.7631 (1.5170)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.096 (0.096)\tLoss 1.0762 (1.0762)\tPrec@1 80.469 (80.469)\n",
            "Test: [50/79]\tTime 0.017 (0.016)\tLoss 1.1857 (1.0954)\tPrec@1 73.438 (78.355)\n",
            " * Prec@1 78.350\n",
            "\n",
            "Epoch: 253/600\n",
            "Learning rate: 0.062472\n",
            "Epoch: [252][0/391]\tTime 0.216 (0.216)\tData 0.157 (0.157)\tLoss 1.6696 (1.6696)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [252][50/391]\tTime 0.032 (0.052)\tData 0.000 (0.017)\tLoss 1.6874 (1.5618)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [252][100/391]\tTime 0.032 (0.050)\tData 0.000 (0.016)\tLoss 1.1596 (1.5413)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [252][150/391]\tTime 0.031 (0.049)\tData 0.000 (0.016)\tLoss 1.7592 (1.5174)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [252][200/391]\tTime 0.031 (0.049)\tData 0.000 (0.016)\tLoss 1.5274 (1.5093)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [252][250/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.8165 (1.5179)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [252][300/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 1.1779 (1.5117)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [252][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.7638 (1.5112)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 1.1292 (1.1292)\tPrec@1 75.000 (75.000)\n",
            "Test: [50/79]\tTime 0.014 (0.016)\tLoss 1.1491 (1.0896)\tPrec@1 74.219 (77.282)\n",
            " * Prec@1 77.340\n",
            "\n",
            "Epoch: 254/600\n",
            "Learning rate: 0.062219\n",
            "Epoch: [253][0/391]\tTime 0.199 (0.199)\tData 0.152 (0.152)\tLoss 1.6330 (1.6330)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [253][50/391]\tTime 0.069 (0.051)\tData 0.035 (0.018)\tLoss 1.5541 (1.5578)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [253][100/391]\tTime 0.064 (0.050)\tData 0.031 (0.017)\tLoss 1.2351 (1.5339)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [253][150/391]\tTime 0.066 (0.049)\tData 0.034 (0.017)\tLoss 1.7077 (1.5395)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [253][200/391]\tTime 0.067 (0.049)\tData 0.034 (0.017)\tLoss 1.7238 (1.5366)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [253][250/391]\tTime 0.067 (0.049)\tData 0.034 (0.017)\tLoss 1.4163 (1.5290)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [253][300/391]\tTime 0.050 (0.049)\tData 0.018 (0.017)\tLoss 1.8345 (1.5298)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [253][350/391]\tTime 0.063 (0.049)\tData 0.031 (0.016)\tLoss 1.3026 (1.5217)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 1.0322 (1.0322)\tPrec@1 78.906 (78.906)\n",
            "Test: [50/79]\tTime 0.017 (0.016)\tLoss 1.0229 (0.9951)\tPrec@1 81.250 (82.353)\n",
            " * Prec@1 82.330\n",
            "\n",
            "Epoch: 255/600\n",
            "Learning rate: 0.061965\n",
            "Epoch: [254][0/391]\tTime 0.197 (0.197)\tData 0.151 (0.151)\tLoss 1.2378 (1.2378)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [254][50/391]\tTime 0.055 (0.050)\tData 0.022 (0.018)\tLoss 1.6817 (1.5286)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [254][100/391]\tTime 0.069 (0.050)\tData 0.035 (0.017)\tLoss 1.7214 (1.5147)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [254][150/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.4766 (1.4833)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [254][200/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.3886 (1.5012)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [254][250/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.3402 (1.4993)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [254][300/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.8422 (1.5004)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [254][350/391]\tTime 0.059 (0.049)\tData 0.027 (0.016)\tLoss 1.2689 (1.5070)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.094 (0.094)\tLoss 0.9235 (0.9235)\tPrec@1 85.938 (85.938)\n",
            "Test: [50/79]\tTime 0.010 (0.016)\tLoss 0.9456 (0.9263)\tPrec@1 82.812 (84.620)\n",
            " * Prec@1 84.650\n",
            "\n",
            "Epoch: 256/600\n",
            "Learning rate: 0.061711\n",
            "Epoch: [255][0/391]\tTime 0.199 (0.199)\tData 0.152 (0.152)\tLoss 1.5473 (1.5473)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [255][50/391]\tTime 0.065 (0.051)\tData 0.033 (0.019)\tLoss 1.7192 (1.5173)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [255][100/391]\tTime 0.064 (0.049)\tData 0.033 (0.018)\tLoss 1.6852 (1.5086)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [255][150/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 1.8731 (1.5084)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [255][200/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 1.7140 (1.5017)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [255][250/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.7721 (1.5026)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [255][300/391]\tTime 0.040 (0.049)\tData 0.008 (0.017)\tLoss 1.6008 (1.5057)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [255][350/391]\tTime 0.039 (0.049)\tData 0.007 (0.017)\tLoss 1.5427 (1.5122)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.099 (0.099)\tLoss 0.8956 (0.8956)\tPrec@1 86.719 (86.719)\n",
            "Test: [50/79]\tTime 0.022 (0.017)\tLoss 0.9426 (0.9449)\tPrec@1 84.375 (83.333)\n",
            " * Prec@1 83.240\n",
            "\n",
            "Epoch: 257/600\n",
            "Learning rate: 0.061456\n",
            "Epoch: [256][0/391]\tTime 0.202 (0.202)\tData 0.156 (0.156)\tLoss 1.1676 (1.1676)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [256][50/391]\tTime 0.064 (0.051)\tData 0.032 (0.019)\tLoss 1.3461 (1.5149)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [256][100/391]\tTime 0.063 (0.050)\tData 0.031 (0.018)\tLoss 1.3306 (1.5286)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [256][150/391]\tTime 0.063 (0.049)\tData 0.031 (0.017)\tLoss 1.7390 (1.5138)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [256][200/391]\tTime 0.064 (0.049)\tData 0.031 (0.017)\tLoss 1.4398 (1.5072)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [256][250/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.5614 (1.4979)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [256][300/391]\tTime 0.064 (0.049)\tData 0.031 (0.017)\tLoss 1.6857 (1.4888)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [256][350/391]\tTime 0.067 (0.049)\tData 0.035 (0.017)\tLoss 1.6890 (1.4939)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.092 (0.092)\tLoss 0.9572 (0.9572)\tPrec@1 86.719 (86.719)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 1.0680 (1.0116)\tPrec@1 80.469 (82.261)\n",
            " * Prec@1 82.540\n",
            "\n",
            "Epoch: 258/600\n",
            "Learning rate: 0.061201\n",
            "Epoch: [257][0/391]\tTime 0.213 (0.213)\tData 0.154 (0.154)\tLoss 1.2876 (1.2876)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [257][50/391]\tTime 0.064 (0.052)\tData 0.032 (0.018)\tLoss 1.7980 (1.5203)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [257][100/391]\tTime 0.039 (0.050)\tData 0.007 (0.017)\tLoss 1.2352 (1.5374)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [257][150/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.1971 (1.5477)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [257][200/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.7405 (1.5546)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [257][250/391]\tTime 0.033 (0.049)\tData 0.000 (0.016)\tLoss 0.9618 (1.5462)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [257][300/391]\tTime 0.050 (0.049)\tData 0.018 (0.016)\tLoss 1.1314 (1.5355)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [257][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.1919 (1.5319)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.094 (0.094)\tLoss 0.9314 (0.9314)\tPrec@1 86.719 (86.719)\n",
            "Test: [50/79]\tTime 0.011 (0.016)\tLoss 1.0361 (0.9844)\tPrec@1 78.906 (82.292)\n",
            " * Prec@1 82.540\n",
            "\n",
            "Epoch: 259/600\n",
            "Learning rate: 0.060946\n",
            "Epoch: [258][0/391]\tTime 0.198 (0.198)\tData 0.153 (0.153)\tLoss 1.5312 (1.5312)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [258][50/391]\tTime 0.075 (0.051)\tData 0.037 (0.018)\tLoss 1.7374 (1.5736)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [258][100/391]\tTime 0.068 (0.050)\tData 0.035 (0.017)\tLoss 1.7670 (1.5457)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [258][150/391]\tTime 0.064 (0.050)\tData 0.033 (0.017)\tLoss 1.7937 (1.5554)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [258][200/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.2029 (1.5481)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [258][250/391]\tTime 0.060 (0.049)\tData 0.029 (0.017)\tLoss 1.1783 (1.5454)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [258][300/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 1.8761 (1.5427)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [258][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.6269 (1.5306)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.093 (0.093)\tLoss 1.0637 (1.0637)\tPrec@1 78.906 (78.906)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 1.0477 (1.0124)\tPrec@1 80.469 (81.970)\n",
            " * Prec@1 81.830\n",
            "\n",
            "Epoch: 260/600\n",
            "Learning rate: 0.060691\n",
            "Epoch: [259][0/391]\tTime 0.196 (0.196)\tData 0.151 (0.151)\tLoss 1.7058 (1.7058)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [259][50/391]\tTime 0.064 (0.051)\tData 0.031 (0.018)\tLoss 1.7066 (1.5378)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [259][100/391]\tTime 0.065 (0.050)\tData 0.033 (0.017)\tLoss 1.7537 (1.5416)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [259][150/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.6720 (1.5405)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [259][200/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.1927 (1.5235)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [259][250/391]\tTime 0.066 (0.049)\tData 0.034 (0.017)\tLoss 1.6545 (1.5227)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [259][300/391]\tTime 0.062 (0.049)\tData 0.030 (0.017)\tLoss 1.2381 (1.5230)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [259][350/391]\tTime 0.062 (0.049)\tData 0.030 (0.016)\tLoss 1.1553 (1.5269)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.094 (0.094)\tLoss 1.0018 (1.0018)\tPrec@1 82.812 (82.812)\n",
            "Test: [50/79]\tTime 0.015 (0.016)\tLoss 1.0305 (0.9698)\tPrec@1 78.125 (84.115)\n",
            " * Prec@1 84.170\n",
            "\n",
            "Epoch: 261/600\n",
            "Learning rate: 0.060435\n",
            "Epoch: [260][0/391]\tTime 0.200 (0.200)\tData 0.154 (0.154)\tLoss 1.4795 (1.4795)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [260][50/391]\tTime 0.064 (0.051)\tData 0.032 (0.019)\tLoss 1.7351 (1.5246)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [260][100/391]\tTime 0.065 (0.049)\tData 0.031 (0.017)\tLoss 1.4358 (1.4791)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [260][150/391]\tTime 0.067 (0.049)\tData 0.035 (0.017)\tLoss 1.8559 (1.4927)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [260][200/391]\tTime 0.032 (0.050)\tData 0.000 (0.016)\tLoss 1.4191 (1.4952)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [260][250/391]\tTime 0.035 (0.049)\tData 0.003 (0.016)\tLoss 1.1816 (1.4987)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [260][300/391]\tTime 0.032 (0.049)\tData 0.001 (0.016)\tLoss 1.2031 (1.5034)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [260][350/391]\tTime 0.031 (0.049)\tData 0.000 (0.016)\tLoss 1.7895 (1.5090)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 1.0241 (1.0241)\tPrec@1 81.250 (81.250)\n",
            "Test: [50/79]\tTime 0.024 (0.016)\tLoss 0.9989 (1.0158)\tPrec@1 80.469 (81.173)\n",
            " * Prec@1 81.250\n",
            "\n",
            "Epoch: 262/600\n",
            "Learning rate: 0.060179\n",
            "Epoch: [261][0/391]\tTime 0.204 (0.204)\tData 0.157 (0.157)\tLoss 1.5739 (1.5739)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [261][50/391]\tTime 0.065 (0.051)\tData 0.032 (0.018)\tLoss 1.2151 (1.4499)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [261][100/391]\tTime 0.049 (0.049)\tData 0.018 (0.017)\tLoss 1.3914 (1.5115)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [261][150/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.3270 (1.5261)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [261][200/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.8053 (1.5226)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [261][250/391]\tTime 0.033 (0.049)\tData 0.000 (0.017)\tLoss 1.3909 (1.5281)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [261][300/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 1.4348 (1.5235)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [261][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.7457 (1.5266)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 0.9801 (0.9801)\tPrec@1 84.375 (84.375)\n",
            "Test: [50/79]\tTime 0.019 (0.016)\tLoss 1.0310 (0.9673)\tPrec@1 82.031 (85.049)\n",
            " * Prec@1 85.440\n",
            "\n",
            "Epoch: 263/600\n",
            "Learning rate: 0.059923\n",
            "Epoch: [262][0/391]\tTime 0.205 (0.205)\tData 0.160 (0.160)\tLoss 1.7507 (1.7507)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [262][50/391]\tTime 0.064 (0.055)\tData 0.032 (0.018)\tLoss 1.2197 (1.5539)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [262][100/391]\tTime 0.065 (0.051)\tData 0.033 (0.017)\tLoss 1.7034 (1.5272)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [262][150/391]\tTime 0.031 (0.051)\tData 0.000 (0.017)\tLoss 1.6110 (1.4936)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [262][200/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.3661 (1.4903)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [262][250/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.1944 (1.4989)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [262][300/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.5548 (1.4996)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [262][350/391]\tTime 0.031 (0.050)\tData 0.000 (0.017)\tLoss 1.5488 (1.4979)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.094 (0.094)\tLoss 0.9940 (0.9940)\tPrec@1 85.938 (85.938)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.9986 (1.0042)\tPrec@1 81.250 (82.828)\n",
            " * Prec@1 82.900\n",
            "\n",
            "Epoch: 264/600\n",
            "Learning rate: 0.059666\n",
            "Epoch: [263][0/391]\tTime 0.204 (0.204)\tData 0.159 (0.159)\tLoss 1.4368 (1.4368)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [263][50/391]\tTime 0.063 (0.051)\tData 0.031 (0.018)\tLoss 1.7645 (1.4644)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [263][100/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.6184 (1.4708)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [263][150/391]\tTime 0.063 (0.049)\tData 0.030 (0.016)\tLoss 1.4079 (1.4845)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [263][200/391]\tTime 0.063 (0.049)\tData 0.030 (0.016)\tLoss 1.2891 (1.4791)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [263][250/391]\tTime 0.065 (0.049)\tData 0.032 (0.016)\tLoss 1.4937 (1.4876)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [263][300/391]\tTime 0.066 (0.049)\tData 0.032 (0.016)\tLoss 1.6365 (1.4986)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [263][350/391]\tTime 0.064 (0.049)\tData 0.033 (0.016)\tLoss 1.6182 (1.5101)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 0.9646 (0.9646)\tPrec@1 83.594 (83.594)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.9290 (0.9405)\tPrec@1 85.938 (85.738)\n",
            " * Prec@1 85.700\n",
            "\n",
            "Epoch: 265/600\n",
            "Learning rate: 0.059410\n",
            "Epoch: [264][0/391]\tTime 0.202 (0.202)\tData 0.155 (0.155)\tLoss 1.0659 (1.0659)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [264][50/391]\tTime 0.046 (0.050)\tData 0.014 (0.018)\tLoss 1.6723 (1.5525)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [264][100/391]\tTime 0.033 (0.049)\tData 0.001 (0.017)\tLoss 1.2965 (1.5396)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [264][150/391]\tTime 0.041 (0.050)\tData 0.000 (0.017)\tLoss 1.7922 (1.5265)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [264][200/391]\tTime 0.032 (0.051)\tData 0.000 (0.017)\tLoss 1.5532 (1.5244)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [264][250/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.2508 (1.5138)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [264][300/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.3365 (1.5121)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [264][350/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.1591 (1.5086)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.098 (0.098)\tLoss 0.9278 (0.9278)\tPrec@1 85.938 (85.938)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.9032 (0.9351)\tPrec@1 84.375 (85.003)\n",
            " * Prec@1 85.210\n",
            "\n",
            "Epoch: 266/600\n",
            "Learning rate: 0.059153\n",
            "Epoch: [265][0/391]\tTime 0.205 (0.205)\tData 0.159 (0.159)\tLoss 1.6367 (1.6367)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [265][50/391]\tTime 0.066 (0.053)\tData 0.035 (0.020)\tLoss 1.1676 (1.4426)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [265][100/391]\tTime 0.066 (0.051)\tData 0.033 (0.019)\tLoss 1.3760 (1.4612)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [265][150/391]\tTime 0.071 (0.050)\tData 0.037 (0.018)\tLoss 1.5999 (1.4897)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [265][200/391]\tTime 0.064 (0.050)\tData 0.032 (0.018)\tLoss 1.2472 (1.5111)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [265][250/391]\tTime 0.064 (0.050)\tData 0.032 (0.017)\tLoss 1.1963 (1.5126)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [265][300/391]\tTime 0.062 (0.049)\tData 0.031 (0.017)\tLoss 1.5517 (1.5118)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [265][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.3826 (1.5027)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.096 (0.096)\tLoss 0.9997 (0.9997)\tPrec@1 81.250 (81.250)\n",
            "Test: [50/79]\tTime 0.017 (0.016)\tLoss 1.0512 (0.9850)\tPrec@1 79.688 (82.215)\n",
            " * Prec@1 82.280\n",
            "\n",
            "Epoch: 267/600\n",
            "Learning rate: 0.058895\n",
            "Epoch: [266][0/391]\tTime 0.198 (0.198)\tData 0.151 (0.151)\tLoss 1.7842 (1.7842)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [266][50/391]\tTime 0.066 (0.051)\tData 0.035 (0.019)\tLoss 1.6893 (1.5152)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [266][100/391]\tTime 0.065 (0.050)\tData 0.033 (0.018)\tLoss 1.2027 (1.5138)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [266][150/391]\tTime 0.066 (0.050)\tData 0.033 (0.018)\tLoss 1.7624 (1.5086)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [266][200/391]\tTime 0.036 (0.049)\tData 0.003 (0.017)\tLoss 1.0061 (1.5220)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [266][250/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.2068 (1.5131)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [266][300/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 1.8191 (1.5024)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [266][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.6834 (1.5007)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.096 (0.096)\tLoss 0.9936 (0.9936)\tPrec@1 82.031 (82.031)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 1.0896 (1.0345)\tPrec@1 77.344 (79.565)\n",
            " * Prec@1 79.260\n",
            "\n",
            "Epoch: 268/600\n",
            "Learning rate: 0.058638\n",
            "Epoch: [267][0/391]\tTime 0.200 (0.200)\tData 0.154 (0.154)\tLoss 1.1840 (1.1840)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [267][50/391]\tTime 0.067 (0.053)\tData 0.034 (0.018)\tLoss 1.6058 (1.5269)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [267][100/391]\tTime 0.067 (0.051)\tData 0.035 (0.018)\tLoss 1.5379 (1.5006)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [267][150/391]\tTime 0.067 (0.051)\tData 0.035 (0.017)\tLoss 1.4879 (1.4989)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [267][200/391]\tTime 0.066 (0.050)\tData 0.034 (0.017)\tLoss 1.6887 (1.5132)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [267][250/391]\tTime 0.067 (0.050)\tData 0.035 (0.017)\tLoss 1.6303 (1.5154)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [267][300/391]\tTime 0.065 (0.050)\tData 0.033 (0.017)\tLoss 1.7492 (1.5228)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [267][350/391]\tTime 0.063 (0.050)\tData 0.030 (0.017)\tLoss 1.6020 (1.5229)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.103 (0.103)\tLoss 1.0025 (1.0025)\tPrec@1 82.031 (82.031)\n",
            "Test: [50/79]\tTime 0.011 (0.016)\tLoss 1.0499 (0.9983)\tPrec@1 78.125 (81.388)\n",
            " * Prec@1 81.390\n",
            "\n",
            "Epoch: 269/600\n",
            "Learning rate: 0.058380\n",
            "Epoch: [268][0/391]\tTime 0.203 (0.203)\tData 0.157 (0.157)\tLoss 1.5500 (1.5500)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [268][50/391]\tTime 0.065 (0.051)\tData 0.031 (0.018)\tLoss 1.5312 (1.5521)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [268][100/391]\tTime 0.068 (0.050)\tData 0.034 (0.017)\tLoss 1.7040 (1.5372)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [268][150/391]\tTime 0.067 (0.050)\tData 0.035 (0.017)\tLoss 1.3675 (1.5352)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [268][200/391]\tTime 0.062 (0.050)\tData 0.030 (0.017)\tLoss 1.7092 (1.5458)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [268][250/391]\tTime 0.063 (0.049)\tData 0.032 (0.017)\tLoss 1.5887 (1.5267)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [268][300/391]\tTime 0.065 (0.049)\tData 0.034 (0.017)\tLoss 1.7883 (1.5124)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [268][350/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.7109 (1.5100)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.096 (0.096)\tLoss 0.8870 (0.8870)\tPrec@1 86.719 (86.719)\n",
            "Test: [50/79]\tTime 0.016 (0.016)\tLoss 0.9689 (0.9471)\tPrec@1 82.031 (83.532)\n",
            " * Prec@1 83.930\n",
            "\n",
            "Epoch: 270/600\n",
            "Learning rate: 0.058122\n",
            "Epoch: [269][0/391]\tTime 0.211 (0.211)\tData 0.155 (0.155)\tLoss 1.4357 (1.4357)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [269][50/391]\tTime 0.032 (0.052)\tData 0.000 (0.019)\tLoss 1.2391 (1.4771)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [269][100/391]\tTime 0.033 (0.051)\tData 0.000 (0.018)\tLoss 1.5962 (1.5118)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [269][150/391]\tTime 0.078 (0.051)\tData 0.036 (0.017)\tLoss 1.5148 (1.5107)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [269][200/391]\tTime 0.067 (0.051)\tData 0.035 (0.017)\tLoss 0.9813 (1.5053)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [269][250/391]\tTime 0.064 (0.051)\tData 0.031 (0.017)\tLoss 1.4382 (1.5032)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [269][300/391]\tTime 0.075 (0.051)\tData 0.033 (0.017)\tLoss 1.7794 (1.5015)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [269][350/391]\tTime 0.043 (0.051)\tData 0.000 (0.016)\tLoss 1.3689 (1.4948)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.093 (0.093)\tLoss 1.0158 (1.0158)\tPrec@1 82.031 (82.031)\n",
            "Test: [50/79]\tTime 0.010 (0.016)\tLoss 1.0242 (1.0336)\tPrec@1 79.688 (80.162)\n",
            " * Prec@1 80.230\n",
            "\n",
            "Epoch: 271/600\n",
            "Learning rate: 0.057864\n",
            "Epoch: [270][0/391]\tTime 0.208 (0.208)\tData 0.152 (0.152)\tLoss 1.5476 (1.5476)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [270][50/391]\tTime 0.031 (0.051)\tData 0.000 (0.019)\tLoss 1.6256 (1.5283)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [270][100/391]\tTime 0.031 (0.050)\tData 0.000 (0.018)\tLoss 1.8141 (1.4978)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [270][150/391]\tTime 0.034 (0.050)\tData 0.000 (0.017)\tLoss 1.0961 (1.5137)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [270][200/391]\tTime 0.073 (0.051)\tData 0.031 (0.017)\tLoss 1.7858 (1.5117)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [270][250/391]\tTime 0.063 (0.051)\tData 0.031 (0.017)\tLoss 1.6071 (1.5139)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [270][300/391]\tTime 0.065 (0.051)\tData 0.033 (0.017)\tLoss 1.4905 (1.5145)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [270][350/391]\tTime 0.067 (0.051)\tData 0.035 (0.017)\tLoss 1.4871 (1.5152)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.093 (0.093)\tLoss 0.9695 (0.9695)\tPrec@1 82.031 (82.031)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 1.0101 (0.9594)\tPrec@1 80.469 (82.613)\n",
            " * Prec@1 82.810\n",
            "\n",
            "Epoch: 272/600\n",
            "Learning rate: 0.057605\n",
            "Epoch: [271][0/391]\tTime 0.198 (0.198)\tData 0.153 (0.153)\tLoss 1.8023 (1.8023)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [271][50/391]\tTime 0.058 (0.050)\tData 0.026 (0.018)\tLoss 1.8088 (1.5224)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [271][100/391]\tTime 0.040 (0.049)\tData 0.009 (0.017)\tLoss 0.9689 (1.4898)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [271][150/391]\tTime 0.033 (0.049)\tData 0.000 (0.017)\tLoss 1.4802 (1.4830)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [271][200/391]\tTime 0.034 (0.049)\tData 0.000 (0.016)\tLoss 1.7517 (1.4857)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [271][250/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.1904 (1.4942)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [271][300/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.7337 (1.4982)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [271][350/391]\tTime 0.033 (0.049)\tData 0.000 (0.016)\tLoss 1.2100 (1.4970)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.094 (0.094)\tLoss 1.0635 (1.0635)\tPrec@1 82.031 (82.031)\n",
            "Test: [50/79]\tTime 0.011 (0.016)\tLoss 1.0435 (1.0762)\tPrec@1 78.125 (78.676)\n",
            " * Prec@1 78.920\n",
            "\n",
            "Epoch: 273/600\n",
            "Learning rate: 0.057347\n",
            "Epoch: [272][0/391]\tTime 0.214 (0.214)\tData 0.154 (0.154)\tLoss 1.4793 (1.4793)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [272][50/391]\tTime 0.042 (0.053)\tData 0.000 (0.018)\tLoss 1.1878 (1.4983)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [272][100/391]\tTime 0.032 (0.051)\tData 0.000 (0.017)\tLoss 1.3338 (1.4765)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [272][150/391]\tTime 0.033 (0.050)\tData 0.000 (0.017)\tLoss 1.6026 (1.5083)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [272][200/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.5220 (1.5134)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [272][250/391]\tTime 0.035 (0.050)\tData 0.000 (0.017)\tLoss 1.4130 (1.5125)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [272][300/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.7419 (1.5239)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [272][350/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.2259 (1.5270)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.104 (0.104)\tLoss 0.9475 (0.9475)\tPrec@1 85.156 (85.156)\n",
            "Test: [50/79]\tTime 0.010 (0.016)\tLoss 0.9521 (0.9428)\tPrec@1 81.250 (84.115)\n",
            " * Prec@1 84.540\n",
            "\n",
            "Epoch: 274/600\n",
            "Learning rate: 0.057088\n",
            "Epoch: [273][0/391]\tTime 0.199 (0.199)\tData 0.153 (0.153)\tLoss 1.3291 (1.3291)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [273][50/391]\tTime 0.066 (0.051)\tData 0.034 (0.019)\tLoss 1.6969 (1.5346)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [273][100/391]\tTime 0.065 (0.050)\tData 0.033 (0.017)\tLoss 1.8332 (1.5347)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [273][150/391]\tTime 0.042 (0.050)\tData 0.000 (0.017)\tLoss 1.5376 (1.5234)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [273][200/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.0478 (1.5167)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [273][250/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 0.9708 (1.5182)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [273][300/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.6636 (1.5136)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [273][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.7249 (1.5089)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 0.9394 (0.9394)\tPrec@1 85.938 (85.938)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.9358 (0.9529)\tPrec@1 84.375 (82.904)\n",
            " * Prec@1 83.360\n",
            "\n",
            "Epoch: 275/600\n",
            "Learning rate: 0.056829\n",
            "Epoch: [274][0/391]\tTime 0.196 (0.196)\tData 0.151 (0.151)\tLoss 1.6812 (1.6812)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [274][50/391]\tTime 0.062 (0.051)\tData 0.030 (0.018)\tLoss 1.3974 (1.4561)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [274][100/391]\tTime 0.063 (0.049)\tData 0.031 (0.017)\tLoss 1.7725 (1.4936)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [274][150/391]\tTime 0.046 (0.049)\tData 0.014 (0.017)\tLoss 1.6399 (1.5050)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [274][200/391]\tTime 0.063 (0.049)\tData 0.031 (0.017)\tLoss 1.7836 (1.4968)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [274][250/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.7564 (1.4954)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [274][300/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.7548 (1.4988)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [274][350/391]\tTime 0.062 (0.049)\tData 0.029 (0.016)\tLoss 1.2234 (1.5026)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.094 (0.094)\tLoss 0.9367 (0.9367)\tPrec@1 85.938 (85.938)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.9372 (0.9414)\tPrec@1 88.281 (85.095)\n",
            " * Prec@1 85.360\n",
            "\n",
            "Epoch: 276/600\n",
            "Learning rate: 0.056570\n",
            "Epoch: [275][0/391]\tTime 0.201 (0.201)\tData 0.155 (0.155)\tLoss 1.5409 (1.5409)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [275][50/391]\tTime 0.068 (0.052)\tData 0.036 (0.020)\tLoss 1.7389 (1.4817)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [275][100/391]\tTime 0.068 (0.051)\tData 0.036 (0.019)\tLoss 1.1472 (1.5146)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [275][150/391]\tTime 0.066 (0.050)\tData 0.034 (0.018)\tLoss 1.7922 (1.5020)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [275][200/391]\tTime 0.068 (0.050)\tData 0.036 (0.018)\tLoss 1.5157 (1.4901)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [275][250/391]\tTime 0.068 (0.050)\tData 0.034 (0.018)\tLoss 1.5392 (1.4832)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [275][300/391]\tTime 0.063 (0.050)\tData 0.031 (0.018)\tLoss 1.2038 (1.4793)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [275][350/391]\tTime 0.069 (0.050)\tData 0.036 (0.017)\tLoss 1.4005 (1.4847)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 1.0110 (1.0110)\tPrec@1 83.594 (83.594)\n",
            "Test: [50/79]\tTime 0.019 (0.016)\tLoss 0.9900 (0.9898)\tPrec@1 82.812 (82.966)\n",
            " * Prec@1 83.370\n",
            "\n",
            "Epoch: 277/600\n",
            "Learning rate: 0.056310\n",
            "Epoch: [276][0/391]\tTime 0.198 (0.198)\tData 0.153 (0.153)\tLoss 1.3803 (1.3803)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [276][50/391]\tTime 0.063 (0.052)\tData 0.032 (0.019)\tLoss 1.6773 (1.5060)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [276][100/391]\tTime 0.062 (0.050)\tData 0.031 (0.018)\tLoss 1.2439 (1.4795)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [276][150/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.0837 (1.4767)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [276][200/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.6102 (1.4894)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [276][250/391]\tTime 0.065 (0.049)\tData 0.032 (0.017)\tLoss 1.2887 (1.4960)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [276][300/391]\tTime 0.065 (0.049)\tData 0.032 (0.017)\tLoss 1.8097 (1.5017)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [276][350/391]\tTime 0.063 (0.049)\tData 0.031 (0.017)\tLoss 1.1629 (1.4990)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 1.0323 (1.0323)\tPrec@1 81.250 (81.250)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 1.0546 (1.0142)\tPrec@1 78.125 (81.771)\n",
            " * Prec@1 82.090\n",
            "\n",
            "Epoch: 278/600\n",
            "Learning rate: 0.056051\n",
            "Epoch: [277][0/391]\tTime 0.208 (0.208)\tData 0.153 (0.153)\tLoss 1.7588 (1.7588)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [277][50/391]\tTime 0.066 (0.052)\tData 0.033 (0.019)\tLoss 1.1656 (1.4827)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [277][100/391]\tTime 0.066 (0.051)\tData 0.035 (0.018)\tLoss 1.7773 (1.5155)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [277][150/391]\tTime 0.066 (0.050)\tData 0.034 (0.018)\tLoss 1.4722 (1.5013)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [277][200/391]\tTime 0.067 (0.050)\tData 0.035 (0.018)\tLoss 1.6786 (1.5119)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [277][250/391]\tTime 0.075 (0.050)\tData 0.032 (0.018)\tLoss 1.1746 (1.5067)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [277][300/391]\tTime 0.066 (0.050)\tData 0.033 (0.017)\tLoss 1.3170 (1.5091)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [277][350/391]\tTime 0.064 (0.050)\tData 0.033 (0.017)\tLoss 1.8017 (1.5040)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 1.0082 (1.0082)\tPrec@1 80.469 (80.469)\n",
            "Test: [50/79]\tTime 0.010 (0.016)\tLoss 0.9624 (0.9439)\tPrec@1 83.594 (83.257)\n",
            " * Prec@1 83.350\n",
            "\n",
            "Epoch: 279/600\n",
            "Learning rate: 0.055791\n",
            "Epoch: [278][0/391]\tTime 0.199 (0.199)\tData 0.153 (0.153)\tLoss 1.7262 (1.7262)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [278][50/391]\tTime 0.066 (0.051)\tData 0.033 (0.019)\tLoss 1.6621 (1.5710)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [278][100/391]\tTime 0.068 (0.051)\tData 0.036 (0.018)\tLoss 1.6703 (1.5408)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [278][150/391]\tTime 0.042 (0.052)\tData 0.000 (0.017)\tLoss 1.7790 (1.5477)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [278][200/391]\tTime 0.042 (0.054)\tData 0.000 (0.016)\tLoss 1.7815 (1.5285)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [278][250/391]\tTime 0.042 (0.055)\tData 0.000 (0.016)\tLoss 1.5254 (1.5153)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [278][300/391]\tTime 0.067 (0.054)\tData 0.034 (0.016)\tLoss 1.6458 (1.5146)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [278][350/391]\tTime 0.065 (0.053)\tData 0.032 (0.016)\tLoss 1.7020 (1.5155)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.096 (0.096)\tLoss 1.0387 (1.0387)\tPrec@1 80.469 (80.469)\n",
            "Test: [50/79]\tTime 0.019 (0.016)\tLoss 1.0392 (1.0160)\tPrec@1 78.906 (80.974)\n",
            " * Prec@1 81.260\n",
            "\n",
            "Epoch: 280/600\n",
            "Learning rate: 0.055531\n",
            "Epoch: [279][0/391]\tTime 0.202 (0.202)\tData 0.155 (0.155)\tLoss 1.1450 (1.1450)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [279][50/391]\tTime 0.064 (0.051)\tData 0.031 (0.018)\tLoss 1.5049 (1.4493)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [279][100/391]\tTime 0.035 (0.050)\tData 0.000 (0.017)\tLoss 1.4920 (1.4815)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [279][150/391]\tTime 0.076 (0.051)\tData 0.033 (0.017)\tLoss 1.4956 (1.5056)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [279][200/391]\tTime 0.062 (0.053)\tData 0.029 (0.017)\tLoss 1.6704 (1.5096)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [279][250/391]\tTime 0.065 (0.052)\tData 0.032 (0.016)\tLoss 1.0330 (1.5141)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [279][300/391]\tTime 0.063 (0.051)\tData 0.032 (0.016)\tLoss 1.4826 (1.5150)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [279][350/391]\tTime 0.065 (0.051)\tData 0.033 (0.016)\tLoss 1.5051 (1.5068)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.098 (0.098)\tLoss 1.0664 (1.0664)\tPrec@1 78.906 (78.906)\n",
            "Test: [50/79]\tTime 0.019 (0.016)\tLoss 1.1317 (1.0414)\tPrec@1 77.344 (79.182)\n",
            " * Prec@1 79.070\n",
            "\n",
            "Epoch: 281/600\n",
            "Learning rate: 0.055271\n",
            "Epoch: [280][0/391]\tTime 0.199 (0.199)\tData 0.153 (0.153)\tLoss 1.2789 (1.2789)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [280][50/391]\tTime 0.064 (0.051)\tData 0.032 (0.019)\tLoss 1.6071 (1.5405)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [280][100/391]\tTime 0.065 (0.050)\tData 0.031 (0.017)\tLoss 1.5819 (1.5168)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [280][150/391]\tTime 0.065 (0.049)\tData 0.031 (0.017)\tLoss 1.1614 (1.5347)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [280][200/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.1269 (1.5213)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [280][250/391]\tTime 0.033 (0.049)\tData 0.000 (0.017)\tLoss 1.0996 (1.5115)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [280][300/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.2867 (1.5049)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [280][350/391]\tTime 0.034 (0.049)\tData 0.000 (0.017)\tLoss 1.6922 (1.5044)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.093 (0.093)\tLoss 1.0302 (1.0302)\tPrec@1 81.250 (81.250)\n",
            "Test: [50/79]\tTime 0.020 (0.016)\tLoss 1.0223 (0.9925)\tPrec@1 83.594 (83.165)\n",
            " * Prec@1 83.300\n",
            "\n",
            "Epoch: 282/600\n",
            "Learning rate: 0.055011\n",
            "Epoch: [281][0/391]\tTime 0.201 (0.201)\tData 0.156 (0.156)\tLoss 1.5667 (1.5667)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [281][50/391]\tTime 0.063 (0.051)\tData 0.031 (0.018)\tLoss 1.3100 (1.5086)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [281][100/391]\tTime 0.065 (0.049)\tData 0.034 (0.018)\tLoss 1.2908 (1.5266)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [281][150/391]\tTime 0.066 (0.049)\tData 0.034 (0.017)\tLoss 1.6969 (1.5152)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [281][200/391]\tTime 0.067 (0.049)\tData 0.035 (0.017)\tLoss 1.6986 (1.5110)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [281][250/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.6295 (1.5095)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [281][300/391]\tTime 0.064 (0.049)\tData 0.033 (0.017)\tLoss 1.7009 (1.5213)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [281][350/391]\tTime 0.064 (0.049)\tData 0.031 (0.017)\tLoss 1.3764 (1.5155)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 1.0237 (1.0237)\tPrec@1 79.688 (79.688)\n",
            "Test: [50/79]\tTime 0.011 (0.017)\tLoss 1.0184 (0.9625)\tPrec@1 83.594 (83.502)\n",
            " * Prec@1 83.530\n",
            "\n",
            "Epoch: 283/600\n",
            "Learning rate: 0.054751\n",
            "Epoch: [282][0/391]\tTime 0.201 (0.201)\tData 0.154 (0.154)\tLoss 0.9902 (0.9902)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [282][50/391]\tTime 0.031 (0.051)\tData 0.000 (0.019)\tLoss 1.3531 (1.5278)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [282][100/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.4613 (1.5159)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [282][150/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.1230 (1.5006)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [282][200/391]\tTime 0.033 (0.050)\tData 0.000 (0.018)\tLoss 1.6580 (1.5055)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [282][250/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.7930 (1.4944)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [282][300/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.6616 (1.4988)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [282][350/391]\tTime 0.034 (0.049)\tData 0.000 (0.017)\tLoss 1.8707 (1.4966)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.093 (0.093)\tLoss 0.9663 (0.9663)\tPrec@1 84.375 (84.375)\n",
            "Test: [50/79]\tTime 0.010 (0.016)\tLoss 0.9144 (0.9061)\tPrec@1 86.719 (86.305)\n",
            " * Prec@1 86.220\n",
            "\n",
            "Epoch: 284/600\n",
            "Learning rate: 0.054490\n",
            "Epoch: [283][0/391]\tTime 0.203 (0.203)\tData 0.155 (0.155)\tLoss 1.5119 (1.5119)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [283][50/391]\tTime 0.045 (0.053)\tData 0.014 (0.018)\tLoss 1.4407 (1.5511)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [283][100/391]\tTime 0.035 (0.051)\tData 0.003 (0.017)\tLoss 1.4114 (1.5495)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [283][150/391]\tTime 0.033 (0.050)\tData 0.000 (0.017)\tLoss 1.7689 (1.5400)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [283][200/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.4419 (1.5288)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [283][250/391]\tTime 0.033 (0.049)\tData 0.000 (0.017)\tLoss 1.5771 (1.5339)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [283][300/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.0222 (1.5181)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [283][350/391]\tTime 0.069 (0.049)\tData 0.037 (0.017)\tLoss 1.4715 (1.5136)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.094 (0.094)\tLoss 1.0018 (1.0018)\tPrec@1 82.812 (82.812)\n",
            "Test: [50/79]\tTime 0.016 (0.016)\tLoss 0.9617 (0.9862)\tPrec@1 84.375 (83.441)\n",
            " * Prec@1 83.530\n",
            "\n",
            "Epoch: 285/600\n",
            "Learning rate: 0.054230\n",
            "Epoch: [284][0/391]\tTime 0.199 (0.199)\tData 0.152 (0.152)\tLoss 1.1859 (1.1859)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [284][50/391]\tTime 0.072 (0.052)\tData 0.037 (0.019)\tLoss 1.0624 (1.4899)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [284][100/391]\tTime 0.062 (0.051)\tData 0.030 (0.017)\tLoss 1.0306 (1.5122)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [284][150/391]\tTime 0.063 (0.050)\tData 0.032 (0.017)\tLoss 1.4001 (1.5083)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [284][200/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.4186 (1.5274)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [284][250/391]\tTime 0.065 (0.049)\tData 0.034 (0.017)\tLoss 1.6298 (1.5271)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [284][300/391]\tTime 0.063 (0.049)\tData 0.031 (0.016)\tLoss 1.3967 (1.5212)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [284][350/391]\tTime 0.063 (0.049)\tData 0.031 (0.016)\tLoss 1.6249 (1.5216)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 0.9135 (0.9135)\tPrec@1 86.719 (86.719)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.8929 (0.9218)\tPrec@1 87.500 (85.570)\n",
            " * Prec@1 85.830\n",
            "\n",
            "Epoch: 286/600\n",
            "Learning rate: 0.053969\n",
            "Epoch: [285][0/391]\tTime 0.199 (0.199)\tData 0.154 (0.154)\tLoss 1.5956 (1.5956)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [285][50/391]\tTime 0.064 (0.050)\tData 0.032 (0.018)\tLoss 1.7353 (1.4415)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [285][100/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.5565 (1.4786)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [285][150/391]\tTime 0.066 (0.051)\tData 0.034 (0.017)\tLoss 1.1933 (1.4919)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [285][200/391]\tTime 0.066 (0.050)\tData 0.034 (0.017)\tLoss 1.2679 (1.4912)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [285][250/391]\tTime 0.065 (0.050)\tData 0.033 (0.017)\tLoss 1.7828 (1.4930)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [285][300/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.3441 (1.5012)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [285][350/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.1226 (1.4953)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.098 (0.098)\tLoss 0.9315 (0.9315)\tPrec@1 87.500 (87.500)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.9899 (0.9625)\tPrec@1 79.688 (83.824)\n",
            " * Prec@1 83.800\n",
            "\n",
            "Epoch: 287/600\n",
            "Learning rate: 0.053708\n",
            "Epoch: [286][0/391]\tTime 0.216 (0.216)\tData 0.158 (0.158)\tLoss 1.6850 (1.6850)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [286][50/391]\tTime 0.042 (0.061)\tData 0.000 (0.018)\tLoss 1.6460 (1.4883)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [286][100/391]\tTime 0.032 (0.055)\tData 0.000 (0.017)\tLoss 1.7755 (1.4947)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [286][150/391]\tTime 0.031 (0.053)\tData 0.000 (0.017)\tLoss 1.5784 (1.4864)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [286][200/391]\tTime 0.068 (0.052)\tData 0.036 (0.017)\tLoss 1.0079 (1.4982)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [286][250/391]\tTime 0.068 (0.052)\tData 0.036 (0.017)\tLoss 1.6587 (1.4898)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [286][300/391]\tTime 0.067 (0.051)\tData 0.035 (0.017)\tLoss 1.6977 (1.4929)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [286][350/391]\tTime 0.067 (0.051)\tData 0.034 (0.017)\tLoss 1.0447 (1.4871)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 1.0079 (1.0079)\tPrec@1 79.688 (79.688)\n",
            "Test: [50/79]\tTime 0.010 (0.018)\tLoss 0.9676 (0.9768)\tPrec@1 83.594 (82.736)\n",
            " * Prec@1 83.120\n",
            "\n",
            "Epoch: 288/600\n",
            "Learning rate: 0.053447\n",
            "Epoch: [287][0/391]\tTime 0.198 (0.198)\tData 0.153 (0.153)\tLoss 1.1155 (1.1155)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [287][50/391]\tTime 0.064 (0.051)\tData 0.031 (0.018)\tLoss 1.5689 (1.4665)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [287][100/391]\tTime 0.067 (0.050)\tData 0.035 (0.018)\tLoss 1.2700 (1.4981)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [287][150/391]\tTime 0.067 (0.050)\tData 0.035 (0.018)\tLoss 1.6697 (1.4948)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [287][200/391]\tTime 0.064 (0.050)\tData 0.031 (0.017)\tLoss 1.7852 (1.4910)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [287][250/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.4067 (1.4877)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [287][300/391]\tTime 0.063 (0.049)\tData 0.031 (0.017)\tLoss 0.9697 (1.4874)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [287][350/391]\tTime 0.063 (0.049)\tData 0.031 (0.017)\tLoss 1.5513 (1.4853)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.096 (0.096)\tLoss 0.9368 (0.9368)\tPrec@1 88.281 (88.281)\n",
            "Test: [50/79]\tTime 0.019 (0.016)\tLoss 0.9627 (0.9622)\tPrec@1 84.375 (85.248)\n",
            " * Prec@1 85.150\n",
            "\n",
            "Epoch: 289/600\n",
            "Learning rate: 0.053186\n",
            "Epoch: [288][0/391]\tTime 0.212 (0.212)\tData 0.155 (0.155)\tLoss 1.0402 (1.0402)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [288][50/391]\tTime 0.031 (0.052)\tData 0.000 (0.017)\tLoss 0.9051 (1.4210)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [288][100/391]\tTime 0.031 (0.050)\tData 0.000 (0.017)\tLoss 1.8075 (1.4554)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [288][150/391]\tTime 0.031 (0.050)\tData 0.000 (0.017)\tLoss 1.7937 (1.4635)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [288][200/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 1.7811 (1.4725)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [288][250/391]\tTime 0.033 (0.049)\tData 0.000 (0.017)\tLoss 1.7704 (1.4828)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [288][300/391]\tTime 0.033 (0.049)\tData 0.000 (0.017)\tLoss 1.6541 (1.4889)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [288][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.4316 (1.4840)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.098 (0.098)\tLoss 0.8833 (0.8833)\tPrec@1 89.844 (89.844)\n",
            "Test: [50/79]\tTime 0.017 (0.016)\tLoss 0.8866 (0.9027)\tPrec@1 85.156 (85.846)\n",
            " * Prec@1 86.010\n",
            "\n",
            "Epoch: 290/600\n",
            "Learning rate: 0.052925\n",
            "Epoch: [289][0/391]\tTime 0.199 (0.199)\tData 0.153 (0.153)\tLoss 1.4502 (1.4502)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [289][50/391]\tTime 0.036 (0.051)\tData 0.000 (0.019)\tLoss 1.7632 (1.4894)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [289][100/391]\tTime 0.032 (0.051)\tData 0.000 (0.018)\tLoss 1.6595 (1.4825)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [289][150/391]\tTime 0.033 (0.050)\tData 0.000 (0.018)\tLoss 1.5863 (1.5064)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [289][200/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.5111 (1.4938)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [289][250/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.0578 (1.5023)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [289][300/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.2832 (1.5045)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [289][350/391]\tTime 0.031 (0.050)\tData 0.000 (0.018)\tLoss 1.4402 (1.5027)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 0.9480 (0.9480)\tPrec@1 84.375 (84.375)\n",
            "Test: [50/79]\tTime 0.018 (0.017)\tLoss 0.9490 (0.9550)\tPrec@1 82.812 (83.732)\n",
            " * Prec@1 83.900\n",
            "\n",
            "Epoch: 291/600\n",
            "Learning rate: 0.052664\n",
            "Epoch: [290][0/391]\tTime 0.200 (0.200)\tData 0.153 (0.153)\tLoss 1.7047 (1.7047)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [290][50/391]\tTime 0.065 (0.051)\tData 0.033 (0.018)\tLoss 0.9563 (1.4636)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [290][100/391]\tTime 0.065 (0.050)\tData 0.032 (0.017)\tLoss 1.7632 (1.4748)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [290][150/391]\tTime 0.065 (0.049)\tData 0.034 (0.017)\tLoss 1.3314 (1.4858)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [290][200/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.7645 (1.4976)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [290][250/391]\tTime 0.063 (0.049)\tData 0.032 (0.016)\tLoss 1.1307 (1.4945)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [290][300/391]\tTime 0.065 (0.049)\tData 0.031 (0.016)\tLoss 1.1392 (1.4935)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [290][350/391]\tTime 0.064 (0.049)\tData 0.031 (0.016)\tLoss 1.7543 (1.4916)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.094 (0.094)\tLoss 0.9274 (0.9274)\tPrec@1 89.844 (89.844)\n",
            "Test: [50/79]\tTime 0.020 (0.016)\tLoss 0.9330 (0.9613)\tPrec@1 85.938 (85.371)\n",
            " * Prec@1 85.230\n",
            "\n",
            "Epoch: 292/600\n",
            "Learning rate: 0.052403\n",
            "Epoch: [291][0/391]\tTime 0.201 (0.201)\tData 0.154 (0.154)\tLoss 1.0783 (1.0783)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [291][50/391]\tTime 0.032 (0.051)\tData 0.000 (0.019)\tLoss 1.2273 (1.4489)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [291][100/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.4713 (1.4526)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [291][150/391]\tTime 0.033 (0.050)\tData 0.000 (0.018)\tLoss 1.6525 (1.4635)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [291][200/391]\tTime 0.037 (0.050)\tData 0.006 (0.017)\tLoss 1.8808 (1.4693)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [291][250/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.1628 (1.4711)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [291][300/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.6610 (1.4688)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [291][350/391]\tTime 0.033 (0.050)\tData 0.000 (0.017)\tLoss 1.0885 (1.4643)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.098 (0.098)\tLoss 0.9287 (0.9287)\tPrec@1 82.812 (82.812)\n",
            "Test: [50/79]\tTime 0.010 (0.016)\tLoss 1.0012 (0.9622)\tPrec@1 80.469 (83.349)\n",
            " * Prec@1 83.290\n",
            "\n",
            "Epoch: 293/600\n",
            "Learning rate: 0.052142\n",
            "Epoch: [292][0/391]\tTime 0.196 (0.196)\tData 0.151 (0.151)\tLoss 1.6612 (1.6612)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [292][50/391]\tTime 0.063 (0.050)\tData 0.031 (0.018)\tLoss 1.7177 (1.5072)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [292][100/391]\tTime 0.054 (0.049)\tData 0.021 (0.017)\tLoss 1.7144 (1.4931)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [292][150/391]\tTime 0.040 (0.049)\tData 0.009 (0.017)\tLoss 1.7141 (1.5237)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [292][200/391]\tTime 0.044 (0.049)\tData 0.011 (0.017)\tLoss 1.7218 (1.5065)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [292][250/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.7462 (1.4974)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [292][300/391]\tTime 0.063 (0.049)\tData 0.031 (0.017)\tLoss 1.3132 (1.5016)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [292][350/391]\tTime 0.067 (0.049)\tData 0.035 (0.017)\tLoss 1.6459 (1.4977)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 0.9399 (0.9399)\tPrec@1 84.375 (84.375)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.9154 (0.9411)\tPrec@1 91.406 (85.938)\n",
            " * Prec@1 85.780\n",
            "\n",
            "Epoch: 294/600\n",
            "Learning rate: 0.051880\n",
            "Epoch: [293][0/391]\tTime 0.213 (0.213)\tData 0.157 (0.157)\tLoss 1.6088 (1.6088)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [293][50/391]\tTime 0.064 (0.052)\tData 0.032 (0.019)\tLoss 1.5230 (1.4967)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [293][100/391]\tTime 0.065 (0.050)\tData 0.033 (0.017)\tLoss 1.4977 (1.4902)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [293][150/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.7404 (1.4910)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [293][200/391]\tTime 0.045 (0.049)\tData 0.013 (0.017)\tLoss 1.6495 (1.4946)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [293][250/391]\tTime 0.069 (0.049)\tData 0.031 (0.016)\tLoss 0.9224 (1.4897)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [293][300/391]\tTime 0.062 (0.049)\tData 0.030 (0.016)\tLoss 1.1562 (1.4817)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [293][350/391]\tTime 0.034 (0.049)\tData 0.002 (0.016)\tLoss 1.8019 (1.4777)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.094 (0.094)\tLoss 0.9259 (0.9259)\tPrec@1 86.719 (86.719)\n",
            "Test: [50/79]\tTime 0.018 (0.017)\tLoss 0.9886 (0.9421)\tPrec@1 78.125 (84.819)\n",
            " * Prec@1 85.440\n",
            "\n",
            "Epoch: 295/600\n",
            "Learning rate: 0.051619\n",
            "Epoch: [294][0/391]\tTime 0.203 (0.203)\tData 0.157 (0.157)\tLoss 1.7104 (1.7104)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [294][50/391]\tTime 0.070 (0.053)\tData 0.037 (0.020)\tLoss 1.5478 (1.4989)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [294][100/391]\tTime 0.065 (0.051)\tData 0.033 (0.018)\tLoss 1.7383 (1.4922)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [294][150/391]\tTime 0.065 (0.050)\tData 0.033 (0.018)\tLoss 0.9618 (1.4867)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [294][200/391]\tTime 0.068 (0.050)\tData 0.036 (0.018)\tLoss 1.7864 (1.4940)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [294][250/391]\tTime 0.070 (0.050)\tData 0.037 (0.018)\tLoss 1.5297 (1.5035)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [294][300/391]\tTime 0.060 (0.050)\tData 0.027 (0.017)\tLoss 1.0364 (1.5123)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [294][350/391]\tTime 0.066 (0.050)\tData 0.034 (0.017)\tLoss 1.4596 (1.5082)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.093 (0.093)\tLoss 1.1524 (1.1524)\tPrec@1 77.344 (77.344)\n",
            "Test: [50/79]\tTime 0.010 (0.016)\tLoss 1.1466 (1.1179)\tPrec@1 72.656 (76.608)\n",
            " * Prec@1 76.580\n",
            "\n",
            "Epoch: 296/600\n",
            "Learning rate: 0.051358\n",
            "Epoch: [295][0/391]\tTime 0.199 (0.199)\tData 0.153 (0.153)\tLoss 1.8148 (1.8148)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [295][50/391]\tTime 0.061 (0.051)\tData 0.029 (0.018)\tLoss 1.5886 (1.5627)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [295][100/391]\tTime 0.066 (0.050)\tData 0.033 (0.017)\tLoss 1.4987 (1.5481)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [295][150/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.6301 (1.5298)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [295][200/391]\tTime 0.034 (0.049)\tData 0.001 (0.017)\tLoss 1.7997 (1.5290)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [295][250/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.0980 (1.5272)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [295][300/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.8056 (1.5152)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [295][350/391]\tTime 0.033 (0.049)\tData 0.000 (0.017)\tLoss 1.8636 (1.5118)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.096 (0.096)\tLoss 0.9823 (0.9823)\tPrec@1 78.906 (78.906)\n",
            "Test: [50/79]\tTime 0.020 (0.016)\tLoss 1.0025 (0.9883)\tPrec@1 78.906 (82.307)\n",
            " * Prec@1 82.210\n",
            "\n",
            "Epoch: 297/600\n",
            "Learning rate: 0.051096\n",
            "Epoch: [296][0/391]\tTime 0.213 (0.213)\tData 0.155 (0.155)\tLoss 1.7437 (1.7437)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [296][50/391]\tTime 0.032 (0.053)\tData 0.000 (0.018)\tLoss 1.5931 (1.5296)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [296][100/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.3776 (1.5083)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [296][150/391]\tTime 0.031 (0.050)\tData 0.000 (0.017)\tLoss 1.7463 (1.4960)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [296][200/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.5186 (1.4792)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [296][250/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 1.7637 (1.4894)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [296][300/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.7515 (1.4947)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [296][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.7982 (1.4989)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.9710 (0.9710)\tPrec@1 88.281 (88.281)\n",
            "Test: [50/79]\tTime 0.010 (0.017)\tLoss 0.9744 (0.9347)\tPrec@1 81.250 (84.574)\n",
            " * Prec@1 84.800\n",
            "\n",
            "Epoch: 298/600\n",
            "Learning rate: 0.050835\n",
            "Epoch: [297][0/391]\tTime 0.211 (0.211)\tData 0.154 (0.154)\tLoss 1.0241 (1.0241)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [297][50/391]\tTime 0.065 (0.052)\tData 0.033 (0.018)\tLoss 1.7444 (1.4345)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [297][100/391]\tTime 0.063 (0.050)\tData 0.031 (0.017)\tLoss 1.8589 (1.4441)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [297][150/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.0619 (1.4608)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [297][200/391]\tTime 0.065 (0.049)\tData 0.032 (0.016)\tLoss 1.7471 (1.4833)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [297][250/391]\tTime 0.065 (0.049)\tData 0.033 (0.016)\tLoss 1.3122 (1.4830)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [297][300/391]\tTime 0.065 (0.049)\tData 0.032 (0.016)\tLoss 1.7531 (1.4731)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [297][350/391]\tTime 0.031 (0.049)\tData 0.000 (0.016)\tLoss 1.6961 (1.4806)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.096 (0.096)\tLoss 0.9653 (0.9653)\tPrec@1 82.812 (82.812)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.9130 (0.9591)\tPrec@1 85.938 (83.349)\n",
            " * Prec@1 83.320\n",
            "\n",
            "Epoch: 299/600\n",
            "Learning rate: 0.050573\n",
            "Epoch: [298][0/391]\tTime 0.202 (0.202)\tData 0.156 (0.156)\tLoss 1.4391 (1.4391)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [298][50/391]\tTime 0.068 (0.053)\tData 0.036 (0.020)\tLoss 1.8273 (1.4452)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [298][100/391]\tTime 0.069 (0.051)\tData 0.037 (0.019)\tLoss 1.5415 (1.4818)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [298][150/391]\tTime 0.068 (0.051)\tData 0.037 (0.019)\tLoss 1.7866 (1.4909)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [298][200/391]\tTime 0.067 (0.050)\tData 0.035 (0.018)\tLoss 1.8047 (1.4745)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [298][250/391]\tTime 0.063 (0.050)\tData 0.029 (0.018)\tLoss 1.2948 (1.4831)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [298][300/391]\tTime 0.068 (0.050)\tData 0.036 (0.018)\tLoss 1.2106 (1.4752)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [298][350/391]\tTime 0.069 (0.050)\tData 0.036 (0.018)\tLoss 1.0417 (1.4757)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 1.0158 (1.0158)\tPrec@1 78.906 (78.906)\n",
            "Test: [50/79]\tTime 0.019 (0.016)\tLoss 0.9703 (0.9432)\tPrec@1 81.250 (83.931)\n",
            " * Prec@1 83.900\n",
            "\n",
            "Epoch: 300/600\n",
            "Learning rate: 0.050312\n",
            "Epoch: [299][0/391]\tTime 0.202 (0.202)\tData 0.154 (0.154)\tLoss 1.2988 (1.2988)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [299][50/391]\tTime 0.031 (0.054)\tData 0.000 (0.017)\tLoss 1.7957 (1.4643)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [299][100/391]\tTime 0.032 (0.052)\tData 0.000 (0.018)\tLoss 1.7488 (1.4897)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [299][150/391]\tTime 0.032 (0.051)\tData 0.000 (0.017)\tLoss 1.7640 (1.4937)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [299][200/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.6971 (1.4854)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [299][250/391]\tTime 0.033 (0.050)\tData 0.000 (0.017)\tLoss 1.7661 (1.4772)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [299][300/391]\tTime 0.034 (0.050)\tData 0.000 (0.017)\tLoss 1.7350 (1.4652)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [299][350/391]\tTime 0.062 (0.049)\tData 0.030 (0.017)\tLoss 1.4653 (1.4724)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.096 (0.096)\tLoss 0.9296 (0.9296)\tPrec@1 88.281 (88.281)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.9662 (1.0009)\tPrec@1 85.938 (81.985)\n",
            " * Prec@1 82.240\n",
            "\n",
            "Epoch: 301/600\n",
            "Learning rate: 0.050050\n",
            "Epoch: [300][0/391]\tTime 0.211 (0.211)\tData 0.154 (0.154)\tLoss 1.4359 (1.4359)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [300][50/391]\tTime 0.070 (0.053)\tData 0.036 (0.018)\tLoss 1.2650 (1.5889)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [300][100/391]\tTime 0.069 (0.051)\tData 0.037 (0.018)\tLoss 1.7518 (1.5169)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [300][150/391]\tTime 0.064 (0.051)\tData 0.032 (0.017)\tLoss 1.6260 (1.5062)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [300][200/391]\tTime 0.066 (0.050)\tData 0.033 (0.017)\tLoss 1.6061 (1.5139)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [300][250/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.2518 (1.5015)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [300][300/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.2127 (1.5063)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [300][350/391]\tTime 0.076 (0.050)\tData 0.031 (0.017)\tLoss 1.3226 (1.5182)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.096 (0.096)\tLoss 0.9195 (0.9195)\tPrec@1 84.375 (84.375)\n",
            "Test: [50/79]\tTime 0.019 (0.016)\tLoss 0.8959 (0.9432)\tPrec@1 85.156 (83.762)\n",
            " * Prec@1 83.890\n",
            "\n",
            "Epoch: 302/600\n",
            "Learning rate: 0.049788\n",
            "Epoch: [301][0/391]\tTime 0.203 (0.203)\tData 0.156 (0.156)\tLoss 1.3933 (1.3933)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [301][50/391]\tTime 0.065 (0.051)\tData 0.032 (0.018)\tLoss 1.0810 (1.4721)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [301][100/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.6228 (1.4722)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [301][150/391]\tTime 0.061 (0.049)\tData 0.027 (0.017)\tLoss 1.6035 (1.4677)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [301][200/391]\tTime 0.065 (0.049)\tData 0.033 (0.016)\tLoss 1.6311 (1.4897)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [301][250/391]\tTime 0.065 (0.049)\tData 0.032 (0.016)\tLoss 1.5834 (1.4779)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [301][300/391]\tTime 0.063 (0.048)\tData 0.030 (0.016)\tLoss 1.8062 (1.4802)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [301][350/391]\tTime 0.063 (0.048)\tData 0.032 (0.016)\tLoss 1.8362 (1.4838)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 0.9721 (0.9721)\tPrec@1 84.375 (84.375)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.9676 (0.9598)\tPrec@1 84.375 (84.697)\n",
            " * Prec@1 84.780\n",
            "\n",
            "Epoch: 303/600\n",
            "Learning rate: 0.049527\n",
            "Epoch: [302][0/391]\tTime 0.199 (0.199)\tData 0.155 (0.155)\tLoss 1.7129 (1.7129)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [302][50/391]\tTime 0.066 (0.051)\tData 0.034 (0.019)\tLoss 1.8399 (1.5193)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [302][100/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.3754 (1.5230)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [302][150/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.3447 (1.5166)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [302][200/391]\tTime 0.064 (0.050)\tData 0.030 (0.017)\tLoss 1.2617 (1.5072)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [302][250/391]\tTime 0.066 (0.050)\tData 0.034 (0.017)\tLoss 1.2211 (1.4996)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [302][300/391]\tTime 0.067 (0.050)\tData 0.036 (0.017)\tLoss 1.6607 (1.5054)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [302][350/391]\tTime 0.067 (0.050)\tData 0.034 (0.017)\tLoss 1.0423 (1.5086)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 0.9602 (0.9602)\tPrec@1 84.375 (84.375)\n",
            "Test: [50/79]\tTime 0.014 (0.016)\tLoss 0.9722 (0.9467)\tPrec@1 82.812 (85.309)\n",
            " * Prec@1 85.470\n",
            "\n",
            "Epoch: 304/600\n",
            "Learning rate: 0.049265\n",
            "Epoch: [303][0/391]\tTime 0.203 (0.203)\tData 0.156 (0.156)\tLoss 1.7449 (1.7449)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [303][50/391]\tTime 0.066 (0.052)\tData 0.032 (0.019)\tLoss 1.7637 (1.5190)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [303][100/391]\tTime 0.065 (0.050)\tData 0.033 (0.017)\tLoss 1.6932 (1.5114)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [303][150/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.5585 (1.4969)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [303][200/391]\tTime 0.066 (0.049)\tData 0.034 (0.017)\tLoss 0.9955 (1.4914)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [303][250/391]\tTime 0.063 (0.049)\tData 0.032 (0.017)\tLoss 1.6731 (1.4916)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [303][300/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.7099 (1.4859)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [303][350/391]\tTime 0.068 (0.049)\tData 0.036 (0.017)\tLoss 1.3090 (1.4856)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.093 (0.093)\tLoss 0.9295 (0.9295)\tPrec@1 85.156 (85.156)\n",
            "Test: [50/79]\tTime 0.011 (0.016)\tLoss 0.9486 (0.9290)\tPrec@1 82.031 (84.390)\n",
            " * Prec@1 84.570\n",
            "\n",
            "Epoch: 305/600\n",
            "Learning rate: 0.049004\n",
            "Epoch: [304][0/391]\tTime 0.210 (0.210)\tData 0.151 (0.151)\tLoss 1.7208 (1.7208)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [304][50/391]\tTime 0.031 (0.051)\tData 0.000 (0.018)\tLoss 1.2187 (1.5561)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [304][100/391]\tTime 0.032 (0.051)\tData 0.000 (0.018)\tLoss 1.7855 (1.5068)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [304][150/391]\tTime 0.032 (0.051)\tData 0.000 (0.018)\tLoss 1.2307 (1.5130)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [304][200/391]\tTime 0.033 (0.050)\tData 0.000 (0.018)\tLoss 1.7012 (1.5200)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [304][250/391]\tTime 0.033 (0.050)\tData 0.000 (0.018)\tLoss 1.7347 (1.5120)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [304][300/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.3074 (1.5033)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [304][350/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.8220 (1.5000)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.096 (0.096)\tLoss 1.0028 (1.0028)\tPrec@1 84.375 (84.375)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 1.0078 (0.9885)\tPrec@1 81.250 (82.108)\n",
            " * Prec@1 82.340\n",
            "\n",
            "Epoch: 306/600\n",
            "Learning rate: 0.048742\n",
            "Epoch: [305][0/391]\tTime 0.199 (0.199)\tData 0.152 (0.152)\tLoss 1.7015 (1.7015)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [305][50/391]\tTime 0.065 (0.051)\tData 0.033 (0.018)\tLoss 1.8650 (1.5079)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [305][100/391]\tTime 0.063 (0.050)\tData 0.029 (0.017)\tLoss 1.5470 (1.5091)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [305][150/391]\tTime 0.064 (0.049)\tData 0.033 (0.017)\tLoss 1.7062 (1.5017)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [305][200/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.7453 (1.5000)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [305][250/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.1189 (1.4963)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [305][300/391]\tTime 0.065 (0.049)\tData 0.032 (0.017)\tLoss 1.6179 (1.4964)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [305][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 0.9420 (1.4930)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 0.9485 (0.9485)\tPrec@1 85.938 (85.938)\n",
            "Test: [50/79]\tTime 0.010 (0.016)\tLoss 0.9279 (0.9388)\tPrec@1 85.938 (84.222)\n",
            " * Prec@1 84.710\n",
            "\n",
            "Epoch: 307/600\n",
            "Learning rate: 0.048481\n",
            "Epoch: [306][0/391]\tTime 0.210 (0.210)\tData 0.154 (0.154)\tLoss 1.6754 (1.6754)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [306][50/391]\tTime 0.063 (0.051)\tData 0.031 (0.018)\tLoss 1.6013 (1.4956)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [306][100/391]\tTime 0.065 (0.050)\tData 0.033 (0.017)\tLoss 1.3186 (1.4951)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [306][150/391]\tTime 0.060 (0.049)\tData 0.027 (0.017)\tLoss 1.6408 (1.4997)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [306][200/391]\tTime 0.066 (0.049)\tData 0.033 (0.017)\tLoss 1.8636 (1.4910)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [306][250/391]\tTime 0.063 (0.050)\tData 0.030 (0.017)\tLoss 1.4821 (1.4998)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [306][300/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.7424 (1.4981)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [306][350/391]\tTime 0.066 (0.049)\tData 0.034 (0.017)\tLoss 1.5495 (1.4932)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.093 (0.093)\tLoss 0.9888 (0.9888)\tPrec@1 82.812 (82.812)\n",
            "Test: [50/79]\tTime 0.022 (0.017)\tLoss 1.0189 (0.9761)\tPrec@1 79.688 (82.782)\n",
            " * Prec@1 82.930\n",
            "\n",
            "Epoch: 308/600\n",
            "Learning rate: 0.048220\n",
            "Epoch: [307][0/391]\tTime 0.216 (0.216)\tData 0.154 (0.154)\tLoss 1.3699 (1.3699)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [307][50/391]\tTime 0.065 (0.051)\tData 0.032 (0.018)\tLoss 1.6498 (1.5217)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [307][100/391]\tTime 0.058 (0.050)\tData 0.027 (0.017)\tLoss 1.7187 (1.5061)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [307][150/391]\tTime 0.066 (0.050)\tData 0.034 (0.017)\tLoss 1.7224 (1.5049)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [307][200/391]\tTime 0.070 (0.050)\tData 0.036 (0.017)\tLoss 1.1152 (1.4963)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [307][250/391]\tTime 0.065 (0.050)\tData 0.032 (0.017)\tLoss 1.6265 (1.5013)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [307][300/391]\tTime 0.065 (0.050)\tData 0.032 (0.017)\tLoss 1.5903 (1.5016)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [307][350/391]\tTime 0.064 (0.049)\tData 0.031 (0.017)\tLoss 1.2232 (1.4956)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 0.8687 (0.8687)\tPrec@1 86.719 (86.719)\n",
            "Test: [50/79]\tTime 0.021 (0.016)\tLoss 0.9771 (0.9011)\tPrec@1 82.031 (85.692)\n",
            " * Prec@1 85.890\n",
            "\n",
            "Epoch: 309/600\n",
            "Learning rate: 0.047958\n",
            "Epoch: [308][0/391]\tTime 0.206 (0.206)\tData 0.155 (0.155)\tLoss 1.8197 (1.8197)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [308][50/391]\tTime 0.032 (0.052)\tData 0.000 (0.017)\tLoss 1.0665 (1.4747)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [308][100/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.0952 (1.4778)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [308][150/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.5307 (1.4985)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [308][200/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.2584 (1.4962)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [308][250/391]\tTime 0.033 (0.049)\tData 0.000 (0.017)\tLoss 1.5188 (1.5062)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [308][300/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.2576 (1.4980)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [308][350/391]\tTime 0.034 (0.049)\tData 0.000 (0.017)\tLoss 1.4309 (1.5013)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.096 (0.096)\tLoss 0.9152 (0.9152)\tPrec@1 84.375 (84.375)\n",
            "Test: [50/79]\tTime 0.011 (0.016)\tLoss 0.9545 (0.9106)\tPrec@1 86.719 (86.029)\n",
            " * Prec@1 86.130\n",
            "\n",
            "Epoch: 310/600\n",
            "Learning rate: 0.047697\n",
            "Epoch: [309][0/391]\tTime 0.215 (0.215)\tData 0.158 (0.158)\tLoss 1.7894 (1.7894)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [309][50/391]\tTime 0.034 (0.056)\tData 0.000 (0.017)\tLoss 1.5961 (1.4725)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [309][100/391]\tTime 0.033 (0.052)\tData 0.000 (0.017)\tLoss 1.1372 (1.4573)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [309][150/391]\tTime 0.057 (0.051)\tData 0.025 (0.017)\tLoss 1.3435 (1.4865)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [309][200/391]\tTime 0.064 (0.051)\tData 0.032 (0.017)\tLoss 1.0977 (1.4721)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [309][250/391]\tTime 0.067 (0.051)\tData 0.035 (0.017)\tLoss 1.6846 (1.4698)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [309][300/391]\tTime 0.066 (0.050)\tData 0.034 (0.017)\tLoss 1.8195 (1.4666)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [309][350/391]\tTime 0.065 (0.050)\tData 0.032 (0.017)\tLoss 1.5791 (1.4760)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.094 (0.094)\tLoss 0.9544 (0.9544)\tPrec@1 86.719 (86.719)\n",
            "Test: [50/79]\tTime 0.011 (0.016)\tLoss 0.9713 (0.9540)\tPrec@1 84.375 (84.544)\n",
            " * Prec@1 84.790\n",
            "\n",
            "Epoch: 311/600\n",
            "Learning rate: 0.047436\n",
            "Epoch: [310][0/391]\tTime 0.200 (0.200)\tData 0.155 (0.155)\tLoss 1.4949 (1.4949)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [310][50/391]\tTime 0.064 (0.051)\tData 0.032 (0.019)\tLoss 1.4615 (1.4363)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [310][100/391]\tTime 0.068 (0.050)\tData 0.034 (0.017)\tLoss 1.6432 (1.4332)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [310][150/391]\tTime 0.065 (0.049)\tData 0.032 (0.017)\tLoss 1.5149 (1.4528)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [310][200/391]\tTime 0.064 (0.049)\tData 0.033 (0.017)\tLoss 1.2140 (1.4609)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [310][250/391]\tTime 0.033 (0.049)\tData 0.002 (0.016)\tLoss 1.4176 (1.4675)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [310][300/391]\tTime 0.056 (0.049)\tData 0.024 (0.016)\tLoss 1.4817 (1.4694)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [310][350/391]\tTime 0.069 (0.049)\tData 0.033 (0.016)\tLoss 1.3922 (1.4676)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.9744 (0.9744)\tPrec@1 83.594 (83.594)\n",
            "Test: [50/79]\tTime 0.017 (0.016)\tLoss 0.9507 (0.9692)\tPrec@1 85.938 (83.624)\n",
            " * Prec@1 83.640\n",
            "\n",
            "Epoch: 312/600\n",
            "Learning rate: 0.047175\n",
            "Epoch: [311][0/391]\tTime 0.200 (0.200)\tData 0.154 (0.154)\tLoss 1.7086 (1.7086)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [311][50/391]\tTime 0.065 (0.051)\tData 0.033 (0.019)\tLoss 1.5439 (1.4808)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [311][100/391]\tTime 0.066 (0.050)\tData 0.032 (0.017)\tLoss 1.4896 (1.4910)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [311][150/391]\tTime 0.062 (0.049)\tData 0.027 (0.017)\tLoss 1.5996 (1.5093)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [311][200/391]\tTime 0.066 (0.049)\tData 0.034 (0.017)\tLoss 1.8432 (1.5096)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [311][250/391]\tTime 0.063 (0.049)\tData 0.031 (0.017)\tLoss 1.8267 (1.5082)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [311][300/391]\tTime 0.067 (0.049)\tData 0.035 (0.016)\tLoss 1.7240 (1.5016)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [311][350/391]\tTime 0.064 (0.049)\tData 0.032 (0.016)\tLoss 1.7760 (1.4911)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.099 (0.099)\tLoss 1.0815 (1.0815)\tPrec@1 78.125 (78.125)\n",
            "Test: [50/79]\tTime 0.022 (0.016)\tLoss 1.1410 (1.0542)\tPrec@1 73.438 (78.922)\n",
            " * Prec@1 78.730\n",
            "\n",
            "Epoch: 313/600\n",
            "Learning rate: 0.046914\n",
            "Epoch: [312][0/391]\tTime 0.220 (0.220)\tData 0.157 (0.157)\tLoss 1.7688 (1.7688)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [312][50/391]\tTime 0.062 (0.053)\tData 0.030 (0.020)\tLoss 1.0219 (1.5066)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [312][100/391]\tTime 0.068 (0.051)\tData 0.035 (0.018)\tLoss 1.6158 (1.4961)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [312][150/391]\tTime 0.067 (0.051)\tData 0.035 (0.018)\tLoss 1.2297 (1.4929)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [312][200/391]\tTime 0.073 (0.051)\tData 0.029 (0.018)\tLoss 1.1329 (1.4979)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [312][250/391]\tTime 0.064 (0.050)\tData 0.032 (0.018)\tLoss 1.8250 (1.5115)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [312][300/391]\tTime 0.066 (0.050)\tData 0.035 (0.017)\tLoss 1.6854 (1.5041)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [312][350/391]\tTime 0.066 (0.050)\tData 0.033 (0.017)\tLoss 1.7190 (1.5056)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.9695 (0.9695)\tPrec@1 86.719 (86.719)\n",
            "Test: [50/79]\tTime 0.011 (0.016)\tLoss 0.9606 (0.9622)\tPrec@1 86.719 (85.585)\n",
            " * Prec@1 85.810\n",
            "\n",
            "Epoch: 314/600\n",
            "Learning rate: 0.046653\n",
            "Epoch: [313][0/391]\tTime 0.218 (0.218)\tData 0.158 (0.158)\tLoss 1.7289 (1.7289)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [313][50/391]\tTime 0.070 (0.052)\tData 0.039 (0.019)\tLoss 1.5450 (1.5156)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [313][100/391]\tTime 0.068 (0.051)\tData 0.036 (0.019)\tLoss 1.6400 (1.4921)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [313][150/391]\tTime 0.068 (0.051)\tData 0.036 (0.018)\tLoss 1.0602 (1.4905)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [313][200/391]\tTime 0.069 (0.050)\tData 0.037 (0.018)\tLoss 1.7855 (1.4966)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [313][250/391]\tTime 0.063 (0.050)\tData 0.031 (0.018)\tLoss 1.3217 (1.4854)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [313][300/391]\tTime 0.065 (0.050)\tData 0.034 (0.018)\tLoss 1.8402 (1.4771)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [313][350/391]\tTime 0.077 (0.050)\tData 0.044 (0.018)\tLoss 1.1216 (1.4799)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.100 (0.100)\tLoss 0.9849 (0.9849)\tPrec@1 79.688 (79.688)\n",
            "Test: [50/79]\tTime 0.019 (0.017)\tLoss 0.9984 (0.9616)\tPrec@1 82.031 (83.624)\n",
            " * Prec@1 83.660\n",
            "\n",
            "Epoch: 315/600\n",
            "Learning rate: 0.046392\n",
            "Epoch: [314][0/391]\tTime 0.199 (0.199)\tData 0.154 (0.154)\tLoss 1.5330 (1.5330)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [314][50/391]\tTime 0.039 (0.053)\tData 0.001 (0.017)\tLoss 1.7712 (1.5523)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [314][100/391]\tTime 0.032 (0.051)\tData 0.000 (0.016)\tLoss 1.5248 (1.5037)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [314][150/391]\tTime 0.031 (0.050)\tData 0.000 (0.016)\tLoss 1.2200 (1.5098)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [314][200/391]\tTime 0.032 (0.050)\tData 0.000 (0.016)\tLoss 1.1640 (1.5021)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [314][250/391]\tTime 0.036 (0.049)\tData 0.004 (0.016)\tLoss 0.9595 (1.4955)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [314][300/391]\tTime 0.039 (0.049)\tData 0.007 (0.016)\tLoss 1.7672 (1.5017)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [314][350/391]\tTime 0.031 (0.049)\tData 0.000 (0.016)\tLoss 1.7058 (1.4994)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.096 (0.096)\tLoss 0.9416 (0.9416)\tPrec@1 83.594 (83.594)\n",
            "Test: [50/79]\tTime 0.010 (0.016)\tLoss 0.9808 (0.9146)\tPrec@1 79.688 (85.401)\n",
            " * Prec@1 85.540\n",
            "\n",
            "Epoch: 316/600\n",
            "Learning rate: 0.046131\n",
            "Epoch: [315][0/391]\tTime 0.202 (0.202)\tData 0.155 (0.155)\tLoss 1.3101 (1.3101)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [315][50/391]\tTime 0.063 (0.051)\tData 0.031 (0.018)\tLoss 1.4594 (1.4659)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [315][100/391]\tTime 0.055 (0.049)\tData 0.024 (0.017)\tLoss 1.0420 (1.4838)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [315][150/391]\tTime 0.047 (0.049)\tData 0.016 (0.017)\tLoss 1.6022 (1.4754)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [315][200/391]\tTime 0.044 (0.049)\tData 0.013 (0.017)\tLoss 1.6905 (1.4848)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [315][250/391]\tTime 0.041 (0.049)\tData 0.010 (0.017)\tLoss 1.7912 (1.4917)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [315][300/391]\tTime 0.052 (0.048)\tData 0.021 (0.017)\tLoss 1.4953 (1.4917)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [315][350/391]\tTime 0.032 (0.048)\tData 0.000 (0.017)\tLoss 1.1015 (1.4933)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 0.9750 (0.9750)\tPrec@1 82.031 (82.031)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.9947 (0.9998)\tPrec@1 81.250 (80.775)\n",
            " * Prec@1 81.170\n",
            "\n",
            "Epoch: 317/600\n",
            "Learning rate: 0.045870\n",
            "Epoch: [316][0/391]\tTime 0.209 (0.209)\tData 0.153 (0.153)\tLoss 1.1079 (1.1079)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [316][50/391]\tTime 0.065 (0.051)\tData 0.032 (0.018)\tLoss 1.2952 (1.4445)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [316][100/391]\tTime 0.065 (0.050)\tData 0.031 (0.017)\tLoss 1.5526 (1.4717)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [316][150/391]\tTime 0.064 (0.049)\tData 0.030 (0.016)\tLoss 1.8095 (1.4536)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [316][200/391]\tTime 0.066 (0.049)\tData 0.034 (0.016)\tLoss 1.4168 (1.4530)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [316][250/391]\tTime 0.066 (0.049)\tData 0.034 (0.016)\tLoss 1.5732 (1.4691)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [316][300/391]\tTime 0.064 (0.049)\tData 0.033 (0.016)\tLoss 0.9262 (1.4690)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [316][350/391]\tTime 0.063 (0.049)\tData 0.031 (0.016)\tLoss 1.2255 (1.4677)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.100 (0.100)\tLoss 0.9086 (0.9086)\tPrec@1 85.938 (85.938)\n",
            "Test: [50/79]\tTime 0.011 (0.017)\tLoss 0.9063 (0.9310)\tPrec@1 87.500 (84.789)\n",
            " * Prec@1 84.840\n",
            "\n",
            "Epoch: 318/600\n",
            "Learning rate: 0.045610\n",
            "Epoch: [317][0/391]\tTime 0.214 (0.214)\tData 0.154 (0.154)\tLoss 1.6909 (1.6909)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [317][50/391]\tTime 0.064 (0.051)\tData 0.032 (0.019)\tLoss 1.7713 (1.5125)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [317][100/391]\tTime 0.065 (0.050)\tData 0.034 (0.018)\tLoss 1.7087 (1.4829)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [317][150/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.7618 (1.4834)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [317][200/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.6789 (1.4903)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [317][250/391]\tTime 0.065 (0.049)\tData 0.034 (0.017)\tLoss 1.2668 (1.4937)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [317][300/391]\tTime 0.050 (0.049)\tData 0.018 (0.017)\tLoss 1.4793 (1.4902)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [317][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.7828 (1.4827)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.093 (0.093)\tLoss 0.9363 (0.9363)\tPrec@1 85.938 (85.938)\n",
            "Test: [50/79]\tTime 0.010 (0.016)\tLoss 0.9328 (0.9377)\tPrec@1 88.281 (85.800)\n",
            " * Prec@1 86.020\n",
            "\n",
            "Epoch: 319/600\n",
            "Learning rate: 0.045349\n",
            "Epoch: [318][0/391]\tTime 0.201 (0.201)\tData 0.156 (0.156)\tLoss 1.0734 (1.0734)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [318][50/391]\tTime 0.066 (0.054)\tData 0.034 (0.019)\tLoss 1.6765 (1.4685)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [318][100/391]\tTime 0.069 (0.052)\tData 0.036 (0.018)\tLoss 1.0515 (1.4768)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [318][150/391]\tTime 0.068 (0.051)\tData 0.037 (0.018)\tLoss 1.7961 (1.4929)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [318][200/391]\tTime 0.064 (0.050)\tData 0.033 (0.017)\tLoss 1.5661 (1.4904)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [318][250/391]\tTime 0.066 (0.050)\tData 0.034 (0.017)\tLoss 0.9716 (1.4901)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [318][300/391]\tTime 0.065 (0.050)\tData 0.033 (0.017)\tLoss 1.6078 (1.4929)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [318][350/391]\tTime 0.063 (0.049)\tData 0.032 (0.017)\tLoss 1.4258 (1.4895)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.098 (0.098)\tLoss 1.0189 (1.0189)\tPrec@1 78.125 (78.125)\n",
            "Test: [50/79]\tTime 0.010 (0.016)\tLoss 1.0054 (0.9656)\tPrec@1 83.594 (84.176)\n",
            " * Prec@1 84.040\n",
            "\n",
            "Epoch: 320/600\n",
            "Learning rate: 0.045089\n",
            "Epoch: [319][0/391]\tTime 0.201 (0.201)\tData 0.155 (0.155)\tLoss 1.6435 (1.6435)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [319][50/391]\tTime 0.068 (0.051)\tData 0.034 (0.019)\tLoss 0.9497 (1.4478)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [319][100/391]\tTime 0.062 (0.050)\tData 0.030 (0.017)\tLoss 1.8024 (1.4766)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [319][150/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 1.0316 (1.4895)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [319][200/391]\tTime 0.034 (0.049)\tData 0.000 (0.017)\tLoss 1.5480 (1.4738)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [319][250/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.7971 (1.4805)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [319][300/391]\tTime 0.052 (0.049)\tData 0.019 (0.017)\tLoss 1.7660 (1.4809)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [319][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.7684 (1.4834)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 1.0126 (1.0126)\tPrec@1 79.688 (79.688)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 1.0186 (0.9663)\tPrec@1 78.125 (82.583)\n",
            " * Prec@1 83.030\n",
            "\n",
            "Epoch: 321/600\n",
            "Learning rate: 0.044829\n",
            "Epoch: [320][0/391]\tTime 0.203 (0.203)\tData 0.156 (0.156)\tLoss 1.6339 (1.6339)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [320][50/391]\tTime 0.064 (0.051)\tData 0.031 (0.018)\tLoss 1.2118 (1.4966)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [320][100/391]\tTime 0.076 (0.050)\tData 0.033 (0.017)\tLoss 1.2660 (1.4945)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [320][150/391]\tTime 0.043 (0.049)\tData 0.011 (0.017)\tLoss 1.1758 (1.4765)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [320][200/391]\tTime 0.056 (0.049)\tData 0.025 (0.017)\tLoss 1.6323 (1.4803)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [320][250/391]\tTime 0.051 (0.049)\tData 0.019 (0.016)\tLoss 1.8497 (1.4869)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [320][300/391]\tTime 0.042 (0.049)\tData 0.010 (0.016)\tLoss 1.5661 (1.4913)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [320][350/391]\tTime 0.031 (0.049)\tData 0.000 (0.016)\tLoss 1.4358 (1.4909)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.093 (0.093)\tLoss 0.9513 (0.9513)\tPrec@1 85.156 (85.156)\n",
            "Test: [50/79]\tTime 0.013 (0.016)\tLoss 0.9701 (0.9050)\tPrec@1 83.594 (85.892)\n",
            " * Prec@1 86.260\n",
            "\n",
            "Epoch: 322/600\n",
            "Learning rate: 0.044569\n",
            "Epoch: [321][0/391]\tTime 0.206 (0.206)\tData 0.161 (0.161)\tLoss 1.8417 (1.8417)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [321][50/391]\tTime 0.064 (0.051)\tData 0.033 (0.019)\tLoss 1.6951 (1.5076)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [321][100/391]\tTime 0.064 (0.049)\tData 0.032 (0.018)\tLoss 1.2070 (1.5117)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [321][150/391]\tTime 0.064 (0.049)\tData 0.030 (0.017)\tLoss 1.7614 (1.4931)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [321][200/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 1.7909 (1.4945)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [321][250/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.7018 (1.4924)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [321][300/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.7464 (1.4844)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [321][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.4184 (1.4775)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.096 (0.096)\tLoss 0.9020 (0.9020)\tPrec@1 87.500 (87.500)\n",
            "Test: [50/79]\tTime 0.010 (0.017)\tLoss 0.9877 (0.9237)\tPrec@1 80.469 (85.677)\n",
            " * Prec@1 85.950\n",
            "\n",
            "Epoch: 323/600\n",
            "Learning rate: 0.044309\n",
            "Epoch: [322][0/391]\tTime 0.198 (0.198)\tData 0.152 (0.152)\tLoss 1.7210 (1.7210)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [322][50/391]\tTime 0.064 (0.051)\tData 0.031 (0.018)\tLoss 1.4785 (1.4890)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [322][100/391]\tTime 0.064 (0.050)\tData 0.032 (0.017)\tLoss 1.6308 (1.5047)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [322][150/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.6946 (1.4948)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [322][200/391]\tTime 0.062 (0.049)\tData 0.030 (0.017)\tLoss 1.4915 (1.5049)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [322][250/391]\tTime 0.065 (0.049)\tData 0.034 (0.017)\tLoss 1.7881 (1.4968)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [322][300/391]\tTime 0.069 (0.049)\tData 0.036 (0.017)\tLoss 1.7380 (1.5030)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [322][350/391]\tTime 0.067 (0.049)\tData 0.035 (0.017)\tLoss 1.0540 (1.4972)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.094 (0.094)\tLoss 1.0013 (1.0013)\tPrec@1 84.375 (84.375)\n",
            "Test: [50/79]\tTime 0.011 (0.016)\tLoss 1.1008 (1.0319)\tPrec@1 75.781 (80.913)\n",
            " * Prec@1 81.460\n",
            "\n",
            "Epoch: 324/600\n",
            "Learning rate: 0.044049\n",
            "Epoch: [323][0/391]\tTime 0.201 (0.201)\tData 0.154 (0.154)\tLoss 1.2587 (1.2587)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [323][50/391]\tTime 0.055 (0.051)\tData 0.024 (0.018)\tLoss 1.2827 (1.4934)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [323][100/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.7046 (1.4851)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [323][150/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.4014 (1.4705)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [323][200/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.2149 (1.4830)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [323][250/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.5971 (1.4839)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [323][300/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.2635 (1.4861)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [323][350/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.6878 (1.4875)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.104 (0.104)\tLoss 0.9053 (0.9053)\tPrec@1 85.156 (85.156)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.9647 (0.9351)\tPrec@1 85.156 (85.172)\n",
            " * Prec@1 85.310\n",
            "\n",
            "Epoch: 325/600\n",
            "Learning rate: 0.043790\n",
            "Epoch: [324][0/391]\tTime 0.202 (0.202)\tData 0.153 (0.153)\tLoss 1.5338 (1.5338)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [324][50/391]\tTime 0.063 (0.051)\tData 0.028 (0.018)\tLoss 1.6474 (1.4737)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [324][100/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 0.9623 (1.4615)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [324][150/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.6162 (1.4648)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [324][200/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 1.8301 (1.4697)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [324][250/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.3695 (1.4733)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [324][300/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.7804 (1.4763)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [324][350/391]\tTime 0.042 (0.050)\tData 0.000 (0.017)\tLoss 1.6390 (1.4873)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.094 (0.094)\tLoss 0.9419 (0.9419)\tPrec@1 89.062 (89.062)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.9156 (0.9234)\tPrec@1 85.156 (86.474)\n",
            " * Prec@1 86.430\n",
            "\n",
            "Epoch: 326/600\n",
            "Learning rate: 0.043530\n",
            "Epoch: [325][0/391]\tTime 0.214 (0.214)\tData 0.158 (0.158)\tLoss 1.5888 (1.5888)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [325][50/391]\tTime 0.072 (0.061)\tData 0.029 (0.018)\tLoss 1.7502 (1.4545)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [325][100/391]\tTime 0.063 (0.057)\tData 0.031 (0.017)\tLoss 1.2933 (1.4673)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [325][150/391]\tTime 0.064 (0.054)\tData 0.033 (0.017)\tLoss 1.8039 (1.4545)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [325][200/391]\tTime 0.063 (0.053)\tData 0.032 (0.017)\tLoss 1.1833 (1.4531)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [325][250/391]\tTime 0.064 (0.052)\tData 0.032 (0.017)\tLoss 1.6817 (1.4671)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [325][300/391]\tTime 0.063 (0.051)\tData 0.030 (0.016)\tLoss 1.0439 (1.4692)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [325][350/391]\tTime 0.065 (0.051)\tData 0.033 (0.016)\tLoss 1.3093 (1.4695)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 0.9623 (0.9623)\tPrec@1 82.812 (82.812)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 1.0070 (0.9494)\tPrec@1 79.688 (83.900)\n",
            " * Prec@1 83.770\n",
            "\n",
            "Epoch: 327/600\n",
            "Learning rate: 0.043271\n",
            "Epoch: [326][0/391]\tTime 0.208 (0.208)\tData 0.159 (0.159)\tLoss 1.1713 (1.1713)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [326][50/391]\tTime 0.067 (0.051)\tData 0.036 (0.019)\tLoss 1.2652 (1.4557)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [326][100/391]\tTime 0.067 (0.050)\tData 0.035 (0.018)\tLoss 1.1597 (1.4718)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [326][150/391]\tTime 0.076 (0.050)\tData 0.033 (0.017)\tLoss 1.7112 (1.4868)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [326][200/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.6476 (1.4909)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [326][250/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 0.9614 (1.4853)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [326][300/391]\tTime 0.068 (0.049)\tData 0.036 (0.017)\tLoss 1.7062 (1.4854)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [326][350/391]\tTime 0.067 (0.049)\tData 0.036 (0.017)\tLoss 1.6448 (1.4851)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.094 (0.094)\tLoss 0.9232 (0.9232)\tPrec@1 87.500 (87.500)\n",
            "Test: [50/79]\tTime 0.010 (0.016)\tLoss 0.9138 (0.8973)\tPrec@1 86.719 (85.892)\n",
            " * Prec@1 86.500\n",
            "\n",
            "Epoch: 328/600\n",
            "Learning rate: 0.043012\n",
            "Epoch: [327][0/391]\tTime 0.216 (0.216)\tData 0.160 (0.160)\tLoss 1.7593 (1.7593)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [327][50/391]\tTime 0.063 (0.051)\tData 0.031 (0.019)\tLoss 1.0257 (1.4914)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [327][100/391]\tTime 0.063 (0.050)\tData 0.031 (0.018)\tLoss 1.5486 (1.4770)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [327][150/391]\tTime 0.063 (0.049)\tData 0.031 (0.017)\tLoss 1.4990 (1.5000)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [327][200/391]\tTime 0.056 (0.049)\tData 0.025 (0.017)\tLoss 1.5063 (1.5013)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [327][250/391]\tTime 0.064 (0.049)\tData 0.031 (0.017)\tLoss 1.6823 (1.4880)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [327][300/391]\tTime 0.049 (0.048)\tData 0.017 (0.016)\tLoss 1.6334 (1.4904)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [327][350/391]\tTime 0.037 (0.048)\tData 0.005 (0.016)\tLoss 1.6890 (1.4902)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.096 (0.096)\tLoss 0.9544 (0.9544)\tPrec@1 82.812 (82.812)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.9063 (0.9285)\tPrec@1 83.594 (84.237)\n",
            " * Prec@1 84.270\n",
            "\n",
            "Epoch: 329/600\n",
            "Learning rate: 0.042753\n",
            "Epoch: [328][0/391]\tTime 0.200 (0.200)\tData 0.154 (0.154)\tLoss 1.0956 (1.0956)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [328][50/391]\tTime 0.068 (0.053)\tData 0.036 (0.019)\tLoss 1.0614 (1.4549)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [328][100/391]\tTime 0.066 (0.051)\tData 0.033 (0.018)\tLoss 1.4810 (1.4742)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [328][150/391]\tTime 0.067 (0.051)\tData 0.035 (0.018)\tLoss 1.7342 (1.4555)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [328][200/391]\tTime 0.065 (0.051)\tData 0.033 (0.018)\tLoss 1.2011 (1.4587)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [328][250/391]\tTime 0.069 (0.050)\tData 0.035 (0.018)\tLoss 1.6017 (1.4596)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [328][300/391]\tTime 0.064 (0.050)\tData 0.031 (0.018)\tLoss 1.4299 (1.4642)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [328][350/391]\tTime 0.064 (0.050)\tData 0.032 (0.017)\tLoss 1.8028 (1.4656)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.105 (0.105)\tLoss 0.9879 (0.9879)\tPrec@1 85.156 (85.156)\n",
            "Test: [50/79]\tTime 0.013 (0.017)\tLoss 0.9947 (0.9693)\tPrec@1 82.031 (83.961)\n",
            " * Prec@1 84.140\n",
            "\n",
            "Epoch: 330/600\n",
            "Learning rate: 0.042495\n",
            "Epoch: [329][0/391]\tTime 0.214 (0.214)\tData 0.155 (0.155)\tLoss 1.7666 (1.7666)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [329][50/391]\tTime 0.035 (0.052)\tData 0.000 (0.018)\tLoss 1.5784 (1.5177)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [329][100/391]\tTime 0.032 (0.051)\tData 0.000 (0.018)\tLoss 1.6366 (1.4851)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [329][150/391]\tTime 0.033 (0.051)\tData 0.000 (0.018)\tLoss 1.7282 (1.4767)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [329][200/391]\tTime 0.032 (0.051)\tData 0.000 (0.018)\tLoss 1.7961 (1.4827)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [329][250/391]\tTime 0.033 (0.051)\tData 0.000 (0.018)\tLoss 1.4296 (1.4724)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [329][300/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.7924 (1.4748)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [329][350/391]\tTime 0.033 (0.050)\tData 0.000 (0.017)\tLoss 1.1359 (1.4787)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 0.9625 (0.9625)\tPrec@1 83.594 (83.594)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.8785 (0.9165)\tPrec@1 90.625 (85.539)\n",
            " * Prec@1 85.850\n",
            "\n",
            "Epoch: 331/600\n",
            "Learning rate: 0.042236\n",
            "Epoch: [330][0/391]\tTime 0.200 (0.200)\tData 0.154 (0.154)\tLoss 1.0635 (1.0635)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [330][50/391]\tTime 0.042 (0.051)\tData 0.000 (0.017)\tLoss 1.6933 (1.4966)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [330][100/391]\tTime 0.033 (0.052)\tData 0.000 (0.017)\tLoss 1.7221 (1.4812)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [330][150/391]\tTime 0.032 (0.051)\tData 0.000 (0.016)\tLoss 1.7416 (1.4847)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [330][200/391]\tTime 0.031 (0.050)\tData 0.000 (0.016)\tLoss 1.1564 (1.4879)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [330][250/391]\tTime 0.037 (0.050)\tData 0.006 (0.017)\tLoss 1.6600 (1.4974)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [330][300/391]\tTime 0.035 (0.050)\tData 0.000 (0.016)\tLoss 1.4675 (1.4980)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [330][350/391]\tTime 0.063 (0.050)\tData 0.031 (0.016)\tLoss 1.2057 (1.4930)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.094 (0.094)\tLoss 0.9261 (0.9261)\tPrec@1 86.719 (86.719)\n",
            "Test: [50/79]\tTime 0.020 (0.016)\tLoss 0.9078 (0.9125)\tPrec@1 85.938 (85.784)\n",
            " * Prec@1 85.610\n",
            "\n",
            "Epoch: 332/600\n",
            "Learning rate: 0.041978\n",
            "Epoch: [331][0/391]\tTime 0.200 (0.200)\tData 0.154 (0.154)\tLoss 1.3964 (1.3964)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [331][50/391]\tTime 0.049 (0.050)\tData 0.017 (0.018)\tLoss 1.4949 (1.4803)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [331][100/391]\tTime 0.036 (0.049)\tData 0.004 (0.017)\tLoss 1.8051 (1.4775)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [331][150/391]\tTime 0.035 (0.049)\tData 0.002 (0.017)\tLoss 1.8262 (1.4747)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [331][200/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.2912 (1.4835)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [331][250/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.6872 (1.4796)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [331][300/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.2565 (1.4678)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [331][350/391]\tTime 0.032 (0.048)\tData 0.000 (0.016)\tLoss 1.1479 (1.4642)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 0.8872 (0.8872)\tPrec@1 89.844 (89.844)\n",
            "Test: [50/79]\tTime 0.019 (0.016)\tLoss 0.9098 (0.9159)\tPrec@1 83.594 (85.769)\n",
            " * Prec@1 85.720\n",
            "\n",
            "Epoch: 333/600\n",
            "Learning rate: 0.041720\n",
            "Epoch: [332][0/391]\tTime 0.200 (0.200)\tData 0.154 (0.154)\tLoss 1.7806 (1.7806)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [332][50/391]\tTime 0.063 (0.051)\tData 0.029 (0.018)\tLoss 1.2562 (1.4704)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [332][100/391]\tTime 0.033 (0.051)\tData 0.000 (0.016)\tLoss 1.2596 (1.4538)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [332][150/391]\tTime 0.033 (0.050)\tData 0.000 (0.016)\tLoss 1.6294 (1.4685)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [332][200/391]\tTime 0.032 (0.050)\tData 0.000 (0.016)\tLoss 0.9641 (1.4785)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [332][250/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.6717 (1.4706)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [332][300/391]\tTime 0.031 (0.049)\tData 0.000 (0.016)\tLoss 1.5862 (1.4680)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [332][350/391]\tTime 0.062 (0.049)\tData 0.031 (0.016)\tLoss 1.8190 (1.4656)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.094 (0.094)\tLoss 0.8176 (0.8176)\tPrec@1 90.625 (90.625)\n",
            "Test: [50/79]\tTime 0.020 (0.016)\tLoss 0.8979 (0.8750)\tPrec@1 84.375 (86.811)\n",
            " * Prec@1 86.910\n",
            "\n",
            "Epoch: 334/600\n",
            "Learning rate: 0.041462\n",
            "Epoch: [333][0/391]\tTime 0.209 (0.209)\tData 0.161 (0.161)\tLoss 1.7492 (1.7492)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [333][50/391]\tTime 0.066 (0.051)\tData 0.033 (0.018)\tLoss 1.3425 (1.4164)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [333][100/391]\tTime 0.066 (0.050)\tData 0.033 (0.017)\tLoss 1.3197 (1.4679)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [333][150/391]\tTime 0.065 (0.050)\tData 0.033 (0.017)\tLoss 1.5611 (1.4441)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [333][200/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.5217 (1.4682)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [333][250/391]\tTime 0.041 (0.049)\tData 0.000 (0.016)\tLoss 1.7154 (1.4832)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [333][300/391]\tTime 0.032 (0.051)\tData 0.000 (0.016)\tLoss 1.0354 (1.4695)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [333][350/391]\tTime 0.032 (0.050)\tData 0.000 (0.016)\tLoss 1.3091 (1.4726)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.096 (0.096)\tLoss 0.8781 (0.8781)\tPrec@1 89.062 (89.062)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.9104 (0.9111)\tPrec@1 85.156 (86.780)\n",
            " * Prec@1 87.070\n",
            "\n",
            "Epoch: 335/600\n",
            "Learning rate: 0.041205\n",
            "Epoch: [334][0/391]\tTime 0.204 (0.204)\tData 0.157 (0.157)\tLoss 1.3102 (1.3102)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [334][50/391]\tTime 0.067 (0.053)\tData 0.033 (0.018)\tLoss 1.3888 (1.4766)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [334][100/391]\tTime 0.067 (0.051)\tData 0.033 (0.017)\tLoss 1.7986 (1.4816)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [334][150/391]\tTime 0.066 (0.050)\tData 0.034 (0.017)\tLoss 1.4421 (1.4730)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [334][200/391]\tTime 0.067 (0.050)\tData 0.035 (0.017)\tLoss 1.6948 (1.4707)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [334][250/391]\tTime 0.065 (0.050)\tData 0.033 (0.017)\tLoss 1.7217 (1.4612)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [334][300/391]\tTime 0.061 (0.050)\tData 0.029 (0.017)\tLoss 1.6954 (1.4710)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [334][350/391]\tTime 0.062 (0.050)\tData 0.030 (0.017)\tLoss 1.6972 (1.4641)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.094 (0.094)\tLoss 0.9346 (0.9346)\tPrec@1 85.156 (85.156)\n",
            "Test: [50/79]\tTime 0.021 (0.016)\tLoss 0.9844 (0.9664)\tPrec@1 81.250 (83.640)\n",
            " * Prec@1 83.740\n",
            "\n",
            "Epoch: 336/600\n",
            "Learning rate: 0.040947\n",
            "Epoch: [335][0/391]\tTime 0.201 (0.201)\tData 0.154 (0.154)\tLoss 1.2921 (1.2921)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [335][50/391]\tTime 0.063 (0.051)\tData 0.031 (0.018)\tLoss 1.6082 (1.4521)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [335][100/391]\tTime 0.062 (0.050)\tData 0.029 (0.017)\tLoss 1.5807 (1.4495)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [335][150/391]\tTime 0.044 (0.051)\tData 0.000 (0.016)\tLoss 1.4751 (1.4931)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [335][200/391]\tTime 0.043 (0.053)\tData 0.000 (0.017)\tLoss 1.2647 (1.4922)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [335][250/391]\tTime 0.043 (0.054)\tData 0.000 (0.017)\tLoss 1.7408 (1.4811)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [335][300/391]\tTime 0.076 (0.055)\tData 0.034 (0.016)\tLoss 1.0024 (1.4782)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [335][350/391]\tTime 0.064 (0.054)\tData 0.033 (0.016)\tLoss 1.2434 (1.4682)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 1.0394 (1.0394)\tPrec@1 81.250 (81.250)\n",
            "Test: [50/79]\tTime 0.010 (0.016)\tLoss 1.0980 (1.0051)\tPrec@1 74.219 (82.138)\n",
            " * Prec@1 82.040\n",
            "\n",
            "Epoch: 337/600\n",
            "Learning rate: 0.040690\n",
            "Epoch: [336][0/391]\tTime 0.199 (0.199)\tData 0.152 (0.152)\tLoss 0.9631 (0.9631)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [336][50/391]\tTime 0.032 (0.051)\tData 0.000 (0.019)\tLoss 1.7349 (1.4966)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [336][100/391]\tTime 0.079 (0.051)\tData 0.046 (0.017)\tLoss 1.5921 (1.4740)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [336][150/391]\tTime 0.067 (0.050)\tData 0.035 (0.017)\tLoss 1.5704 (1.4772)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [336][200/391]\tTime 0.065 (0.050)\tData 0.034 (0.017)\tLoss 1.7371 (1.4674)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [336][250/391]\tTime 0.064 (0.050)\tData 0.032 (0.017)\tLoss 1.1797 (1.4712)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [336][300/391]\tTime 0.036 (0.049)\tData 0.005 (0.017)\tLoss 1.7582 (1.4677)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [336][350/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.7592 (1.4721)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.096 (0.096)\tLoss 0.8652 (0.8652)\tPrec@1 89.844 (89.844)\n",
            "Test: [50/79]\tTime 0.020 (0.016)\tLoss 0.8908 (0.8685)\tPrec@1 85.156 (87.990)\n",
            " * Prec@1 87.880\n",
            "\n",
            "Epoch: 338/600\n",
            "Learning rate: 0.040434\n",
            "Epoch: [337][0/391]\tTime 0.215 (0.215)\tData 0.156 (0.156)\tLoss 1.7149 (1.7149)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [337][50/391]\tTime 0.063 (0.053)\tData 0.032 (0.019)\tLoss 1.0040 (1.5144)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [337][100/391]\tTime 0.065 (0.050)\tData 0.033 (0.017)\tLoss 1.5963 (1.5163)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [337][150/391]\tTime 0.064 (0.050)\tData 0.032 (0.017)\tLoss 1.7322 (1.5019)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [337][200/391]\tTime 0.063 (0.049)\tData 0.030 (0.017)\tLoss 1.7421 (1.4928)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [337][250/391]\tTime 0.057 (0.049)\tData 0.025 (0.017)\tLoss 0.9632 (1.4842)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [337][300/391]\tTime 0.033 (0.049)\tData 0.000 (0.016)\tLoss 1.7619 (1.4886)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [337][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.5318 (1.4964)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.102 (0.102)\tLoss 0.9384 (0.9384)\tPrec@1 85.156 (85.156)\n",
            "Test: [50/79]\tTime 0.013 (0.016)\tLoss 0.9898 (0.9293)\tPrec@1 81.250 (85.600)\n",
            " * Prec@1 85.940\n",
            "\n",
            "Epoch: 339/600\n",
            "Learning rate: 0.040177\n",
            "Epoch: [338][0/391]\tTime 0.201 (0.201)\tData 0.153 (0.153)\tLoss 1.5330 (1.5330)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [338][50/391]\tTime 0.064 (0.051)\tData 0.030 (0.019)\tLoss 1.6717 (1.4904)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [338][100/391]\tTime 0.063 (0.049)\tData 0.031 (0.018)\tLoss 1.7003 (1.5010)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [338][150/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 0.9760 (1.4928)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [338][200/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.8285 (1.4802)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [338][250/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.6607 (1.4860)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [338][300/391]\tTime 0.033 (0.050)\tData 0.000 (0.016)\tLoss 1.7941 (1.4818)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [338][350/391]\tTime 0.033 (0.049)\tData 0.000 (0.016)\tLoss 1.4985 (1.4851)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.105 (0.105)\tLoss 0.9383 (0.9383)\tPrec@1 85.938 (85.938)\n",
            "Test: [50/79]\tTime 0.010 (0.017)\tLoss 0.9432 (0.9199)\tPrec@1 81.250 (84.911)\n",
            " * Prec@1 84.920\n",
            "\n",
            "Epoch: 340/600\n",
            "Learning rate: 0.039921\n",
            "Epoch: [339][0/391]\tTime 0.210 (0.210)\tData 0.158 (0.158)\tLoss 1.2155 (1.2155)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [339][50/391]\tTime 0.064 (0.052)\tData 0.033 (0.019)\tLoss 1.3588 (1.4685)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [339][100/391]\tTime 0.064 (0.050)\tData 0.032 (0.018)\tLoss 0.9886 (1.4581)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [339][150/391]\tTime 0.063 (0.050)\tData 0.031 (0.018)\tLoss 0.9832 (1.4496)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [339][200/391]\tTime 0.064 (0.049)\tData 0.031 (0.017)\tLoss 1.4740 (1.4614)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [339][250/391]\tTime 0.064 (0.049)\tData 0.033 (0.017)\tLoss 1.3163 (1.4647)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [339][300/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.7584 (1.4770)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [339][350/391]\tTime 0.065 (0.049)\tData 0.034 (0.017)\tLoss 1.6645 (1.4765)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.093 (0.093)\tLoss 0.8722 (0.8722)\tPrec@1 89.844 (89.844)\n",
            "Test: [50/79]\tTime 0.016 (0.016)\tLoss 0.9108 (0.9000)\tPrec@1 84.375 (85.953)\n",
            " * Prec@1 86.050\n",
            "\n",
            "Epoch: 341/600\n",
            "Learning rate: 0.039665\n",
            "Epoch: [340][0/391]\tTime 0.199 (0.199)\tData 0.153 (0.153)\tLoss 1.3621 (1.3621)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [340][50/391]\tTime 0.062 (0.052)\tData 0.018 (0.018)\tLoss 1.3421 (1.4658)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [340][100/391]\tTime 0.077 (0.051)\tData 0.035 (0.017)\tLoss 1.6338 (1.4566)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [340][150/391]\tTime 0.064 (0.050)\tData 0.032 (0.017)\tLoss 0.9433 (1.4733)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [340][200/391]\tTime 0.063 (0.049)\tData 0.032 (0.017)\tLoss 1.0293 (1.4883)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [340][250/391]\tTime 0.062 (0.049)\tData 0.030 (0.016)\tLoss 1.6540 (1.4793)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [340][300/391]\tTime 0.063 (0.049)\tData 0.031 (0.016)\tLoss 1.6697 (1.4745)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [340][350/391]\tTime 0.064 (0.049)\tData 0.033 (0.016)\tLoss 1.2933 (1.4704)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.096 (0.096)\tLoss 0.9562 (0.9562)\tPrec@1 84.375 (84.375)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.9261 (0.9440)\tPrec@1 83.594 (83.441)\n",
            " * Prec@1 83.550\n",
            "\n",
            "Epoch: 342/600\n",
            "Learning rate: 0.039409\n",
            "Epoch: [341][0/391]\tTime 0.199 (0.199)\tData 0.154 (0.154)\tLoss 1.2810 (1.2810)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [341][50/391]\tTime 0.064 (0.050)\tData 0.032 (0.018)\tLoss 1.0211 (1.4368)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [341][100/391]\tTime 0.060 (0.049)\tData 0.026 (0.017)\tLoss 1.6811 (1.4627)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [341][150/391]\tTime 0.066 (0.049)\tData 0.034 (0.017)\tLoss 1.3655 (1.4675)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [341][200/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.7698 (1.4613)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [341][250/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.1981 (1.4501)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [341][300/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.7517 (1.4572)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [341][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.3445 (1.4567)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.105 (0.105)\tLoss 0.9834 (0.9834)\tPrec@1 85.156 (85.156)\n",
            "Test: [50/79]\tTime 0.010 (0.016)\tLoss 0.9905 (0.9668)\tPrec@1 78.906 (82.843)\n",
            " * Prec@1 83.110\n",
            "\n",
            "Epoch: 343/600\n",
            "Learning rate: 0.039154\n",
            "Epoch: [342][0/391]\tTime 0.198 (0.198)\tData 0.152 (0.152)\tLoss 1.7595 (1.7595)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [342][50/391]\tTime 0.059 (0.050)\tData 0.026 (0.018)\tLoss 1.7542 (1.4763)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [342][100/391]\tTime 0.058 (0.049)\tData 0.026 (0.017)\tLoss 1.7493 (1.4713)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [342][150/391]\tTime 0.067 (0.049)\tData 0.033 (0.017)\tLoss 1.6458 (1.4493)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [342][200/391]\tTime 0.066 (0.049)\tData 0.034 (0.017)\tLoss 1.7473 (1.4614)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [342][250/391]\tTime 0.067 (0.049)\tData 0.035 (0.017)\tLoss 1.3176 (1.4727)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [342][300/391]\tTime 0.067 (0.049)\tData 0.035 (0.017)\tLoss 1.2388 (1.4738)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [342][350/391]\tTime 0.066 (0.049)\tData 0.035 (0.017)\tLoss 1.0588 (1.4712)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.096 (0.096)\tLoss 0.9068 (0.9068)\tPrec@1 86.719 (86.719)\n",
            "Test: [50/79]\tTime 0.017 (0.016)\tLoss 0.9553 (0.8917)\tPrec@1 82.031 (87.546)\n",
            " * Prec@1 87.580\n",
            "\n",
            "Epoch: 344/600\n",
            "Learning rate: 0.038899\n",
            "Epoch: [343][0/391]\tTime 0.200 (0.200)\tData 0.154 (0.154)\tLoss 1.6046 (1.6046)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [343][50/391]\tTime 0.065 (0.051)\tData 0.034 (0.019)\tLoss 1.7381 (1.4455)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [343][100/391]\tTime 0.063 (0.050)\tData 0.031 (0.018)\tLoss 1.6651 (1.4301)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [343][150/391]\tTime 0.065 (0.049)\tData 0.034 (0.017)\tLoss 1.0625 (1.4394)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [343][200/391]\tTime 0.059 (0.050)\tData 0.026 (0.017)\tLoss 1.0835 (1.4350)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [343][250/391]\tTime 0.065 (0.049)\tData 0.032 (0.017)\tLoss 1.6632 (1.4505)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [343][300/391]\tTime 0.052 (0.049)\tData 0.021 (0.017)\tLoss 1.5440 (1.4519)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [343][350/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 1.6273 (1.4502)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.102 (0.102)\tLoss 1.0407 (1.0407)\tPrec@1 77.344 (77.344)\n",
            "Test: [50/79]\tTime 0.014 (0.016)\tLoss 1.0697 (0.9947)\tPrec@1 75.000 (81.694)\n",
            " * Prec@1 81.610\n",
            "\n",
            "Epoch: 345/600\n",
            "Learning rate: 0.038644\n",
            "Epoch: [344][0/391]\tTime 0.206 (0.206)\tData 0.154 (0.154)\tLoss 0.9331 (0.9331)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [344][50/391]\tTime 0.064 (0.052)\tData 0.032 (0.018)\tLoss 1.0494 (1.4407)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [344][100/391]\tTime 0.063 (0.050)\tData 0.031 (0.017)\tLoss 1.4798 (1.4556)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [344][150/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.6649 (1.4531)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [344][200/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.7876 (1.4543)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [344][250/391]\tTime 0.063 (0.049)\tData 0.031 (0.016)\tLoss 1.2422 (1.4565)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [344][300/391]\tTime 0.064 (0.049)\tData 0.032 (0.016)\tLoss 1.3990 (1.4514)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [344][350/391]\tTime 0.065 (0.049)\tData 0.031 (0.016)\tLoss 1.2647 (1.4549)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 0.8897 (0.8897)\tPrec@1 87.500 (87.500)\n",
            "Test: [50/79]\tTime 0.011 (0.016)\tLoss 0.9203 (0.9011)\tPrec@1 85.938 (86.504)\n",
            " * Prec@1 86.510\n",
            "\n",
            "Epoch: 346/600\n",
            "Learning rate: 0.038389\n",
            "Epoch: [345][0/391]\tTime 0.209 (0.209)\tData 0.152 (0.152)\tLoss 0.9180 (0.9180)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [345][50/391]\tTime 0.033 (0.053)\tData 0.000 (0.017)\tLoss 1.2715 (1.4725)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [345][100/391]\tTime 0.034 (0.051)\tData 0.000 (0.017)\tLoss 1.5009 (1.4625)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [345][150/391]\tTime 0.033 (0.050)\tData 0.000 (0.017)\tLoss 1.7109 (1.4785)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [345][200/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.5712 (1.4736)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [345][250/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.2369 (1.4752)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [345][300/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.6831 (1.4814)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [345][350/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.3586 (1.4884)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.9130 (0.9130)\tPrec@1 84.375 (84.375)\n",
            "Test: [50/79]\tTime 0.020 (0.016)\tLoss 0.9426 (0.9241)\tPrec@1 82.031 (84.651)\n",
            " * Prec@1 84.840\n",
            "\n",
            "Epoch: 347/600\n",
            "Learning rate: 0.038135\n",
            "Epoch: [346][0/391]\tTime 0.199 (0.199)\tData 0.153 (0.153)\tLoss 1.6498 (1.6498)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [346][50/391]\tTime 0.042 (0.051)\tData 0.010 (0.019)\tLoss 0.8711 (1.4838)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [346][100/391]\tTime 0.053 (0.050)\tData 0.021 (0.018)\tLoss 1.6868 (1.4952)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [346][150/391]\tTime 0.066 (0.050)\tData 0.033 (0.017)\tLoss 1.7003 (1.4741)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [346][200/391]\tTime 0.064 (0.049)\tData 0.031 (0.017)\tLoss 1.6463 (1.4664)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [346][250/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.7022 (1.4751)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [346][300/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.4588 (1.4783)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [346][350/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 1.7382 (1.4816)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.094 (0.094)\tLoss 0.9359 (0.9359)\tPrec@1 86.719 (86.719)\n",
            "Test: [50/79]\tTime 0.010 (0.016)\tLoss 0.8910 (0.9075)\tPrec@1 89.844 (86.780)\n",
            " * Prec@1 87.160\n",
            "\n",
            "Epoch: 348/600\n",
            "Learning rate: 0.037881\n",
            "Epoch: [347][0/391]\tTime 0.197 (0.197)\tData 0.152 (0.152)\tLoss 1.7220 (1.7220)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [347][50/391]\tTime 0.066 (0.052)\tData 0.035 (0.020)\tLoss 0.9830 (1.4166)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [347][100/391]\tTime 0.064 (0.050)\tData 0.033 (0.018)\tLoss 1.7470 (1.4517)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [347][150/391]\tTime 0.041 (0.050)\tData 0.009 (0.018)\tLoss 1.5218 (1.4460)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [347][200/391]\tTime 0.061 (0.049)\tData 0.029 (0.018)\tLoss 1.0011 (1.4438)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [347][250/391]\tTime 0.067 (0.049)\tData 0.035 (0.018)\tLoss 1.7569 (1.4422)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [347][300/391]\tTime 0.065 (0.049)\tData 0.033 (0.018)\tLoss 0.9718 (1.4319)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [347][350/391]\tTime 0.067 (0.049)\tData 0.035 (0.017)\tLoss 1.5262 (1.4439)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 1.0100 (1.0100)\tPrec@1 78.906 (78.906)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 1.0621 (1.0010)\tPrec@1 78.125 (80.576)\n",
            " * Prec@1 80.700\n",
            "\n",
            "Epoch: 349/600\n",
            "Learning rate: 0.037628\n",
            "Epoch: [348][0/391]\tTime 0.204 (0.204)\tData 0.159 (0.159)\tLoss 1.7304 (1.7304)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [348][50/391]\tTime 0.067 (0.052)\tData 0.036 (0.020)\tLoss 1.7902 (1.4988)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [348][100/391]\tTime 0.067 (0.051)\tData 0.036 (0.019)\tLoss 1.5588 (1.4658)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [348][150/391]\tTime 0.067 (0.050)\tData 0.035 (0.018)\tLoss 1.3550 (1.4516)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [348][200/391]\tTime 0.065 (0.050)\tData 0.033 (0.018)\tLoss 1.5867 (1.4453)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [348][250/391]\tTime 0.068 (0.050)\tData 0.036 (0.018)\tLoss 1.0365 (1.4429)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [348][300/391]\tTime 0.064 (0.050)\tData 0.033 (0.018)\tLoss 1.4765 (1.4432)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [348][350/391]\tTime 0.051 (0.050)\tData 0.020 (0.018)\tLoss 1.7174 (1.4500)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 0.9284 (0.9284)\tPrec@1 86.719 (86.719)\n",
            "Test: [50/79]\tTime 0.017 (0.017)\tLoss 0.8975 (0.8979)\tPrec@1 88.281 (87.255)\n",
            " * Prec@1 87.410\n",
            "\n",
            "Epoch: 350/600\n",
            "Learning rate: 0.037375\n",
            "Epoch: [349][0/391]\tTime 0.202 (0.202)\tData 0.155 (0.155)\tLoss 1.6234 (1.6234)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [349][50/391]\tTime 0.064 (0.051)\tData 0.031 (0.018)\tLoss 1.6207 (1.4518)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [349][100/391]\tTime 0.065 (0.050)\tData 0.033 (0.017)\tLoss 1.4739 (1.4700)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [349][150/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.6720 (1.4534)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [349][200/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.6488 (1.4703)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [349][250/391]\tTime 0.066 (0.049)\tData 0.034 (0.017)\tLoss 1.7877 (1.4690)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [349][300/391]\tTime 0.065 (0.049)\tData 0.032 (0.016)\tLoss 1.7626 (1.4731)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [349][350/391]\tTime 0.065 (0.049)\tData 0.032 (0.016)\tLoss 1.7582 (1.4746)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.8792 (0.8792)\tPrec@1 89.062 (89.062)\n",
            "Test: [50/79]\tTime 0.016 (0.016)\tLoss 0.8726 (0.8790)\tPrec@1 88.281 (88.511)\n",
            " * Prec@1 88.440\n",
            "\n",
            "Epoch: 351/600\n",
            "Learning rate: 0.037122\n",
            "Epoch: [350][0/391]\tTime 0.207 (0.207)\tData 0.161 (0.161)\tLoss 1.6857 (1.6857)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [350][50/391]\tTime 0.070 (0.053)\tData 0.036 (0.020)\tLoss 1.0341 (1.4430)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [350][100/391]\tTime 0.064 (0.051)\tData 0.032 (0.019)\tLoss 1.3511 (1.4453)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [350][150/391]\tTime 0.065 (0.051)\tData 0.033 (0.018)\tLoss 1.7606 (1.4468)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [350][200/391]\tTime 0.064 (0.051)\tData 0.032 (0.018)\tLoss 1.3466 (1.4531)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [350][250/391]\tTime 0.071 (0.050)\tData 0.039 (0.018)\tLoss 1.7758 (1.4486)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [350][300/391]\tTime 0.070 (0.050)\tData 0.038 (0.018)\tLoss 1.7488 (1.4560)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [350][350/391]\tTime 0.065 (0.050)\tData 0.033 (0.018)\tLoss 1.3081 (1.4509)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.094 (0.094)\tLoss 0.8920 (0.8920)\tPrec@1 89.844 (89.844)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.9697 (0.9225)\tPrec@1 84.375 (86.213)\n",
            " * Prec@1 86.440\n",
            "\n",
            "Epoch: 352/600\n",
            "Learning rate: 0.036870\n",
            "Epoch: [351][0/391]\tTime 0.207 (0.207)\tData 0.157 (0.157)\tLoss 1.6783 (1.6783)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [351][50/391]\tTime 0.032 (0.051)\tData 0.000 (0.019)\tLoss 1.7427 (1.3603)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [351][100/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.6838 (1.4283)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [351][150/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.8046 (1.4212)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [351][200/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.6787 (1.4393)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [351][250/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.6811 (1.4515)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [351][300/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.1612 (1.4513)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [351][350/391]\tTime 0.043 (0.050)\tData 0.000 (0.017)\tLoss 1.7364 (1.4496)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.107 (0.107)\tLoss 0.8784 (0.8784)\tPrec@1 90.625 (90.625)\n",
            "Test: [50/79]\tTime 0.020 (0.017)\tLoss 0.9237 (0.9162)\tPrec@1 85.156 (86.566)\n",
            " * Prec@1 86.620\n",
            "\n",
            "Epoch: 353/600\n",
            "Learning rate: 0.036617\n",
            "Epoch: [352][0/391]\tTime 0.198 (0.198)\tData 0.153 (0.153)\tLoss 1.6427 (1.6427)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [352][50/391]\tTime 0.063 (0.051)\tData 0.030 (0.018)\tLoss 1.2569 (1.4852)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [352][100/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.1789 (1.4429)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [352][150/391]\tTime 0.033 (0.049)\tData 0.000 (0.017)\tLoss 1.0455 (1.4537)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [352][200/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 1.2750 (1.4545)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [352][250/391]\tTime 0.033 (0.049)\tData 0.000 (0.017)\tLoss 1.6576 (1.4758)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [352][300/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 0.9995 (1.4763)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [352][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.6877 (1.4625)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.094 (0.094)\tLoss 0.9426 (0.9426)\tPrec@1 85.156 (85.156)\n",
            "Test: [50/79]\tTime 0.011 (0.016)\tLoss 0.9136 (0.8972)\tPrec@1 87.500 (86.964)\n",
            " * Prec@1 87.120\n",
            "\n",
            "Epoch: 354/600\n",
            "Learning rate: 0.036366\n",
            "Epoch: [353][0/391]\tTime 0.199 (0.199)\tData 0.153 (0.153)\tLoss 1.2939 (1.2939)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [353][50/391]\tTime 0.032 (0.051)\tData 0.000 (0.019)\tLoss 1.0244 (1.4107)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [353][100/391]\tTime 0.031 (0.050)\tData 0.000 (0.018)\tLoss 1.4071 (1.4590)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [353][150/391]\tTime 0.036 (0.050)\tData 0.000 (0.018)\tLoss 1.6521 (1.4716)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [353][200/391]\tTime 0.033 (0.050)\tData 0.000 (0.017)\tLoss 0.9122 (1.4848)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [353][250/391]\tTime 0.033 (0.050)\tData 0.000 (0.018)\tLoss 1.7832 (1.4736)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [353][300/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.7741 (1.4801)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [353][350/391]\tTime 0.033 (0.049)\tData 0.000 (0.017)\tLoss 1.0082 (1.4714)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.101 (0.101)\tLoss 0.9709 (0.9709)\tPrec@1 82.812 (82.812)\n",
            "Test: [50/79]\tTime 0.026 (0.017)\tLoss 0.9079 (0.9098)\tPrec@1 87.500 (86.734)\n",
            " * Prec@1 86.500\n",
            "\n",
            "Epoch: 355/600\n",
            "Learning rate: 0.036114\n",
            "Epoch: [354][0/391]\tTime 0.204 (0.204)\tData 0.160 (0.160)\tLoss 1.5505 (1.5505)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [354][50/391]\tTime 0.063 (0.051)\tData 0.032 (0.019)\tLoss 1.6388 (1.4227)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [354][100/391]\tTime 0.065 (0.050)\tData 0.034 (0.018)\tLoss 1.5597 (1.4389)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [354][150/391]\tTime 0.063 (0.049)\tData 0.032 (0.017)\tLoss 1.7239 (1.4635)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [354][200/391]\tTime 0.033 (0.049)\tData 0.000 (0.016)\tLoss 1.7933 (1.4795)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [354][250/391]\tTime 0.045 (0.049)\tData 0.013 (0.016)\tLoss 1.5901 (1.4769)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [354][300/391]\tTime 0.065 (0.049)\tData 0.033 (0.016)\tLoss 1.2992 (1.4752)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [354][350/391]\tTime 0.065 (0.049)\tData 0.033 (0.016)\tLoss 1.6927 (1.4779)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.094 (0.094)\tLoss 0.8984 (0.8984)\tPrec@1 87.500 (87.500)\n",
            "Test: [50/79]\tTime 0.020 (0.016)\tLoss 0.9585 (0.9170)\tPrec@1 80.469 (85.585)\n",
            " * Prec@1 85.680\n",
            "\n",
            "Epoch: 356/600\n",
            "Learning rate: 0.035863\n",
            "Epoch: [355][0/391]\tTime 0.213 (0.213)\tData 0.153 (0.153)\tLoss 1.5556 (1.5556)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [355][50/391]\tTime 0.065 (0.052)\tData 0.032 (0.019)\tLoss 1.7311 (1.4446)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [355][100/391]\tTime 0.064 (0.050)\tData 0.032 (0.018)\tLoss 1.5479 (1.4576)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [355][150/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.5789 (1.4652)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [355][200/391]\tTime 0.065 (0.049)\tData 0.032 (0.017)\tLoss 1.6760 (1.4578)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [355][250/391]\tTime 0.058 (0.049)\tData 0.024 (0.017)\tLoss 1.4067 (1.4713)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [355][300/391]\tTime 0.063 (0.049)\tData 0.031 (0.016)\tLoss 1.0113 (1.4670)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [355][350/391]\tTime 0.063 (0.049)\tData 0.032 (0.016)\tLoss 1.6551 (1.4746)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 0.9170 (0.9170)\tPrec@1 85.938 (85.938)\n",
            "Test: [50/79]\tTime 0.010 (0.016)\tLoss 0.9041 (0.8967)\tPrec@1 85.156 (86.045)\n",
            " * Prec@1 86.500\n",
            "\n",
            "Epoch: 357/600\n",
            "Learning rate: 0.035613\n",
            "Epoch: [356][0/391]\tTime 0.197 (0.197)\tData 0.152 (0.152)\tLoss 1.5361 (1.5361)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [356][50/391]\tTime 0.062 (0.051)\tData 0.030 (0.018)\tLoss 1.5452 (1.4703)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [356][100/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.7882 (1.4225)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [356][150/391]\tTime 0.060 (0.049)\tData 0.028 (0.017)\tLoss 1.7278 (1.4496)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [356][200/391]\tTime 0.044 (0.049)\tData 0.012 (0.017)\tLoss 1.0703 (1.4532)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [356][250/391]\tTime 0.035 (0.049)\tData 0.003 (0.017)\tLoss 0.9944 (1.4389)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [356][300/391]\tTime 0.042 (0.049)\tData 0.010 (0.017)\tLoss 1.1428 (1.4385)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [356][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.7061 (1.4539)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 0.9191 (0.9191)\tPrec@1 84.375 (84.375)\n",
            "Test: [50/79]\tTime 0.020 (0.016)\tLoss 0.8894 (0.8928)\tPrec@1 85.938 (86.213)\n",
            " * Prec@1 86.310\n",
            "\n",
            "Epoch: 358/600\n",
            "Learning rate: 0.035363\n",
            "Epoch: [357][0/391]\tTime 0.213 (0.213)\tData 0.154 (0.154)\tLoss 1.2859 (1.2859)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [357][50/391]\tTime 0.067 (0.052)\tData 0.035 (0.020)\tLoss 1.7178 (1.3895)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [357][100/391]\tTime 0.065 (0.050)\tData 0.033 (0.018)\tLoss 1.6685 (1.4549)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [357][150/391]\tTime 0.064 (0.050)\tData 0.032 (0.017)\tLoss 0.9302 (1.4558)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [357][200/391]\tTime 0.065 (0.050)\tData 0.034 (0.017)\tLoss 1.6222 (1.4666)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [357][250/391]\tTime 0.062 (0.050)\tData 0.030 (0.017)\tLoss 1.2149 (1.4714)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [357][300/391]\tTime 0.063 (0.049)\tData 0.031 (0.017)\tLoss 1.6492 (1.4760)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [357][350/391]\tTime 0.064 (0.049)\tData 0.031 (0.017)\tLoss 1.6123 (1.4783)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.9281 (0.9281)\tPrec@1 82.812 (82.812)\n",
            "Test: [50/79]\tTime 0.019 (0.016)\tLoss 0.9633 (0.9317)\tPrec@1 81.250 (84.605)\n",
            " * Prec@1 84.880\n",
            "\n",
            "Epoch: 359/600\n",
            "Learning rate: 0.035113\n",
            "Epoch: [358][0/391]\tTime 0.212 (0.212)\tData 0.155 (0.155)\tLoss 1.6164 (1.6164)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [358][50/391]\tTime 0.065 (0.051)\tData 0.033 (0.018)\tLoss 1.0746 (1.4281)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [358][100/391]\tTime 0.066 (0.050)\tData 0.033 (0.018)\tLoss 1.4533 (1.4322)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [358][150/391]\tTime 0.063 (0.049)\tData 0.030 (0.017)\tLoss 1.5179 (1.4637)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [358][200/391]\tTime 0.067 (0.049)\tData 0.033 (0.017)\tLoss 1.1910 (1.4795)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [358][250/391]\tTime 0.066 (0.049)\tData 0.033 (0.017)\tLoss 0.9475 (1.4821)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [358][300/391]\tTime 0.062 (0.049)\tData 0.029 (0.016)\tLoss 1.5232 (1.4824)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [358][350/391]\tTime 0.063 (0.049)\tData 0.031 (0.016)\tLoss 1.6211 (1.4783)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.093 (0.093)\tLoss 0.9188 (0.9188)\tPrec@1 86.719 (86.719)\n",
            "Test: [50/79]\tTime 0.013 (0.016)\tLoss 0.9950 (0.9591)\tPrec@1 78.906 (83.502)\n",
            " * Prec@1 83.640\n",
            "\n",
            "Epoch: 360/600\n",
            "Learning rate: 0.034864\n",
            "Epoch: [359][0/391]\tTime 0.210 (0.210)\tData 0.164 (0.164)\tLoss 1.1351 (1.1351)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [359][50/391]\tTime 0.069 (0.053)\tData 0.037 (0.019)\tLoss 1.3987 (1.4611)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [359][100/391]\tTime 0.063 (0.051)\tData 0.032 (0.018)\tLoss 1.6418 (1.4824)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [359][150/391]\tTime 0.063 (0.050)\tData 0.031 (0.017)\tLoss 1.1909 (1.4727)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [359][200/391]\tTime 0.066 (0.050)\tData 0.034 (0.017)\tLoss 1.5112 (1.4568)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [359][250/391]\tTime 0.038 (0.049)\tData 0.007 (0.016)\tLoss 1.5727 (1.4541)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [359][300/391]\tTime 0.049 (0.049)\tData 0.018 (0.016)\tLoss 1.0790 (1.4533)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [359][350/391]\tTime 0.059 (0.049)\tData 0.027 (0.016)\tLoss 1.3588 (1.4490)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 0.9431 (0.9431)\tPrec@1 85.938 (85.938)\n",
            "Test: [50/79]\tTime 0.015 (0.016)\tLoss 0.9418 (0.9375)\tPrec@1 85.156 (85.524)\n",
            " * Prec@1 85.940\n",
            "\n",
            "Epoch: 361/600\n",
            "Learning rate: 0.034615\n",
            "Epoch: [360][0/391]\tTime 0.214 (0.214)\tData 0.155 (0.155)\tLoss 1.4839 (1.4839)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [360][50/391]\tTime 0.060 (0.052)\tData 0.027 (0.018)\tLoss 1.5540 (1.4728)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [360][100/391]\tTime 0.068 (0.051)\tData 0.036 (0.018)\tLoss 1.7159 (1.4753)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [360][150/391]\tTime 0.064 (0.050)\tData 0.032 (0.017)\tLoss 1.6407 (1.4818)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [360][200/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.7933 (1.4924)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [360][250/391]\tTime 0.031 (0.050)\tData 0.000 (0.017)\tLoss 1.5974 (1.4943)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [360][300/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 1.3050 (1.4901)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [360][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.6823 (1.4873)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.096 (0.096)\tLoss 0.8957 (0.8957)\tPrec@1 88.281 (88.281)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.8728 (0.8757)\tPrec@1 88.281 (87.806)\n",
            " * Prec@1 88.030\n",
            "\n",
            "Epoch: 362/600\n",
            "Learning rate: 0.034366\n",
            "Epoch: [361][0/391]\tTime 0.202 (0.202)\tData 0.155 (0.155)\tLoss 1.7284 (1.7284)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [361][50/391]\tTime 0.065 (0.051)\tData 0.033 (0.018)\tLoss 1.6121 (1.4955)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [361][100/391]\tTime 0.065 (0.050)\tData 0.032 (0.017)\tLoss 1.7820 (1.4869)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [361][150/391]\tTime 0.038 (0.050)\tData 0.006 (0.017)\tLoss 1.7651 (1.4772)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [361][200/391]\tTime 0.031 (0.050)\tData 0.000 (0.017)\tLoss 1.0921 (1.4858)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [361][250/391]\tTime 0.042 (0.049)\tData 0.011 (0.017)\tLoss 1.1459 (1.4842)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [361][300/391]\tTime 0.063 (0.049)\tData 0.032 (0.017)\tLoss 1.4693 (1.4748)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [361][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.0838 (1.4750)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.094 (0.094)\tLoss 0.9461 (0.9461)\tPrec@1 85.156 (85.156)\n",
            "Test: [50/79]\tTime 0.023 (0.017)\tLoss 0.9368 (0.9130)\tPrec@1 82.031 (85.003)\n",
            " * Prec@1 85.470\n",
            "\n",
            "Epoch: 363/600\n",
            "Learning rate: 0.034118\n",
            "Epoch: [362][0/391]\tTime 0.199 (0.199)\tData 0.153 (0.153)\tLoss 1.6031 (1.6031)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [362][50/391]\tTime 0.032 (0.053)\tData 0.000 (0.018)\tLoss 1.7156 (1.4530)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [362][100/391]\tTime 0.032 (0.051)\tData 0.000 (0.018)\tLoss 1.2179 (1.4634)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [362][150/391]\tTime 0.032 (0.051)\tData 0.000 (0.017)\tLoss 1.4721 (1.4676)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [362][200/391]\tTime 0.031 (0.050)\tData 0.000 (0.017)\tLoss 1.1643 (1.4564)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [362][250/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.1489 (1.4607)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [362][300/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.3758 (1.4676)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [362][350/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 1.6803 (1.4713)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.096 (0.096)\tLoss 0.8827 (0.8827)\tPrec@1 86.719 (86.719)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.9052 (0.8936)\tPrec@1 86.719 (86.642)\n",
            " * Prec@1 86.700\n",
            "\n",
            "Epoch: 364/600\n",
            "Learning rate: 0.033870\n",
            "Epoch: [363][0/391]\tTime 0.201 (0.201)\tData 0.154 (0.154)\tLoss 1.0632 (1.0632)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [363][50/391]\tTime 0.065 (0.051)\tData 0.034 (0.019)\tLoss 1.3862 (1.4303)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [363][100/391]\tTime 0.068 (0.050)\tData 0.036 (0.018)\tLoss 1.7999 (1.4455)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [363][150/391]\tTime 0.066 (0.049)\tData 0.033 (0.017)\tLoss 1.5677 (1.4609)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [363][200/391]\tTime 0.066 (0.049)\tData 0.034 (0.017)\tLoss 1.6377 (1.4729)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [363][250/391]\tTime 0.065 (0.049)\tData 0.034 (0.017)\tLoss 1.6488 (1.4650)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [363][300/391]\tTime 0.063 (0.049)\tData 0.031 (0.017)\tLoss 1.4966 (1.4568)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [363][350/391]\tTime 0.033 (0.049)\tData 0.001 (0.017)\tLoss 0.9452 (1.4533)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.098 (0.098)\tLoss 0.9244 (0.9244)\tPrec@1 86.719 (86.719)\n",
            "Test: [50/79]\tTime 0.015 (0.016)\tLoss 0.9361 (0.9436)\tPrec@1 82.812 (85.294)\n",
            " * Prec@1 85.340\n",
            "\n",
            "Epoch: 365/600\n",
            "Learning rate: 0.033623\n",
            "Epoch: [364][0/391]\tTime 0.208 (0.208)\tData 0.161 (0.161)\tLoss 1.5598 (1.5598)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [364][50/391]\tTime 0.064 (0.052)\tData 0.031 (0.019)\tLoss 1.1665 (1.4800)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [364][100/391]\tTime 0.046 (0.050)\tData 0.014 (0.017)\tLoss 1.4421 (1.4648)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [364][150/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.7836 (1.4535)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [364][200/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.6962 (1.4707)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [364][250/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.1413 (1.4648)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [364][300/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.7003 (1.4592)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [364][350/391]\tTime 0.034 (0.050)\tData 0.000 (0.017)\tLoss 1.5777 (1.4590)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.8972 (0.8972)\tPrec@1 88.281 (88.281)\n",
            "Test: [50/79]\tTime 0.017 (0.016)\tLoss 0.8855 (0.9052)\tPrec@1 85.156 (86.765)\n",
            " * Prec@1 86.960\n",
            "\n",
            "Epoch: 366/600\n",
            "Learning rate: 0.033376\n",
            "Epoch: [365][0/391]\tTime 0.200 (0.200)\tData 0.152 (0.152)\tLoss 1.5539 (1.5539)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [365][50/391]\tTime 0.075 (0.052)\tData 0.041 (0.018)\tLoss 1.1626 (1.4736)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [365][100/391]\tTime 0.062 (0.050)\tData 0.030 (0.017)\tLoss 1.2095 (1.4628)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [365][150/391]\tTime 0.063 (0.050)\tData 0.032 (0.017)\tLoss 1.4282 (1.4592)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [365][200/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.5569 (1.4590)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [365][250/391]\tTime 0.039 (0.049)\tData 0.008 (0.017)\tLoss 1.6197 (1.4593)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [365][300/391]\tTime 0.046 (0.049)\tData 0.015 (0.016)\tLoss 1.6034 (1.4607)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [365][350/391]\tTime 0.044 (0.049)\tData 0.012 (0.016)\tLoss 1.6771 (1.4673)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.096 (0.096)\tLoss 0.9295 (0.9295)\tPrec@1 84.375 (84.375)\n",
            "Test: [50/79]\tTime 0.014 (0.016)\tLoss 0.9254 (0.9126)\tPrec@1 83.594 (85.585)\n",
            " * Prec@1 85.900\n",
            "\n",
            "Epoch: 367/600\n",
            "Learning rate: 0.033130\n",
            "Epoch: [366][0/391]\tTime 0.198 (0.198)\tData 0.153 (0.153)\tLoss 1.7844 (1.7844)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [366][50/391]\tTime 0.050 (0.051)\tData 0.018 (0.019)\tLoss 0.9718 (1.4594)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [366][100/391]\tTime 0.077 (0.050)\tData 0.033 (0.018)\tLoss 0.8959 (1.4709)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [366][150/391]\tTime 0.032 (0.051)\tData 0.000 (0.016)\tLoss 1.1783 (1.4698)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [366][200/391]\tTime 0.032 (0.051)\tData 0.000 (0.016)\tLoss 1.2373 (1.4503)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [366][250/391]\tTime 0.032 (0.051)\tData 0.000 (0.016)\tLoss 1.7282 (1.4450)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [366][300/391]\tTime 0.032 (0.050)\tData 0.000 (0.016)\tLoss 1.3980 (1.4548)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [366][350/391]\tTime 0.067 (0.050)\tData 0.033 (0.016)\tLoss 1.2371 (1.4470)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 0.9221 (0.9221)\tPrec@1 89.062 (89.062)\n",
            "Test: [50/79]\tTime 0.011 (0.016)\tLoss 0.9358 (0.9286)\tPrec@1 85.156 (86.949)\n",
            " * Prec@1 87.160\n",
            "\n",
            "Epoch: 368/600\n",
            "Learning rate: 0.032884\n",
            "Epoch: [367][0/391]\tTime 0.204 (0.204)\tData 0.157 (0.157)\tLoss 1.4148 (1.4148)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [367][50/391]\tTime 0.062 (0.051)\tData 0.030 (0.018)\tLoss 1.6606 (1.5060)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [367][100/391]\tTime 0.053 (0.049)\tData 0.021 (0.017)\tLoss 1.4394 (1.5267)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [367][150/391]\tTime 0.055 (0.049)\tData 0.023 (0.017)\tLoss 1.6340 (1.4926)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [367][200/391]\tTime 0.063 (0.049)\tData 0.030 (0.017)\tLoss 1.3311 (1.4702)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [367][250/391]\tTime 0.066 (0.049)\tData 0.033 (0.017)\tLoss 0.9101 (1.4615)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [367][300/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 0.9361 (1.4575)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [367][350/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.7640 (1.4643)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 0.8440 (0.8440)\tPrec@1 93.750 (93.750)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.8512 (0.8734)\tPrec@1 90.625 (88.312)\n",
            " * Prec@1 88.570\n",
            "\n",
            "Epoch: 369/600\n",
            "Learning rate: 0.032639\n",
            "Epoch: [368][0/391]\tTime 0.203 (0.203)\tData 0.156 (0.156)\tLoss 1.0432 (1.0432)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [368][50/391]\tTime 0.066 (0.051)\tData 0.034 (0.018)\tLoss 1.6561 (1.4541)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [368][100/391]\tTime 0.065 (0.050)\tData 0.033 (0.017)\tLoss 1.5555 (1.4465)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [368][150/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.5624 (1.4642)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [368][200/391]\tTime 0.065 (0.049)\tData 0.033 (0.016)\tLoss 1.4211 (1.4707)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [368][250/391]\tTime 0.064 (0.049)\tData 0.032 (0.016)\tLoss 1.2328 (1.4674)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [368][300/391]\tTime 0.063 (0.049)\tData 0.031 (0.016)\tLoss 1.6543 (1.4577)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [368][350/391]\tTime 0.061 (0.048)\tData 0.029 (0.016)\tLoss 1.4380 (1.4584)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.096 (0.096)\tLoss 0.8851 (0.8851)\tPrec@1 88.281 (88.281)\n",
            "Test: [50/79]\tTime 0.019 (0.016)\tLoss 0.8514 (0.8802)\tPrec@1 89.062 (87.592)\n",
            " * Prec@1 87.880\n",
            "\n",
            "Epoch: 370/600\n",
            "Learning rate: 0.032394\n",
            "Epoch: [369][0/391]\tTime 0.201 (0.201)\tData 0.155 (0.155)\tLoss 1.3679 (1.3679)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [369][50/391]\tTime 0.063 (0.052)\tData 0.032 (0.019)\tLoss 1.6545 (1.4387)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [369][100/391]\tTime 0.064 (0.050)\tData 0.032 (0.017)\tLoss 1.2820 (1.4570)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [369][150/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.6662 (1.4375)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [369][200/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.6939 (1.4373)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [369][250/391]\tTime 0.061 (0.049)\tData 0.026 (0.016)\tLoss 0.9236 (1.4318)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [369][300/391]\tTime 0.064 (0.049)\tData 0.032 (0.016)\tLoss 1.0227 (1.4400)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [369][350/391]\tTime 0.063 (0.049)\tData 0.031 (0.016)\tLoss 1.5644 (1.4445)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.096 (0.096)\tLoss 0.8867 (0.8867)\tPrec@1 85.938 (85.938)\n",
            "Test: [50/79]\tTime 0.010 (0.016)\tLoss 0.9322 (0.8898)\tPrec@1 84.375 (87.806)\n",
            " * Prec@1 87.760\n",
            "\n",
            "Epoch: 371/600\n",
            "Learning rate: 0.032150\n",
            "Epoch: [370][0/391]\tTime 0.211 (0.211)\tData 0.164 (0.164)\tLoss 1.6595 (1.6595)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [370][50/391]\tTime 0.061 (0.051)\tData 0.029 (0.019)\tLoss 1.7038 (1.4281)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [370][100/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.4662 (1.4204)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [370][150/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 1.5966 (1.4151)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [370][200/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.6367 (1.4121)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [370][250/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.0563 (1.4191)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [370][300/391]\tTime 0.040 (0.049)\tData 0.006 (0.016)\tLoss 1.7037 (1.4291)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [370][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.7487 (1.4345)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.9188 (0.9188)\tPrec@1 88.281 (88.281)\n",
            "Test: [50/79]\tTime 0.011 (0.016)\tLoss 0.8975 (0.9250)\tPrec@1 84.375 (86.657)\n",
            " * Prec@1 86.740\n",
            "\n",
            "Epoch: 372/600\n",
            "Learning rate: 0.031906\n",
            "Epoch: [371][0/391]\tTime 0.213 (0.213)\tData 0.157 (0.157)\tLoss 1.3013 (1.3013)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [371][50/391]\tTime 0.048 (0.051)\tData 0.016 (0.017)\tLoss 1.7445 (1.4495)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [371][100/391]\tTime 0.066 (0.050)\tData 0.033 (0.017)\tLoss 1.5168 (1.4351)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [371][150/391]\tTime 0.055 (0.049)\tData 0.023 (0.016)\tLoss 1.6745 (1.4463)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [371][200/391]\tTime 0.039 (0.049)\tData 0.007 (0.016)\tLoss 1.6922 (1.4514)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [371][250/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.4318 (1.4526)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [371][300/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.7232 (1.4545)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [371][350/391]\tTime 0.035 (0.049)\tData 0.000 (0.017)\tLoss 1.7299 (1.4547)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.8976 (0.8976)\tPrec@1 88.281 (88.281)\n",
            "Test: [50/79]\tTime 0.021 (0.017)\tLoss 0.9272 (0.9099)\tPrec@1 86.719 (85.800)\n",
            " * Prec@1 85.850\n",
            "\n",
            "Epoch: 373/600\n",
            "Learning rate: 0.031662\n",
            "Epoch: [372][0/391]\tTime 0.213 (0.213)\tData 0.154 (0.154)\tLoss 0.9844 (0.9844)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [372][50/391]\tTime 0.064 (0.052)\tData 0.032 (0.019)\tLoss 1.0449 (1.4291)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [372][100/391]\tTime 0.067 (0.050)\tData 0.034 (0.018)\tLoss 1.7453 (1.4146)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [372][150/391]\tTime 0.064 (0.051)\tData 0.031 (0.017)\tLoss 1.4294 (1.4320)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [372][200/391]\tTime 0.065 (0.050)\tData 0.032 (0.017)\tLoss 1.3053 (1.4437)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [372][250/391]\tTime 0.065 (0.050)\tData 0.034 (0.017)\tLoss 1.7622 (1.4405)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [372][300/391]\tTime 0.058 (0.050)\tData 0.026 (0.017)\tLoss 1.1477 (1.4398)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [372][350/391]\tTime 0.031 (0.050)\tData 0.000 (0.017)\tLoss 1.7928 (1.4414)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.099 (0.099)\tLoss 0.9104 (0.9104)\tPrec@1 89.062 (89.062)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.9593 (0.9204)\tPrec@1 87.500 (87.546)\n",
            " * Prec@1 87.730\n",
            "\n",
            "Epoch: 374/600\n",
            "Learning rate: 0.031419\n",
            "Epoch: [373][0/391]\tTime 0.210 (0.210)\tData 0.152 (0.152)\tLoss 1.4707 (1.4707)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [373][50/391]\tTime 0.044 (0.051)\tData 0.012 (0.017)\tLoss 1.4000 (1.4532)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [373][100/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.1056 (1.4595)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [373][150/391]\tTime 0.057 (0.049)\tData 0.025 (0.016)\tLoss 1.7258 (1.4473)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [373][200/391]\tTime 0.064 (0.049)\tData 0.031 (0.016)\tLoss 1.7603 (1.4322)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [373][250/391]\tTime 0.056 (0.049)\tData 0.024 (0.016)\tLoss 1.1865 (1.4174)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [373][300/391]\tTime 0.040 (0.049)\tData 0.008 (0.016)\tLoss 1.1080 (1.4214)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [373][350/391]\tTime 0.032 (0.048)\tData 0.000 (0.016)\tLoss 1.1321 (1.4238)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 0.8561 (0.8561)\tPrec@1 91.406 (91.406)\n",
            "Test: [50/79]\tTime 0.014 (0.016)\tLoss 0.8926 (0.8804)\tPrec@1 85.938 (86.581)\n",
            " * Prec@1 86.690\n",
            "\n",
            "Epoch: 375/600\n",
            "Learning rate: 0.031177\n",
            "Epoch: [374][0/391]\tTime 0.199 (0.199)\tData 0.154 (0.154)\tLoss 0.8947 (0.8947)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [374][50/391]\tTime 0.067 (0.053)\tData 0.035 (0.019)\tLoss 1.6642 (1.4759)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [374][100/391]\tTime 0.067 (0.051)\tData 0.034 (0.018)\tLoss 1.5783 (1.4556)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [374][150/391]\tTime 0.067 (0.051)\tData 0.035 (0.018)\tLoss 1.1332 (1.4638)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [374][200/391]\tTime 0.068 (0.051)\tData 0.036 (0.018)\tLoss 1.7571 (1.4549)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [374][250/391]\tTime 0.064 (0.051)\tData 0.030 (0.018)\tLoss 1.7024 (1.4521)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [374][300/391]\tTime 0.063 (0.050)\tData 0.031 (0.018)\tLoss 1.6912 (1.4571)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [374][350/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.6506 (1.4630)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 0.9084 (0.9084)\tPrec@1 87.500 (87.500)\n",
            "Test: [50/79]\tTime 0.011 (0.016)\tLoss 0.9465 (0.8909)\tPrec@1 82.812 (86.872)\n",
            " * Prec@1 86.910\n",
            "\n",
            "Epoch: 376/600\n",
            "Learning rate: 0.030935\n",
            "Epoch: [375][0/391]\tTime 0.202 (0.202)\tData 0.155 (0.155)\tLoss 1.4036 (1.4036)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [375][50/391]\tTime 0.060 (0.051)\tData 0.028 (0.018)\tLoss 1.3306 (1.4378)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [375][100/391]\tTime 0.065 (0.050)\tData 0.033 (0.017)\tLoss 1.6207 (1.4073)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [375][150/391]\tTime 0.069 (0.050)\tData 0.037 (0.017)\tLoss 1.7488 (1.4288)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [375][200/391]\tTime 0.068 (0.050)\tData 0.036 (0.017)\tLoss 1.0298 (1.4354)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [375][250/391]\tTime 0.067 (0.050)\tData 0.035 (0.017)\tLoss 0.9693 (1.4315)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [375][300/391]\tTime 0.067 (0.050)\tData 0.034 (0.017)\tLoss 1.6985 (1.4292)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [375][350/391]\tTime 0.072 (0.050)\tData 0.036 (0.017)\tLoss 1.3478 (1.4301)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 0.8908 (0.8908)\tPrec@1 87.500 (87.500)\n",
            "Test: [50/79]\tTime 0.011 (0.016)\tLoss 0.8850 (0.9039)\tPrec@1 86.719 (87.102)\n",
            " * Prec@1 87.190\n",
            "\n",
            "Epoch: 377/600\n",
            "Learning rate: 0.030694\n",
            "Epoch: [376][0/391]\tTime 0.203 (0.203)\tData 0.158 (0.158)\tLoss 0.9886 (0.9886)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [376][50/391]\tTime 0.065 (0.053)\tData 0.033 (0.021)\tLoss 1.5224 (1.4454)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [376][100/391]\tTime 0.066 (0.051)\tData 0.034 (0.019)\tLoss 1.0999 (1.4667)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [376][150/391]\tTime 0.066 (0.051)\tData 0.034 (0.018)\tLoss 1.3532 (1.4712)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [376][200/391]\tTime 0.069 (0.051)\tData 0.037 (0.018)\tLoss 1.7339 (1.4712)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [376][250/391]\tTime 0.068 (0.051)\tData 0.036 (0.018)\tLoss 1.0062 (1.4638)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [376][300/391]\tTime 0.070 (0.051)\tData 0.038 (0.018)\tLoss 1.7001 (1.4606)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [376][350/391]\tTime 0.069 (0.051)\tData 0.037 (0.018)\tLoss 0.8155 (1.4569)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.096 (0.096)\tLoss 0.8776 (0.8776)\tPrec@1 87.500 (87.500)\n",
            "Test: [50/79]\tTime 0.011 (0.016)\tLoss 0.9208 (0.8788)\tPrec@1 83.594 (87.730)\n",
            " * Prec@1 87.870\n",
            "\n",
            "Epoch: 378/600\n",
            "Learning rate: 0.030453\n",
            "Epoch: [377][0/391]\tTime 0.202 (0.202)\tData 0.155 (0.155)\tLoss 0.9491 (0.9491)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [377][50/391]\tTime 0.067 (0.052)\tData 0.035 (0.020)\tLoss 1.7192 (1.4314)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [377][100/391]\tTime 0.067 (0.051)\tData 0.035 (0.019)\tLoss 1.2583 (1.4224)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [377][150/391]\tTime 0.064 (0.050)\tData 0.031 (0.018)\tLoss 0.8555 (1.4180)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [377][200/391]\tTime 0.068 (0.050)\tData 0.037 (0.018)\tLoss 1.7136 (1.4237)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [377][250/391]\tTime 0.066 (0.050)\tData 0.034 (0.018)\tLoss 1.7062 (1.4291)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [377][300/391]\tTime 0.068 (0.050)\tData 0.036 (0.018)\tLoss 1.0182 (1.4290)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [377][350/391]\tTime 0.068 (0.050)\tData 0.036 (0.018)\tLoss 1.5748 (1.4341)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 0.9010 (0.9010)\tPrec@1 88.281 (88.281)\n",
            "Test: [50/79]\tTime 0.010 (0.016)\tLoss 0.9234 (0.9054)\tPrec@1 86.719 (86.535)\n",
            " * Prec@1 86.900\n",
            "\n",
            "Epoch: 379/600\n",
            "Learning rate: 0.030212\n",
            "Epoch: [378][0/391]\tTime 0.200 (0.200)\tData 0.154 (0.154)\tLoss 1.0666 (1.0666)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [378][50/391]\tTime 0.065 (0.051)\tData 0.034 (0.019)\tLoss 1.5057 (1.4189)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [378][100/391]\tTime 0.064 (0.050)\tData 0.032 (0.017)\tLoss 1.4375 (1.4285)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [378][150/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.2172 (1.4301)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [378][200/391]\tTime 0.066 (0.049)\tData 0.034 (0.017)\tLoss 1.7221 (1.4446)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [378][250/391]\tTime 0.062 (0.049)\tData 0.030 (0.017)\tLoss 1.0767 (1.4308)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [378][300/391]\tTime 0.052 (0.049)\tData 0.011 (0.016)\tLoss 0.9531 (1.4380)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [378][350/391]\tTime 0.033 (0.049)\tData 0.000 (0.016)\tLoss 1.7084 (1.4349)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.094 (0.094)\tLoss 0.8316 (0.8316)\tPrec@1 90.625 (90.625)\n",
            "Test: [50/79]\tTime 0.011 (0.017)\tLoss 0.8793 (0.8674)\tPrec@1 87.500 (87.929)\n",
            " * Prec@1 87.910\n",
            "\n",
            "Epoch: 380/600\n",
            "Learning rate: 0.029973\n",
            "Epoch: [379][0/391]\tTime 0.203 (0.203)\tData 0.155 (0.155)\tLoss 0.9902 (0.9902)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [379][50/391]\tTime 0.045 (0.052)\tData 0.013 (0.018)\tLoss 1.6335 (1.5161)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [379][100/391]\tTime 0.032 (0.051)\tData 0.000 (0.018)\tLoss 1.6765 (1.4651)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [379][150/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.7079 (1.4568)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [379][200/391]\tTime 0.031 (0.050)\tData 0.000 (0.018)\tLoss 0.9515 (1.4460)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [379][250/391]\tTime 0.034 (0.050)\tData 0.000 (0.018)\tLoss 1.2559 (1.4471)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [379][300/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.0153 (1.4521)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [379][350/391]\tTime 0.034 (0.050)\tData 0.000 (0.017)\tLoss 1.6693 (1.4567)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.105 (0.105)\tLoss 0.8674 (0.8674)\tPrec@1 89.062 (89.062)\n",
            "Test: [50/79]\tTime 0.010 (0.017)\tLoss 0.8653 (0.8479)\tPrec@1 89.062 (89.553)\n",
            " * Prec@1 89.390\n",
            "\n",
            "Epoch: 381/600\n",
            "Learning rate: 0.029734\n",
            "Epoch: [380][0/391]\tTime 0.203 (0.203)\tData 0.156 (0.156)\tLoss 1.7734 (1.7734)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [380][50/391]\tTime 0.069 (0.051)\tData 0.034 (0.018)\tLoss 1.0746 (1.4588)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [380][100/391]\tTime 0.042 (0.053)\tData 0.000 (0.016)\tLoss 1.5875 (1.4491)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [380][150/391]\tTime 0.033 (0.054)\tData 0.000 (0.016)\tLoss 1.2037 (1.4765)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [380][200/391]\tTime 0.043 (0.055)\tData 0.000 (0.016)\tLoss 1.7897 (1.4708)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [380][250/391]\tTime 0.033 (0.055)\tData 0.000 (0.016)\tLoss 1.3649 (1.4600)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [380][300/391]\tTime 0.032 (0.054)\tData 0.000 (0.016)\tLoss 1.6889 (1.4670)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [380][350/391]\tTime 0.033 (0.053)\tData 0.000 (0.016)\tLoss 1.1694 (1.4626)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 0.8722 (0.8722)\tPrec@1 89.062 (89.062)\n",
            "Test: [50/79]\tTime 0.019 (0.016)\tLoss 0.9463 (0.9292)\tPrec@1 82.812 (85.600)\n",
            " * Prec@1 85.450\n",
            "\n",
            "Epoch: 382/600\n",
            "Learning rate: 0.029495\n",
            "Epoch: [381][0/391]\tTime 0.201 (0.201)\tData 0.155 (0.155)\tLoss 1.0007 (1.0007)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [381][50/391]\tTime 0.068 (0.051)\tData 0.032 (0.017)\tLoss 1.7141 (1.4026)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [381][100/391]\tTime 0.065 (0.050)\tData 0.032 (0.016)\tLoss 1.7862 (1.4088)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [381][150/391]\tTime 0.063 (0.049)\tData 0.031 (0.016)\tLoss 1.4008 (1.4239)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [381][200/391]\tTime 0.065 (0.049)\tData 0.032 (0.016)\tLoss 1.5365 (1.4345)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [381][250/391]\tTime 0.064 (0.049)\tData 0.032 (0.016)\tLoss 1.7526 (1.4414)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [381][300/391]\tTime 0.034 (0.049)\tData 0.000 (0.016)\tLoss 1.7555 (1.4505)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [381][350/391]\tTime 0.039 (0.049)\tData 0.007 (0.016)\tLoss 1.6889 (1.4557)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 0.8674 (0.8674)\tPrec@1 89.844 (89.844)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.8914 (0.8755)\tPrec@1 89.062 (88.710)\n",
            " * Prec@1 88.820\n",
            "\n",
            "Epoch: 383/600\n",
            "Learning rate: 0.029257\n",
            "Epoch: [382][0/391]\tTime 0.201 (0.201)\tData 0.156 (0.156)\tLoss 0.9070 (0.9070)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [382][50/391]\tTime 0.063 (0.054)\tData 0.031 (0.019)\tLoss 1.6474 (1.3678)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [382][100/391]\tTime 0.044 (0.051)\tData 0.012 (0.018)\tLoss 1.7915 (1.4318)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [382][150/391]\tTime 0.034 (0.051)\tData 0.002 (0.017)\tLoss 0.9773 (1.4313)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [382][200/391]\tTime 0.038 (0.050)\tData 0.006 (0.017)\tLoss 1.6674 (1.4423)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [382][250/391]\tTime 0.039 (0.050)\tData 0.006 (0.017)\tLoss 1.5402 (1.4473)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [382][300/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.2644 (1.4541)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [382][350/391]\tTime 0.036 (0.049)\tData 0.000 (0.017)\tLoss 1.3449 (1.4394)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.099 (0.099)\tLoss 0.8649 (0.8649)\tPrec@1 89.844 (89.844)\n",
            "Test: [50/79]\tTime 0.019 (0.016)\tLoss 0.8808 (0.8833)\tPrec@1 85.938 (86.673)\n",
            " * Prec@1 87.010\n",
            "\n",
            "Epoch: 384/600\n",
            "Learning rate: 0.029019\n",
            "Epoch: [383][0/391]\tTime 0.201 (0.201)\tData 0.155 (0.155)\tLoss 1.7899 (1.7899)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [383][50/391]\tTime 0.031 (0.051)\tData 0.000 (0.019)\tLoss 1.5641 (1.5258)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [383][100/391]\tTime 0.032 (0.051)\tData 0.000 (0.018)\tLoss 1.1979 (1.4561)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [383][150/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.2026 (1.4403)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [383][200/391]\tTime 0.033 (0.050)\tData 0.000 (0.018)\tLoss 1.5964 (1.4530)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [383][250/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.7535 (1.4565)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [383][300/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.4340 (1.4486)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [383][350/391]\tTime 0.033 (0.049)\tData 0.000 (0.017)\tLoss 1.3609 (1.4403)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.098 (0.098)\tLoss 0.8900 (0.8900)\tPrec@1 88.281 (88.281)\n",
            "Test: [50/79]\tTime 0.011 (0.017)\tLoss 0.9495 (0.8873)\tPrec@1 82.031 (86.627)\n",
            " * Prec@1 86.610\n",
            "\n",
            "Epoch: 385/600\n",
            "Learning rate: 0.028782\n",
            "Epoch: [384][0/391]\tTime 0.211 (0.211)\tData 0.155 (0.155)\tLoss 1.3430 (1.3430)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [384][50/391]\tTime 0.064 (0.052)\tData 0.032 (0.019)\tLoss 1.6864 (1.4246)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [384][100/391]\tTime 0.064 (0.050)\tData 0.032 (0.017)\tLoss 1.6686 (1.4229)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [384][150/391]\tTime 0.061 (0.050)\tData 0.029 (0.017)\tLoss 1.7458 (1.4421)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [384][200/391]\tTime 0.075 (0.050)\tData 0.033 (0.017)\tLoss 1.7040 (1.4531)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [384][250/391]\tTime 0.063 (0.050)\tData 0.031 (0.016)\tLoss 1.7007 (1.4610)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [384][300/391]\tTime 0.065 (0.049)\tData 0.033 (0.016)\tLoss 0.9710 (1.4457)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [384][350/391]\tTime 0.063 (0.049)\tData 0.031 (0.016)\tLoss 1.0807 (1.4377)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 0.9203 (0.9203)\tPrec@1 87.500 (87.500)\n",
            "Test: [50/79]\tTime 0.015 (0.016)\tLoss 0.9204 (0.9078)\tPrec@1 85.156 (86.734)\n",
            " * Prec@1 86.910\n",
            "\n",
            "Epoch: 386/600\n",
            "Learning rate: 0.028546\n",
            "Epoch: [385][0/391]\tTime 0.203 (0.203)\tData 0.156 (0.156)\tLoss 1.6084 (1.6084)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [385][50/391]\tTime 0.064 (0.051)\tData 0.032 (0.018)\tLoss 1.6751 (1.5014)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [385][100/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.6140 (1.4492)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [385][150/391]\tTime 0.033 (0.050)\tData 0.000 (0.017)\tLoss 1.4937 (1.4555)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [385][200/391]\tTime 0.033 (0.050)\tData 0.000 (0.018)\tLoss 1.6146 (1.4568)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [385][250/391]\tTime 0.035 (0.050)\tData 0.000 (0.018)\tLoss 1.6260 (1.4567)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [385][300/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.5877 (1.4603)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [385][350/391]\tTime 0.033 (0.050)\tData 0.000 (0.017)\tLoss 1.7170 (1.4582)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.094 (0.094)\tLoss 0.9143 (0.9143)\tPrec@1 87.500 (87.500)\n",
            "Test: [50/79]\tTime 0.020 (0.016)\tLoss 0.9389 (0.9033)\tPrec@1 83.594 (87.577)\n",
            " * Prec@1 87.820\n",
            "\n",
            "Epoch: 387/600\n",
            "Learning rate: 0.028310\n",
            "Epoch: [386][0/391]\tTime 0.210 (0.210)\tData 0.152 (0.152)\tLoss 1.4532 (1.4532)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [386][50/391]\tTime 0.074 (0.051)\tData 0.032 (0.018)\tLoss 1.6939 (1.4350)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [386][100/391]\tTime 0.033 (0.051)\tData 0.000 (0.016)\tLoss 1.0408 (1.4207)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [386][150/391]\tTime 0.033 (0.050)\tData 0.000 (0.016)\tLoss 1.1670 (1.4334)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [386][200/391]\tTime 0.032 (0.050)\tData 0.000 (0.016)\tLoss 1.5196 (1.4450)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [386][250/391]\tTime 0.031 (0.049)\tData 0.000 (0.016)\tLoss 1.3897 (1.4400)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [386][300/391]\tTime 0.036 (0.049)\tData 0.000 (0.016)\tLoss 1.3488 (1.4316)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [386][350/391]\tTime 0.033 (0.049)\tData 0.000 (0.016)\tLoss 1.0212 (1.4313)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.098 (0.098)\tLoss 0.9598 (0.9598)\tPrec@1 82.812 (82.812)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.9265 (0.9137)\tPrec@1 84.375 (86.520)\n",
            " * Prec@1 86.430\n",
            "\n",
            "Epoch: 388/600\n",
            "Learning rate: 0.028075\n",
            "Epoch: [387][0/391]\tTime 0.206 (0.206)\tData 0.159 (0.159)\tLoss 1.7611 (1.7611)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [387][50/391]\tTime 0.067 (0.052)\tData 0.032 (0.019)\tLoss 1.0476 (1.4419)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [387][100/391]\tTime 0.066 (0.051)\tData 0.034 (0.018)\tLoss 1.4195 (1.4000)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [387][150/391]\tTime 0.069 (0.050)\tData 0.036 (0.017)\tLoss 1.3934 (1.4005)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [387][200/391]\tTime 0.071 (0.050)\tData 0.039 (0.018)\tLoss 1.6459 (1.4254)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [387][250/391]\tTime 0.067 (0.050)\tData 0.036 (0.018)\tLoss 1.5424 (1.4221)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [387][300/391]\tTime 0.065 (0.050)\tData 0.034 (0.018)\tLoss 1.4206 (1.4308)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [387][350/391]\tTime 0.062 (0.050)\tData 0.029 (0.017)\tLoss 1.4732 (1.4286)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.094 (0.094)\tLoss 0.8552 (0.8552)\tPrec@1 86.719 (86.719)\n",
            "Test: [50/79]\tTime 0.013 (0.016)\tLoss 0.8792 (0.8657)\tPrec@1 85.938 (87.500)\n",
            " * Prec@1 87.690\n",
            "\n",
            "Epoch: 389/600\n",
            "Learning rate: 0.027840\n",
            "Epoch: [388][0/391]\tTime 0.212 (0.212)\tData 0.155 (0.155)\tLoss 0.9676 (0.9676)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [388][50/391]\tTime 0.042 (0.051)\tData 0.010 (0.018)\tLoss 1.3879 (1.4225)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [388][100/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.7252 (1.4523)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [388][150/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.6620 (1.4725)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [388][200/391]\tTime 0.065 (0.049)\tData 0.032 (0.017)\tLoss 1.6480 (1.4553)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [388][250/391]\tTime 0.065 (0.049)\tData 0.032 (0.017)\tLoss 1.7830 (1.4496)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [388][300/391]\tTime 0.065 (0.049)\tData 0.032 (0.017)\tLoss 1.3945 (1.4353)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [388][350/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.7404 (1.4438)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.103 (0.103)\tLoss 0.9488 (0.9488)\tPrec@1 83.594 (83.594)\n",
            "Test: [50/79]\tTime 0.016 (0.017)\tLoss 0.8519 (0.8664)\tPrec@1 89.062 (87.408)\n",
            " * Prec@1 87.650\n",
            "\n",
            "Epoch: 390/600\n",
            "Learning rate: 0.027607\n",
            "Epoch: [389][0/391]\tTime 0.214 (0.214)\tData 0.154 (0.154)\tLoss 1.6217 (1.6217)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [389][50/391]\tTime 0.064 (0.053)\tData 0.033 (0.019)\tLoss 1.3138 (1.4461)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [389][100/391]\tTime 0.063 (0.051)\tData 0.032 (0.018)\tLoss 1.7429 (1.4552)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [389][150/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.7082 (1.4404)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [389][200/391]\tTime 0.034 (0.050)\tData 0.000 (0.017)\tLoss 1.0516 (1.4321)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [389][250/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.5091 (1.4351)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [389][300/391]\tTime 0.066 (0.050)\tData 0.034 (0.017)\tLoss 1.6884 (1.4297)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [389][350/391]\tTime 0.063 (0.050)\tData 0.031 (0.017)\tLoss 1.5951 (1.4292)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.9223 (0.9223)\tPrec@1 88.281 (88.281)\n",
            "Test: [50/79]\tTime 0.019 (0.017)\tLoss 0.8906 (0.8933)\tPrec@1 86.719 (86.550)\n",
            " * Prec@1 86.840\n",
            "\n",
            "Epoch: 391/600\n",
            "Learning rate: 0.027373\n",
            "Epoch: [390][0/391]\tTime 0.215 (0.215)\tData 0.155 (0.155)\tLoss 1.2909 (1.2909)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [390][50/391]\tTime 0.066 (0.055)\tData 0.034 (0.017)\tLoss 1.6809 (1.4554)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [390][100/391]\tTime 0.042 (0.053)\tData 0.000 (0.016)\tLoss 1.6646 (1.4314)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [390][150/391]\tTime 0.032 (0.052)\tData 0.000 (0.016)\tLoss 1.0130 (1.4189)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [390][200/391]\tTime 0.033 (0.051)\tData 0.000 (0.016)\tLoss 1.7691 (1.4237)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [390][250/391]\tTime 0.033 (0.051)\tData 0.000 (0.016)\tLoss 1.6007 (1.4310)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [390][300/391]\tTime 0.032 (0.051)\tData 0.000 (0.017)\tLoss 1.6425 (1.4309)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [390][350/391]\tTime 0.032 (0.051)\tData 0.000 (0.017)\tLoss 1.1535 (1.4309)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.099 (0.099)\tLoss 0.9554 (0.9554)\tPrec@1 85.156 (85.156)\n",
            "Test: [50/79]\tTime 0.014 (0.016)\tLoss 0.9161 (0.8763)\tPrec@1 85.156 (87.377)\n",
            " * Prec@1 87.260\n",
            "\n",
            "Epoch: 392/600\n",
            "Learning rate: 0.027140\n",
            "Epoch: [391][0/391]\tTime 0.212 (0.212)\tData 0.155 (0.155)\tLoss 1.6229 (1.6229)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [391][50/391]\tTime 0.066 (0.057)\tData 0.034 (0.018)\tLoss 0.9750 (1.3958)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [391][100/391]\tTime 0.071 (0.053)\tData 0.037 (0.018)\tLoss 1.7402 (1.3954)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [391][150/391]\tTime 0.046 (0.052)\tData 0.014 (0.018)\tLoss 1.6955 (1.4155)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [391][200/391]\tTime 0.063 (0.051)\tData 0.032 (0.018)\tLoss 1.3559 (1.4153)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [391][250/391]\tTime 0.064 (0.050)\tData 0.032 (0.017)\tLoss 1.0391 (1.4300)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [391][300/391]\tTime 0.065 (0.050)\tData 0.032 (0.017)\tLoss 1.1906 (1.4280)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [391][350/391]\tTime 0.031 (0.050)\tData 0.000 (0.017)\tLoss 1.7801 (1.4279)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.094 (0.094)\tLoss 0.8639 (0.8639)\tPrec@1 87.500 (87.500)\n",
            "Test: [50/79]\tTime 0.016 (0.016)\tLoss 0.8718 (0.8410)\tPrec@1 88.281 (88.557)\n",
            " * Prec@1 88.980\n",
            "\n",
            "Epoch: 393/600\n",
            "Learning rate: 0.026908\n",
            "Epoch: [392][0/391]\tTime 0.202 (0.202)\tData 0.154 (0.154)\tLoss 1.8436 (1.8436)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [392][50/391]\tTime 0.065 (0.051)\tData 0.033 (0.019)\tLoss 1.7130 (1.4516)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [392][100/391]\tTime 0.065 (0.050)\tData 0.033 (0.018)\tLoss 1.3224 (1.4545)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [392][150/391]\tTime 0.063 (0.050)\tData 0.031 (0.017)\tLoss 1.5401 (1.4398)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [392][200/391]\tTime 0.067 (0.049)\tData 0.034 (0.017)\tLoss 1.6514 (1.4305)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [392][250/391]\tTime 0.064 (0.049)\tData 0.033 (0.017)\tLoss 1.6087 (1.4298)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [392][300/391]\tTime 0.063 (0.049)\tData 0.032 (0.017)\tLoss 1.6615 (1.4311)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [392][350/391]\tTime 0.048 (0.049)\tData 0.016 (0.016)\tLoss 1.7306 (1.4310)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.9313 (0.9313)\tPrec@1 88.281 (88.281)\n",
            "Test: [50/79]\tTime 0.010 (0.016)\tLoss 0.9025 (0.8916)\tPrec@1 87.500 (87.868)\n",
            " * Prec@1 88.130\n",
            "\n",
            "Epoch: 394/600\n",
            "Learning rate: 0.026677\n",
            "Epoch: [393][0/391]\tTime 0.210 (0.210)\tData 0.154 (0.154)\tLoss 1.2862 (1.2862)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [393][50/391]\tTime 0.063 (0.051)\tData 0.032 (0.019)\tLoss 1.4540 (1.4821)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [393][100/391]\tTime 0.065 (0.050)\tData 0.033 (0.017)\tLoss 1.6098 (1.4432)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [393][150/391]\tTime 0.065 (0.049)\tData 0.032 (0.017)\tLoss 0.9586 (1.4425)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [393][200/391]\tTime 0.066 (0.049)\tData 0.034 (0.017)\tLoss 1.6933 (1.4449)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [393][250/391]\tTime 0.062 (0.049)\tData 0.031 (0.017)\tLoss 1.6100 (1.4330)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [393][300/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 0.9390 (1.4316)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [393][350/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 1.8140 (1.4388)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.105 (0.105)\tLoss 0.8871 (0.8871)\tPrec@1 89.844 (89.844)\n",
            "Test: [50/79]\tTime 0.011 (0.016)\tLoss 0.8719 (0.8852)\tPrec@1 89.844 (88.588)\n",
            " * Prec@1 88.480\n",
            "\n",
            "Epoch: 395/600\n",
            "Learning rate: 0.026446\n",
            "Epoch: [394][0/391]\tTime 0.201 (0.201)\tData 0.154 (0.154)\tLoss 1.6623 (1.6623)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [394][50/391]\tTime 0.065 (0.051)\tData 0.033 (0.019)\tLoss 1.6368 (1.4027)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [394][100/391]\tTime 0.064 (0.050)\tData 0.033 (0.018)\tLoss 1.7885 (1.4387)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [394][150/391]\tTime 0.067 (0.049)\tData 0.034 (0.017)\tLoss 1.4684 (1.4357)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [394][200/391]\tTime 0.065 (0.049)\tData 0.034 (0.017)\tLoss 1.0953 (1.4373)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [394][250/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.7347 (1.4367)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [394][300/391]\tTime 0.064 (0.049)\tData 0.033 (0.017)\tLoss 1.0075 (1.4278)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [394][350/391]\tTime 0.065 (0.049)\tData 0.034 (0.017)\tLoss 1.7857 (1.4279)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 0.8662 (0.8662)\tPrec@1 89.844 (89.844)\n",
            "Test: [50/79]\tTime 0.020 (0.016)\tLoss 0.8108 (0.8524)\tPrec@1 89.062 (88.511)\n",
            " * Prec@1 88.480\n",
            "\n",
            "Epoch: 396/600\n",
            "Learning rate: 0.026216\n",
            "Epoch: [395][0/391]\tTime 0.201 (0.201)\tData 0.155 (0.155)\tLoss 1.0296 (1.0296)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [395][50/391]\tTime 0.065 (0.051)\tData 0.033 (0.019)\tLoss 1.7964 (1.4009)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [395][100/391]\tTime 0.065 (0.050)\tData 0.033 (0.018)\tLoss 1.5026 (1.4151)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [395][150/391]\tTime 0.063 (0.049)\tData 0.032 (0.017)\tLoss 1.3481 (1.4132)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [395][200/391]\tTime 0.063 (0.049)\tData 0.031 (0.017)\tLoss 1.1110 (1.4062)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [395][250/391]\tTime 0.068 (0.049)\tData 0.037 (0.017)\tLoss 1.5923 (1.4083)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [395][300/391]\tTime 0.068 (0.049)\tData 0.035 (0.017)\tLoss 0.9094 (1.4098)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [395][350/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.6514 (1.4088)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.096 (0.096)\tLoss 0.8970 (0.8970)\tPrec@1 89.062 (89.062)\n",
            "Test: [50/79]\tTime 0.020 (0.016)\tLoss 0.9134 (0.8746)\tPrec@1 86.719 (88.542)\n",
            " * Prec@1 88.950\n",
            "\n",
            "Epoch: 397/600\n",
            "Learning rate: 0.025986\n",
            "Epoch: [396][0/391]\tTime 0.198 (0.198)\tData 0.153 (0.153)\tLoss 1.3514 (1.3514)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [396][50/391]\tTime 0.066 (0.051)\tData 0.031 (0.018)\tLoss 1.5249 (1.4463)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [396][100/391]\tTime 0.066 (0.053)\tData 0.034 (0.017)\tLoss 1.4438 (1.4328)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [396][150/391]\tTime 0.064 (0.051)\tData 0.033 (0.017)\tLoss 1.6502 (1.4385)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [396][200/391]\tTime 0.064 (0.050)\tData 0.033 (0.017)\tLoss 1.6693 (1.4251)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [396][250/391]\tTime 0.062 (0.050)\tData 0.030 (0.017)\tLoss 1.1048 (1.4149)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [396][300/391]\tTime 0.066 (0.050)\tData 0.033 (0.017)\tLoss 1.1628 (1.4191)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [396][350/391]\tTime 0.063 (0.050)\tData 0.031 (0.016)\tLoss 1.1056 (1.4130)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.108 (0.108)\tLoss 0.8949 (0.8949)\tPrec@1 84.375 (84.375)\n",
            "Test: [50/79]\tTime 0.015 (0.016)\tLoss 0.8569 (0.8760)\tPrec@1 92.188 (88.067)\n",
            " * Prec@1 88.270\n",
            "\n",
            "Epoch: 398/600\n",
            "Learning rate: 0.025758\n",
            "Epoch: [397][0/391]\tTime 0.203 (0.203)\tData 0.154 (0.154)\tLoss 1.0906 (1.0906)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [397][50/391]\tTime 0.064 (0.051)\tData 0.032 (0.018)\tLoss 1.0869 (1.4380)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [397][100/391]\tTime 0.068 (0.050)\tData 0.034 (0.017)\tLoss 1.5855 (1.4445)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [397][150/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.6483 (1.4377)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [397][200/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.3930 (1.4397)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [397][250/391]\tTime 0.064 (0.049)\tData 0.031 (0.017)\tLoss 1.5680 (1.4450)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [397][300/391]\tTime 0.061 (0.049)\tData 0.030 (0.017)\tLoss 1.2295 (1.4387)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [397][350/391]\tTime 0.066 (0.049)\tData 0.035 (0.017)\tLoss 1.5826 (1.4297)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.099 (0.099)\tLoss 0.8508 (0.8508)\tPrec@1 89.844 (89.844)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.8696 (0.8808)\tPrec@1 84.375 (86.964)\n",
            " * Prec@1 87.100\n",
            "\n",
            "Epoch: 399/600\n",
            "Learning rate: 0.025529\n",
            "Epoch: [398][0/391]\tTime 0.205 (0.205)\tData 0.159 (0.159)\tLoss 0.9030 (0.9030)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [398][50/391]\tTime 0.067 (0.053)\tData 0.035 (0.020)\tLoss 0.9227 (1.3686)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [398][100/391]\tTime 0.067 (0.051)\tData 0.035 (0.019)\tLoss 1.6591 (1.4314)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [398][150/391]\tTime 0.063 (0.050)\tData 0.029 (0.018)\tLoss 1.7058 (1.4420)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [398][200/391]\tTime 0.066 (0.050)\tData 0.033 (0.017)\tLoss 1.7774 (1.4273)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [398][250/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.4146 (1.4326)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [398][300/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 0.9291 (1.4248)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [398][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.2831 (1.4251)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.096 (0.096)\tLoss 0.8522 (0.8522)\tPrec@1 89.844 (89.844)\n",
            "Test: [50/79]\tTime 0.019 (0.017)\tLoss 0.8605 (0.8721)\tPrec@1 92.188 (88.695)\n",
            " * Prec@1 88.980\n",
            "\n",
            "Epoch: 400/600\n",
            "Learning rate: 0.025302\n",
            "Epoch: [399][0/391]\tTime 0.198 (0.198)\tData 0.153 (0.153)\tLoss 1.3904 (1.3904)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [399][50/391]\tTime 0.032 (0.051)\tData 0.000 (0.019)\tLoss 1.2444 (1.4563)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [399][100/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.4433 (1.4342)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [399][150/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.3775 (1.4235)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [399][200/391]\tTime 0.042 (0.050)\tData 0.000 (0.017)\tLoss 1.5816 (1.4203)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [399][250/391]\tTime 0.033 (0.050)\tData 0.000 (0.017)\tLoss 1.5847 (1.4182)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [399][300/391]\tTime 0.034 (0.049)\tData 0.000 (0.017)\tLoss 1.2350 (1.4182)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [399][350/391]\tTime 0.033 (0.050)\tData 0.000 (0.016)\tLoss 1.8015 (1.4242)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.101 (0.101)\tLoss 0.8836 (0.8836)\tPrec@1 88.281 (88.281)\n",
            "Test: [50/79]\tTime 0.019 (0.017)\tLoss 0.9306 (0.8840)\tPrec@1 85.938 (86.504)\n",
            " * Prec@1 86.670\n",
            "\n",
            "Epoch: 401/600\n",
            "Learning rate: 0.025075\n",
            "Epoch: [400][0/391]\tTime 0.216 (0.216)\tData 0.156 (0.156)\tLoss 1.1072 (1.1072)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [400][50/391]\tTime 0.063 (0.051)\tData 0.032 (0.018)\tLoss 1.4999 (1.3694)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [400][100/391]\tTime 0.068 (0.050)\tData 0.034 (0.017)\tLoss 1.6264 (1.3733)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [400][150/391]\tTime 0.066 (0.049)\tData 0.033 (0.016)\tLoss 1.2802 (1.3829)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [400][200/391]\tTime 0.033 (0.050)\tData 0.000 (0.016)\tLoss 1.3027 (1.4067)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [400][250/391]\tTime 0.031 (0.050)\tData 0.000 (0.016)\tLoss 1.2757 (1.4184)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [400][300/391]\tTime 0.040 (0.049)\tData 0.009 (0.016)\tLoss 1.0020 (1.4141)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [400][350/391]\tTime 0.067 (0.049)\tData 0.036 (0.016)\tLoss 1.0114 (1.4208)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.094 (0.094)\tLoss 0.8764 (0.8764)\tPrec@1 86.719 (86.719)\n",
            "Test: [50/79]\tTime 0.011 (0.016)\tLoss 0.8787 (0.8678)\tPrec@1 86.719 (87.883)\n",
            " * Prec@1 88.280\n",
            "\n",
            "Epoch: 402/600\n",
            "Learning rate: 0.024849\n",
            "Epoch: [401][0/391]\tTime 0.203 (0.203)\tData 0.155 (0.155)\tLoss 1.6740 (1.6740)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [401][50/391]\tTime 0.043 (0.052)\tData 0.000 (0.018)\tLoss 1.6767 (1.3540)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [401][100/391]\tTime 0.032 (0.051)\tData 0.000 (0.017)\tLoss 1.7106 (1.3791)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [401][150/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.2423 (1.3996)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [401][200/391]\tTime 0.047 (0.050)\tData 0.015 (0.016)\tLoss 1.5487 (1.4134)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [401][250/391]\tTime 0.037 (0.049)\tData 0.004 (0.016)\tLoss 1.7363 (1.4186)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [401][300/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 0.9393 (1.4237)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [401][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.0243 (1.4270)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.9073 (0.9073)\tPrec@1 86.719 (86.719)\n",
            "Test: [50/79]\tTime 0.017 (0.016)\tLoss 0.8687 (0.8779)\tPrec@1 86.719 (87.714)\n",
            " * Prec@1 87.880\n",
            "\n",
            "Epoch: 403/600\n",
            "Learning rate: 0.024623\n",
            "Epoch: [402][0/391]\tTime 0.200 (0.200)\tData 0.154 (0.154)\tLoss 1.3192 (1.3192)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [402][50/391]\tTime 0.064 (0.051)\tData 0.031 (0.018)\tLoss 1.6109 (1.3439)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [402][100/391]\tTime 0.063 (0.052)\tData 0.030 (0.017)\tLoss 1.7310 (1.3826)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [402][150/391]\tTime 0.067 (0.051)\tData 0.035 (0.017)\tLoss 1.6784 (1.4063)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [402][200/391]\tTime 0.067 (0.051)\tData 0.035 (0.017)\tLoss 1.3549 (1.3953)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [402][250/391]\tTime 0.063 (0.050)\tData 0.031 (0.017)\tLoss 0.9891 (1.3957)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [402][300/391]\tTime 0.064 (0.050)\tData 0.031 (0.017)\tLoss 1.7154 (1.4049)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [402][350/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.6289 (1.4063)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.099 (0.099)\tLoss 0.8581 (0.8581)\tPrec@1 89.844 (89.844)\n",
            "Test: [50/79]\tTime 0.012 (0.016)\tLoss 0.9395 (0.8923)\tPrec@1 84.375 (87.531)\n",
            " * Prec@1 87.720\n",
            "\n",
            "Epoch: 404/600\n",
            "Learning rate: 0.024399\n",
            "Epoch: [403][0/391]\tTime 0.213 (0.213)\tData 0.156 (0.156)\tLoss 0.8226 (0.8226)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [403][50/391]\tTime 0.064 (0.054)\tData 0.032 (0.018)\tLoss 1.0651 (1.3626)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [403][100/391]\tTime 0.039 (0.051)\tData 0.008 (0.017)\tLoss 1.2017 (1.3947)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [403][150/391]\tTime 0.032 (0.051)\tData 0.000 (0.017)\tLoss 1.7401 (1.4034)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [403][200/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.8131 (1.4243)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [403][250/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.3772 (1.4194)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [403][300/391]\tTime 0.031 (0.050)\tData 0.000 (0.018)\tLoss 1.3515 (1.4198)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [403][350/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.4582 (1.4048)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.104 (0.104)\tLoss 0.8359 (0.8359)\tPrec@1 89.844 (89.844)\n",
            "Test: [50/79]\tTime 0.010 (0.016)\tLoss 0.7995 (0.8618)\tPrec@1 90.625 (87.944)\n",
            " * Prec@1 87.990\n",
            "\n",
            "Epoch: 405/600\n",
            "Learning rate: 0.024175\n",
            "Epoch: [404][0/391]\tTime 0.202 (0.202)\tData 0.155 (0.155)\tLoss 1.6467 (1.6467)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [404][50/391]\tTime 0.076 (0.053)\tData 0.034 (0.019)\tLoss 1.1376 (1.4390)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [404][100/391]\tTime 0.065 (0.051)\tData 0.032 (0.017)\tLoss 1.3692 (1.4321)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [404][150/391]\tTime 0.062 (0.050)\tData 0.029 (0.017)\tLoss 1.5838 (1.4692)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [404][200/391]\tTime 0.065 (0.050)\tData 0.032 (0.016)\tLoss 1.3489 (1.4695)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [404][250/391]\tTime 0.063 (0.050)\tData 0.031 (0.016)\tLoss 1.4238 (1.4637)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [404][300/391]\tTime 0.064 (0.049)\tData 0.032 (0.016)\tLoss 1.5605 (1.4563)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [404][350/391]\tTime 0.063 (0.049)\tData 0.029 (0.016)\tLoss 1.3487 (1.4558)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.098 (0.098)\tLoss 0.8497 (0.8497)\tPrec@1 87.500 (87.500)\n",
            "Test: [50/79]\tTime 0.011 (0.017)\tLoss 0.9110 (0.8744)\tPrec@1 84.375 (87.194)\n",
            " * Prec@1 87.470\n",
            "\n",
            "Epoch: 406/600\n",
            "Learning rate: 0.023951\n",
            "Epoch: [405][0/391]\tTime 0.204 (0.204)\tData 0.158 (0.158)\tLoss 1.2375 (1.2375)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [405][50/391]\tTime 0.066 (0.053)\tData 0.034 (0.020)\tLoss 1.6889 (1.3769)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [405][100/391]\tTime 0.067 (0.051)\tData 0.036 (0.019)\tLoss 1.5934 (1.4158)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [405][150/391]\tTime 0.067 (0.051)\tData 0.035 (0.019)\tLoss 1.3157 (1.4214)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [405][200/391]\tTime 0.066 (0.050)\tData 0.033 (0.018)\tLoss 0.9244 (1.4125)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [405][250/391]\tTime 0.070 (0.050)\tData 0.037 (0.018)\tLoss 0.9379 (1.4010)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [405][300/391]\tTime 0.067 (0.050)\tData 0.035 (0.018)\tLoss 0.9026 (1.4083)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [405][350/391]\tTime 0.068 (0.050)\tData 0.036 (0.018)\tLoss 1.6903 (1.4011)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.096 (0.096)\tLoss 0.8141 (0.8141)\tPrec@1 89.844 (89.844)\n",
            "Test: [50/79]\tTime 0.019 (0.016)\tLoss 0.8941 (0.8612)\tPrec@1 83.594 (86.734)\n",
            " * Prec@1 87.020\n",
            "\n",
            "Epoch: 407/600\n",
            "Learning rate: 0.023729\n",
            "Epoch: [406][0/391]\tTime 0.204 (0.204)\tData 0.157 (0.157)\tLoss 1.1934 (1.1934)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [406][50/391]\tTime 0.062 (0.058)\tData 0.029 (0.019)\tLoss 1.4767 (1.3777)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [406][100/391]\tTime 0.067 (0.053)\tData 0.034 (0.018)\tLoss 1.1101 (1.4208)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [406][150/391]\tTime 0.040 (0.052)\tData 0.008 (0.017)\tLoss 1.2555 (1.4035)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [406][200/391]\tTime 0.032 (0.051)\tData 0.000 (0.017)\tLoss 1.4523 (1.4066)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [406][250/391]\tTime 0.043 (0.051)\tData 0.000 (0.017)\tLoss 0.9705 (1.4071)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [406][300/391]\tTime 0.032 (0.051)\tData 0.000 (0.017)\tLoss 1.1147 (1.4132)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [406][350/391]\tTime 0.033 (0.051)\tData 0.000 (0.017)\tLoss 1.6421 (1.4168)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.8464 (0.8464)\tPrec@1 92.969 (92.969)\n",
            "Test: [50/79]\tTime 0.013 (0.016)\tLoss 0.8940 (0.8651)\tPrec@1 89.062 (89.154)\n",
            " * Prec@1 89.220\n",
            "\n",
            "Epoch: 408/600\n",
            "Learning rate: 0.023507\n",
            "Epoch: [407][0/391]\tTime 0.212 (0.212)\tData 0.154 (0.154)\tLoss 1.5514 (1.5514)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [407][50/391]\tTime 0.048 (0.051)\tData 0.016 (0.018)\tLoss 1.1065 (1.3792)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [407][100/391]\tTime 0.034 (0.054)\tData 0.000 (0.017)\tLoss 1.5989 (1.4052)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [407][150/391]\tTime 0.032 (0.053)\tData 0.000 (0.017)\tLoss 1.6995 (1.4104)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [407][200/391]\tTime 0.035 (0.052)\tData 0.000 (0.017)\tLoss 1.5311 (1.4047)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [407][250/391]\tTime 0.032 (0.051)\tData 0.000 (0.017)\tLoss 1.3900 (1.3978)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [407][300/391]\tTime 0.032 (0.051)\tData 0.000 (0.017)\tLoss 1.1681 (1.4054)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [407][350/391]\tTime 0.032 (0.051)\tData 0.000 (0.017)\tLoss 1.1152 (1.4099)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.105 (0.105)\tLoss 0.8505 (0.8505)\tPrec@1 89.844 (89.844)\n",
            "Test: [50/79]\tTime 0.023 (0.018)\tLoss 0.8628 (0.8569)\tPrec@1 90.625 (88.741)\n",
            " * Prec@1 89.130\n",
            "\n",
            "Epoch: 409/600\n",
            "Learning rate: 0.023285\n",
            "Epoch: [408][0/391]\tTime 0.201 (0.201)\tData 0.153 (0.153)\tLoss 1.7370 (1.7370)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [408][50/391]\tTime 0.042 (0.051)\tData 0.010 (0.018)\tLoss 1.7657 (1.4225)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [408][100/391]\tTime 0.039 (0.050)\tData 0.007 (0.017)\tLoss 1.7211 (1.4199)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [408][150/391]\tTime 0.033 (0.049)\tData 0.000 (0.017)\tLoss 1.2999 (1.4180)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [408][200/391]\tTime 0.038 (0.049)\tData 0.006 (0.017)\tLoss 1.6888 (1.4357)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [408][250/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.5812 (1.4388)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [408][300/391]\tTime 0.058 (0.049)\tData 0.027 (0.017)\tLoss 0.9879 (1.4329)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [408][350/391]\tTime 0.068 (0.049)\tData 0.035 (0.017)\tLoss 1.0912 (1.4243)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.104 (0.104)\tLoss 0.8722 (0.8722)\tPrec@1 88.281 (88.281)\n",
            "Test: [50/79]\tTime 0.019 (0.016)\tLoss 0.8525 (0.8554)\tPrec@1 86.719 (88.833)\n",
            " * Prec@1 89.350\n",
            "\n",
            "Epoch: 410/600\n",
            "Learning rate: 0.023065\n",
            "Epoch: [409][0/391]\tTime 0.201 (0.201)\tData 0.155 (0.155)\tLoss 0.9077 (0.9077)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [409][50/391]\tTime 0.032 (0.052)\tData 0.000 (0.019)\tLoss 1.4648 (1.4343)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [409][100/391]\tTime 0.032 (0.051)\tData 0.000 (0.018)\tLoss 0.9936 (1.4081)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [409][150/391]\tTime 0.032 (0.051)\tData 0.000 (0.018)\tLoss 1.7731 (1.4100)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [409][200/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.6336 (1.4242)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [409][250/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.7333 (1.4187)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [409][300/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.7271 (1.4104)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [409][350/391]\tTime 0.036 (0.050)\tData 0.000 (0.018)\tLoss 1.1845 (1.4160)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.100 (0.100)\tLoss 0.8906 (0.8906)\tPrec@1 88.281 (88.281)\n",
            "Test: [50/79]\tTime 0.019 (0.016)\tLoss 0.9600 (0.8781)\tPrec@1 84.375 (87.194)\n",
            " * Prec@1 87.370\n",
            "\n",
            "Epoch: 411/600\n",
            "Learning rate: 0.022845\n",
            "Epoch: [410][0/391]\tTime 0.211 (0.211)\tData 0.162 (0.162)\tLoss 1.6398 (1.6398)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [410][50/391]\tTime 0.063 (0.052)\tData 0.031 (0.019)\tLoss 1.6911 (1.4723)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [410][100/391]\tTime 0.055 (0.050)\tData 0.023 (0.018)\tLoss 1.1408 (1.4365)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [410][150/391]\tTime 0.031 (0.050)\tData 0.000 (0.017)\tLoss 1.2026 (1.4391)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [410][200/391]\tTime 0.033 (0.050)\tData 0.000 (0.017)\tLoss 1.7119 (1.4293)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [410][250/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.6551 (1.4363)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [410][300/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 1.4727 (1.4357)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [410][350/391]\tTime 0.068 (0.049)\tData 0.036 (0.017)\tLoss 1.2278 (1.4306)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.8222 (0.8222)\tPrec@1 89.062 (89.062)\n",
            "Test: [50/79]\tTime 0.020 (0.017)\tLoss 0.8297 (0.8132)\tPrec@1 85.938 (90.196)\n",
            " * Prec@1 90.220\n",
            "\n",
            "Epoch: 412/600\n",
            "Learning rate: 0.022626\n",
            "Epoch: [411][0/391]\tTime 0.203 (0.203)\tData 0.157 (0.157)\tLoss 1.5967 (1.5967)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [411][50/391]\tTime 0.063 (0.051)\tData 0.031 (0.018)\tLoss 1.4755 (1.4427)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [411][100/391]\tTime 0.068 (0.049)\tData 0.036 (0.017)\tLoss 1.6147 (1.4539)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [411][150/391]\tTime 0.063 (0.049)\tData 0.031 (0.017)\tLoss 1.7674 (1.4295)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [411][200/391]\tTime 0.035 (0.048)\tData 0.004 (0.017)\tLoss 1.7229 (1.4297)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [411][250/391]\tTime 0.066 (0.049)\tData 0.033 (0.017)\tLoss 1.3838 (1.4227)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [411][300/391]\tTime 0.066 (0.049)\tData 0.033 (0.017)\tLoss 1.3001 (1.4191)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [411][350/391]\tTime 0.066 (0.049)\tData 0.035 (0.017)\tLoss 1.7542 (1.4237)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 0.8817 (0.8817)\tPrec@1 88.281 (88.281)\n",
            "Test: [50/79]\tTime 0.010 (0.016)\tLoss 0.8893 (0.8749)\tPrec@1 84.375 (87.423)\n",
            " * Prec@1 87.530\n",
            "\n",
            "Epoch: 413/600\n",
            "Learning rate: 0.022408\n",
            "Epoch: [412][0/391]\tTime 0.212 (0.212)\tData 0.154 (0.154)\tLoss 1.4761 (1.4761)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [412][50/391]\tTime 0.031 (0.051)\tData 0.000 (0.018)\tLoss 1.0586 (1.3803)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [412][100/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.2379 (1.4119)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [412][150/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.6458 (1.4003)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [412][200/391]\tTime 0.038 (0.049)\tData 0.007 (0.017)\tLoss 1.6388 (1.4085)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [412][250/391]\tTime 0.060 (0.049)\tData 0.028 (0.017)\tLoss 1.5517 (1.4131)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [412][300/391]\tTime 0.067 (0.049)\tData 0.034 (0.017)\tLoss 1.1169 (1.4244)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [412][350/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.5375 (1.4206)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 0.8468 (0.8468)\tPrec@1 89.844 (89.844)\n",
            "Test: [50/79]\tTime 0.012 (0.016)\tLoss 0.8145 (0.8267)\tPrec@1 90.625 (90.671)\n",
            " * Prec@1 90.480\n",
            "\n",
            "Epoch: 414/600\n",
            "Learning rate: 0.022191\n",
            "Epoch: [413][0/391]\tTime 0.214 (0.214)\tData 0.155 (0.155)\tLoss 1.7097 (1.7097)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [413][50/391]\tTime 0.063 (0.054)\tData 0.032 (0.017)\tLoss 1.4714 (1.4151)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [413][100/391]\tTime 0.063 (0.054)\tData 0.030 (0.017)\tLoss 1.6957 (1.4283)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [413][150/391]\tTime 0.065 (0.052)\tData 0.032 (0.016)\tLoss 0.9980 (1.4101)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [413][200/391]\tTime 0.031 (0.051)\tData 0.000 (0.017)\tLoss 1.4474 (1.4126)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [413][250/391]\tTime 0.033 (0.051)\tData 0.000 (0.017)\tLoss 1.6993 (1.4167)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [413][300/391]\tTime 0.035 (0.051)\tData 0.000 (0.017)\tLoss 1.6827 (1.4164)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [413][350/391]\tTime 0.031 (0.051)\tData 0.000 (0.017)\tLoss 1.7760 (1.4201)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.106 (0.106)\tLoss 0.8431 (0.8431)\tPrec@1 89.062 (89.062)\n",
            "Test: [50/79]\tTime 0.019 (0.016)\tLoss 0.8408 (0.8467)\tPrec@1 89.062 (88.281)\n",
            " * Prec@1 88.510\n",
            "\n",
            "Epoch: 415/600\n",
            "Learning rate: 0.021974\n",
            "Epoch: [414][0/391]\tTime 0.212 (0.212)\tData 0.153 (0.153)\tLoss 1.1414 (1.1414)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [414][50/391]\tTime 0.065 (0.051)\tData 0.032 (0.018)\tLoss 1.7104 (1.3412)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [414][100/391]\tTime 0.066 (0.049)\tData 0.032 (0.017)\tLoss 1.5800 (1.3855)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [414][150/391]\tTime 0.064 (0.049)\tData 0.031 (0.017)\tLoss 1.4993 (1.3963)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [414][200/391]\tTime 0.042 (0.050)\tData 0.000 (0.017)\tLoss 1.5626 (1.4112)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [414][250/391]\tTime 0.033 (0.050)\tData 0.000 (0.017)\tLoss 1.7365 (1.4125)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [414][300/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.4675 (1.4107)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [414][350/391]\tTime 0.068 (0.050)\tData 0.031 (0.017)\tLoss 1.5562 (1.4156)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.8265 (0.8265)\tPrec@1 92.188 (92.188)\n",
            "Test: [50/79]\tTime 0.019 (0.016)\tLoss 0.8747 (0.8483)\tPrec@1 85.938 (88.817)\n",
            " * Prec@1 89.030\n",
            "\n",
            "Epoch: 416/600\n",
            "Learning rate: 0.021758\n",
            "Epoch: [415][0/391]\tTime 0.203 (0.203)\tData 0.154 (0.154)\tLoss 1.6492 (1.6492)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [415][50/391]\tTime 0.052 (0.051)\tData 0.021 (0.018)\tLoss 1.6546 (1.3932)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [415][100/391]\tTime 0.033 (0.049)\tData 0.001 (0.017)\tLoss 1.3744 (1.4409)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [415][150/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.1819 (1.4219)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [415][200/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.0244 (1.4184)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [415][250/391]\tTime 0.031 (0.049)\tData 0.000 (0.016)\tLoss 1.2679 (1.4040)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [415][300/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.6143 (1.4044)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [415][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.2233 (1.4055)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 0.8487 (0.8487)\tPrec@1 90.625 (90.625)\n",
            "Test: [50/79]\tTime 0.011 (0.017)\tLoss 0.9013 (0.8803)\tPrec@1 89.844 (88.174)\n",
            " * Prec@1 88.260\n",
            "\n",
            "Epoch: 417/600\n",
            "Learning rate: 0.021543\n",
            "Epoch: [416][0/391]\tTime 0.213 (0.213)\tData 0.155 (0.155)\tLoss 1.5057 (1.5057)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [416][50/391]\tTime 0.042 (0.060)\tData 0.000 (0.018)\tLoss 1.6844 (1.4006)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [416][100/391]\tTime 0.067 (0.057)\tData 0.034 (0.016)\tLoss 0.7889 (1.4183)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [416][150/391]\tTime 0.065 (0.054)\tData 0.033 (0.016)\tLoss 1.3861 (1.4097)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [416][200/391]\tTime 0.066 (0.053)\tData 0.033 (0.016)\tLoss 1.5314 (1.4173)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [416][250/391]\tTime 0.066 (0.052)\tData 0.034 (0.016)\tLoss 1.2069 (1.4122)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [416][300/391]\tTime 0.065 (0.052)\tData 0.033 (0.016)\tLoss 1.3446 (1.4149)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [416][350/391]\tTime 0.066 (0.051)\tData 0.033 (0.016)\tLoss 1.2420 (1.4111)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.8744 (0.8744)\tPrec@1 89.844 (89.844)\n",
            "Test: [50/79]\tTime 0.017 (0.016)\tLoss 0.8291 (0.8351)\tPrec@1 90.625 (89.874)\n",
            " * Prec@1 90.040\n",
            "\n",
            "Epoch: 418/600\n",
            "Learning rate: 0.021328\n",
            "Epoch: [417][0/391]\tTime 0.216 (0.216)\tData 0.158 (0.158)\tLoss 1.6434 (1.6434)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [417][50/391]\tTime 0.066 (0.052)\tData 0.034 (0.018)\tLoss 1.5440 (1.4178)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [417][100/391]\tTime 0.031 (0.052)\tData 0.000 (0.017)\tLoss 1.4959 (1.4268)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [417][150/391]\tTime 0.031 (0.050)\tData 0.000 (0.016)\tLoss 1.2797 (1.4050)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [417][200/391]\tTime 0.033 (0.050)\tData 0.000 (0.016)\tLoss 1.6834 (1.3972)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [417][250/391]\tTime 0.033 (0.050)\tData 0.000 (0.016)\tLoss 1.7715 (1.3981)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [417][300/391]\tTime 0.033 (0.049)\tData 0.000 (0.016)\tLoss 1.6453 (1.4057)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [417][350/391]\tTime 0.034 (0.049)\tData 0.000 (0.016)\tLoss 1.4801 (1.4082)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.098 (0.098)\tLoss 0.8121 (0.8121)\tPrec@1 89.062 (89.062)\n",
            "Test: [50/79]\tTime 0.019 (0.016)\tLoss 0.8107 (0.8177)\tPrec@1 87.500 (89.430)\n",
            " * Prec@1 89.730\n",
            "\n",
            "Epoch: 419/600\n",
            "Learning rate: 0.021115\n",
            "Epoch: [418][0/391]\tTime 0.204 (0.204)\tData 0.158 (0.158)\tLoss 1.3160 (1.3160)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [418][50/391]\tTime 0.062 (0.052)\tData 0.030 (0.019)\tLoss 1.0180 (1.3633)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [418][100/391]\tTime 0.064 (0.051)\tData 0.032 (0.018)\tLoss 1.6410 (1.3555)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [418][150/391]\tTime 0.065 (0.050)\tData 0.033 (0.017)\tLoss 1.3052 (1.3623)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [418][200/391]\tTime 0.065 (0.050)\tData 0.033 (0.017)\tLoss 1.1211 (1.3832)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [418][250/391]\tTime 0.064 (0.049)\tData 0.031 (0.017)\tLoss 1.0007 (1.3875)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [418][300/391]\tTime 0.059 (0.049)\tData 0.027 (0.017)\tLoss 1.5698 (1.3999)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [418][350/391]\tTime 0.064 (0.049)\tData 0.033 (0.017)\tLoss 1.7408 (1.4000)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.096 (0.096)\tLoss 0.8196 (0.8196)\tPrec@1 89.062 (89.062)\n",
            "Test: [50/79]\tTime 0.017 (0.016)\tLoss 0.8187 (0.8244)\tPrec@1 89.844 (89.568)\n",
            " * Prec@1 89.720\n",
            "\n",
            "Epoch: 420/600\n",
            "Learning rate: 0.020902\n",
            "Epoch: [419][0/391]\tTime 0.212 (0.212)\tData 0.153 (0.153)\tLoss 1.7158 (1.7158)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [419][50/391]\tTime 0.075 (0.062)\tData 0.032 (0.019)\tLoss 1.5463 (1.3592)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [419][100/391]\tTime 0.032 (0.059)\tData 0.000 (0.017)\tLoss 1.6133 (1.4110)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [419][150/391]\tTime 0.031 (0.055)\tData 0.000 (0.017)\tLoss 1.6816 (1.4023)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [419][200/391]\tTime 0.032 (0.054)\tData 0.000 (0.017)\tLoss 1.6674 (1.4007)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [419][250/391]\tTime 0.031 (0.053)\tData 0.000 (0.017)\tLoss 1.3642 (1.3997)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [419][300/391]\tTime 0.032 (0.052)\tData 0.000 (0.017)\tLoss 1.5819 (1.4061)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [419][350/391]\tTime 0.041 (0.051)\tData 0.010 (0.016)\tLoss 1.3123 (1.4124)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 0.8100 (0.8100)\tPrec@1 89.844 (89.844)\n",
            "Test: [50/79]\tTime 0.021 (0.017)\tLoss 0.8083 (0.8278)\tPrec@1 89.844 (89.323)\n",
            " * Prec@1 89.600\n",
            "\n",
            "Epoch: 421/600\n",
            "Learning rate: 0.020690\n",
            "Epoch: [420][0/391]\tTime 0.204 (0.204)\tData 0.157 (0.157)\tLoss 1.7817 (1.7817)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [420][50/391]\tTime 0.064 (0.051)\tData 0.032 (0.019)\tLoss 1.6907 (1.4016)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [420][100/391]\tTime 0.065 (0.050)\tData 0.033 (0.018)\tLoss 1.7245 (1.3927)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [420][150/391]\tTime 0.065 (0.049)\tData 0.032 (0.017)\tLoss 1.5170 (1.3961)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [420][200/391]\tTime 0.064 (0.049)\tData 0.033 (0.017)\tLoss 1.3762 (1.4009)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [420][250/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.6536 (1.4140)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [420][300/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.6711 (1.3965)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [420][350/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.2310 (1.4049)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 0.8454 (0.8454)\tPrec@1 91.406 (91.406)\n",
            "Test: [50/79]\tTime 0.010 (0.016)\tLoss 0.8528 (0.8650)\tPrec@1 89.844 (89.154)\n",
            " * Prec@1 89.510\n",
            "\n",
            "Epoch: 422/600\n",
            "Learning rate: 0.020479\n",
            "Epoch: [421][0/391]\tTime 0.203 (0.203)\tData 0.158 (0.158)\tLoss 0.9683 (0.9683)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [421][50/391]\tTime 0.065 (0.051)\tData 0.031 (0.018)\tLoss 1.3088 (1.4465)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [421][100/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.2470 (1.4103)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [421][150/391]\tTime 0.064 (0.049)\tData 0.033 (0.017)\tLoss 1.1994 (1.4262)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [421][200/391]\tTime 0.064 (0.049)\tData 0.031 (0.017)\tLoss 1.0119 (1.4144)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [421][250/391]\tTime 0.045 (0.049)\tData 0.013 (0.016)\tLoss 1.3831 (1.4190)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [421][300/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 0.9585 (1.4226)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [421][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.4432 (1.4284)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.096 (0.096)\tLoss 0.8556 (0.8556)\tPrec@1 89.062 (89.062)\n",
            "Test: [50/79]\tTime 0.019 (0.016)\tLoss 0.9073 (0.8553)\tPrec@1 83.594 (88.189)\n",
            " * Prec@1 88.560\n",
            "\n",
            "Epoch: 423/600\n",
            "Learning rate: 0.020269\n",
            "Epoch: [422][0/391]\tTime 0.216 (0.216)\tData 0.155 (0.155)\tLoss 1.6660 (1.6660)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [422][50/391]\tTime 0.032 (0.052)\tData 0.000 (0.017)\tLoss 1.5777 (1.3471)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [422][100/391]\tTime 0.032 (0.050)\tData 0.000 (0.016)\tLoss 1.4639 (1.3894)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [422][150/391]\tTime 0.032 (0.050)\tData 0.000 (0.016)\tLoss 1.6293 (1.3925)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [422][200/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 0.9480 (1.3996)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [422][250/391]\tTime 0.067 (0.050)\tData 0.035 (0.016)\tLoss 1.2050 (1.4016)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [422][300/391]\tTime 0.064 (0.050)\tData 0.032 (0.016)\tLoss 1.5234 (1.3975)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [422][350/391]\tTime 0.067 (0.050)\tData 0.035 (0.016)\tLoss 0.8445 (1.3994)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.104 (0.104)\tLoss 0.8283 (0.8283)\tPrec@1 90.625 (90.625)\n",
            "Test: [50/79]\tTime 0.022 (0.018)\tLoss 0.9424 (0.8574)\tPrec@1 82.812 (88.343)\n",
            " * Prec@1 88.850\n",
            "\n",
            "Epoch: 424/600\n",
            "Learning rate: 0.020059\n",
            "Epoch: [423][0/391]\tTime 0.197 (0.197)\tData 0.151 (0.151)\tLoss 1.5466 (1.5466)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [423][50/391]\tTime 0.061 (0.051)\tData 0.027 (0.017)\tLoss 1.3497 (1.3691)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [423][100/391]\tTime 0.078 (0.052)\tData 0.036 (0.017)\tLoss 0.8800 (1.3989)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [423][150/391]\tTime 0.069 (0.054)\tData 0.037 (0.018)\tLoss 1.3417 (1.3980)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [423][200/391]\tTime 0.066 (0.053)\tData 0.033 (0.018)\tLoss 1.6751 (1.4013)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [423][250/391]\tTime 0.067 (0.052)\tData 0.034 (0.018)\tLoss 1.3830 (1.4061)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [423][300/391]\tTime 0.068 (0.052)\tData 0.036 (0.018)\tLoss 1.6132 (1.4030)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [423][350/391]\tTime 0.069 (0.052)\tData 0.036 (0.018)\tLoss 1.0447 (1.4069)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.096 (0.096)\tLoss 0.8133 (0.8133)\tPrec@1 92.188 (92.188)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.8841 (0.8515)\tPrec@1 85.938 (89.828)\n",
            " * Prec@1 89.960\n",
            "\n",
            "Epoch: 425/600\n",
            "Learning rate: 0.019850\n",
            "Epoch: [424][0/391]\tTime 0.204 (0.204)\tData 0.158 (0.158)\tLoss 1.6827 (1.6827)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [424][50/391]\tTime 0.063 (0.052)\tData 0.032 (0.020)\tLoss 1.1500 (1.4553)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [424][100/391]\tTime 0.031 (0.051)\tData 0.000 (0.017)\tLoss 1.6721 (1.4296)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [424][150/391]\tTime 0.053 (0.051)\tData 0.021 (0.017)\tLoss 0.9418 (1.4054)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [424][200/391]\tTime 0.065 (0.050)\tData 0.034 (0.017)\tLoss 1.4862 (1.4000)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [424][250/391]\tTime 0.064 (0.050)\tData 0.033 (0.017)\tLoss 1.5835 (1.3992)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [424][300/391]\tTime 0.059 (0.050)\tData 0.027 (0.016)\tLoss 1.2595 (1.3922)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [424][350/391]\tTime 0.064 (0.050)\tData 0.033 (0.016)\tLoss 1.3009 (1.3941)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 0.8024 (0.8024)\tPrec@1 91.406 (91.406)\n",
            "Test: [50/79]\tTime 0.017 (0.016)\tLoss 0.8266 (0.8312)\tPrec@1 89.062 (90.610)\n",
            " * Prec@1 90.680\n",
            "\n",
            "Epoch: 426/600\n",
            "Learning rate: 0.019642\n",
            "Epoch: [425][0/391]\tTime 0.202 (0.202)\tData 0.157 (0.157)\tLoss 1.7134 (1.7134)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [425][50/391]\tTime 0.073 (0.055)\tData 0.031 (0.018)\tLoss 1.7251 (1.4129)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [425][100/391]\tTime 0.066 (0.055)\tData 0.031 (0.017)\tLoss 1.2050 (1.3844)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [425][150/391]\tTime 0.064 (0.053)\tData 0.033 (0.016)\tLoss 1.5650 (1.4118)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [425][200/391]\tTime 0.065 (0.052)\tData 0.034 (0.017)\tLoss 0.9600 (1.4032)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [425][250/391]\tTime 0.047 (0.051)\tData 0.016 (0.017)\tLoss 1.5150 (1.4076)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [425][300/391]\tTime 0.048 (0.051)\tData 0.017 (0.017)\tLoss 1.6522 (1.4201)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [425][350/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.7595 (1.4058)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.096 (0.096)\tLoss 0.8283 (0.8283)\tPrec@1 89.062 (89.062)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.8024 (0.8271)\tPrec@1 91.406 (90.089)\n",
            " * Prec@1 90.000\n",
            "\n",
            "Epoch: 427/600\n",
            "Learning rate: 0.019435\n",
            "Epoch: [426][0/391]\tTime 0.214 (0.214)\tData 0.158 (0.158)\tLoss 1.7426 (1.7426)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [426][50/391]\tTime 0.064 (0.052)\tData 0.032 (0.019)\tLoss 1.1438 (1.4171)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [426][100/391]\tTime 0.031 (0.051)\tData 0.000 (0.018)\tLoss 1.7746 (1.4087)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [426][150/391]\tTime 0.042 (0.050)\tData 0.000 (0.017)\tLoss 1.1020 (1.4170)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [426][200/391]\tTime 0.057 (0.050)\tData 0.025 (0.017)\tLoss 1.4103 (1.4052)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [426][250/391]\tTime 0.033 (0.050)\tData 0.001 (0.017)\tLoss 1.8021 (1.4073)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [426][300/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.6755 (1.4113)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [426][350/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.0352 (1.4079)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.099 (0.099)\tLoss 0.8277 (0.8277)\tPrec@1 90.625 (90.625)\n",
            "Test: [50/79]\tTime 0.017 (0.016)\tLoss 0.8211 (0.8463)\tPrec@1 91.406 (89.966)\n",
            " * Prec@1 90.190\n",
            "\n",
            "Epoch: 428/600\n",
            "Learning rate: 0.019229\n",
            "Epoch: [427][0/391]\tTime 0.215 (0.215)\tData 0.158 (0.158)\tLoss 1.6568 (1.6568)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [427][50/391]\tTime 0.065 (0.051)\tData 0.034 (0.019)\tLoss 1.5710 (1.4048)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [427][100/391]\tTime 0.065 (0.050)\tData 0.034 (0.018)\tLoss 1.5006 (1.4043)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [427][150/391]\tTime 0.064 (0.049)\tData 0.031 (0.018)\tLoss 1.6599 (1.3850)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [427][200/391]\tTime 0.064 (0.049)\tData 0.029 (0.017)\tLoss 1.0443 (1.3775)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [427][250/391]\tTime 0.064 (0.049)\tData 0.031 (0.017)\tLoss 1.6562 (1.3886)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [427][300/391]\tTime 0.033 (0.049)\tData 0.000 (0.017)\tLoss 1.4565 (1.3894)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [427][350/391]\tTime 0.033 (0.049)\tData 0.000 (0.017)\tLoss 0.9023 (1.3951)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.8125 (0.8125)\tPrec@1 93.750 (93.750)\n",
            "Test: [50/79]\tTime 0.014 (0.016)\tLoss 0.7934 (0.8387)\tPrec@1 92.188 (89.859)\n",
            " * Prec@1 89.890\n",
            "\n",
            "Epoch: 429/600\n",
            "Learning rate: 0.019024\n",
            "Epoch: [428][0/391]\tTime 0.206 (0.206)\tData 0.159 (0.159)\tLoss 1.2395 (1.2395)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [428][50/391]\tTime 0.032 (0.053)\tData 0.000 (0.017)\tLoss 1.3607 (1.4135)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [428][100/391]\tTime 0.035 (0.051)\tData 0.000 (0.017)\tLoss 1.6833 (1.3874)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [428][150/391]\tTime 0.033 (0.050)\tData 0.000 (0.016)\tLoss 1.4472 (1.4082)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [428][200/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.4754 (1.4200)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [428][250/391]\tTime 0.034 (0.049)\tData 0.000 (0.016)\tLoss 1.0512 (1.4084)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [428][300/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 0.9477 (1.4140)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [428][350/391]\tTime 0.031 (0.049)\tData 0.000 (0.016)\tLoss 1.1603 (1.4118)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.096 (0.096)\tLoss 0.7831 (0.7831)\tPrec@1 91.406 (91.406)\n",
            "Test: [50/79]\tTime 0.012 (0.016)\tLoss 0.8567 (0.8072)\tPrec@1 86.719 (90.196)\n",
            " * Prec@1 90.360\n",
            "\n",
            "Epoch: 430/600\n",
            "Learning rate: 0.018819\n",
            "Epoch: [429][0/391]\tTime 0.212 (0.212)\tData 0.152 (0.152)\tLoss 0.9492 (0.9492)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [429][50/391]\tTime 0.063 (0.051)\tData 0.032 (0.018)\tLoss 1.2012 (1.3681)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [429][100/391]\tTime 0.061 (0.050)\tData 0.029 (0.017)\tLoss 1.1557 (1.3729)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [429][150/391]\tTime 0.054 (0.049)\tData 0.023 (0.017)\tLoss 1.5891 (1.3988)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [429][200/391]\tTime 0.048 (0.049)\tData 0.016 (0.017)\tLoss 1.7041 (1.3961)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [429][250/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 1.7422 (1.3915)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [429][300/391]\tTime 0.034 (0.049)\tData 0.000 (0.017)\tLoss 1.6373 (1.3887)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [429][350/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 1.6914 (1.3970)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 0.8619 (0.8619)\tPrec@1 88.281 (88.281)\n",
            "Test: [50/79]\tTime 0.011 (0.016)\tLoss 0.9124 (0.9018)\tPrec@1 85.938 (87.025)\n",
            " * Prec@1 87.590\n",
            "\n",
            "Epoch: 431/600\n",
            "Learning rate: 0.018615\n",
            "Epoch: [430][0/391]\tTime 0.201 (0.201)\tData 0.154 (0.154)\tLoss 1.7282 (1.7282)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [430][50/391]\tTime 0.063 (0.051)\tData 0.031 (0.018)\tLoss 1.2511 (1.4159)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [430][100/391]\tTime 0.062 (0.049)\tData 0.028 (0.017)\tLoss 1.3107 (1.4226)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [430][150/391]\tTime 0.049 (0.049)\tData 0.016 (0.016)\tLoss 1.3794 (1.4338)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [430][200/391]\tTime 0.037 (0.049)\tData 0.005 (0.016)\tLoss 1.7047 (1.4360)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [430][250/391]\tTime 0.033 (0.048)\tData 0.001 (0.016)\tLoss 1.4874 (1.4266)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [430][300/391]\tTime 0.032 (0.048)\tData 0.000 (0.016)\tLoss 1.6313 (1.4344)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [430][350/391]\tTime 0.032 (0.048)\tData 0.000 (0.016)\tLoss 1.5821 (1.4245)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.096 (0.096)\tLoss 0.8235 (0.8235)\tPrec@1 90.625 (90.625)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.8423 (0.8673)\tPrec@1 91.406 (88.312)\n",
            " * Prec@1 88.590\n",
            "\n",
            "Epoch: 432/600\n",
            "Learning rate: 0.018413\n",
            "Epoch: [431][0/391]\tTime 0.216 (0.216)\tData 0.157 (0.157)\tLoss 1.4371 (1.4371)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [431][50/391]\tTime 0.032 (0.051)\tData 0.000 (0.019)\tLoss 1.0336 (1.4407)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [431][100/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.1513 (1.4027)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [431][150/391]\tTime 0.033 (0.050)\tData 0.000 (0.018)\tLoss 1.4318 (1.4116)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [431][200/391]\tTime 0.031 (0.050)\tData 0.000 (0.017)\tLoss 0.8406 (1.3902)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [431][250/391]\tTime 0.058 (0.049)\tData 0.027 (0.017)\tLoss 1.3809 (1.4012)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [431][300/391]\tTime 0.068 (0.049)\tData 0.034 (0.017)\tLoss 1.0840 (1.3997)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [431][350/391]\tTime 0.066 (0.049)\tData 0.035 (0.017)\tLoss 1.5275 (1.4004)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.101 (0.101)\tLoss 0.8310 (0.8310)\tPrec@1 91.406 (91.406)\n",
            "Test: [50/79]\tTime 0.016 (0.017)\tLoss 0.7936 (0.8367)\tPrec@1 93.750 (90.089)\n",
            " * Prec@1 90.270\n",
            "\n",
            "Epoch: 433/600\n",
            "Learning rate: 0.018211\n",
            "Epoch: [432][0/391]\tTime 0.201 (0.201)\tData 0.154 (0.154)\tLoss 1.3930 (1.3930)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [432][50/391]\tTime 0.064 (0.051)\tData 0.030 (0.018)\tLoss 1.6711 (1.3616)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [432][100/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.6564 (1.3789)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [432][150/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.6535 (1.3788)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [432][200/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.7939 (1.3891)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [432][250/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.2667 (1.3829)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [432][300/391]\tTime 0.033 (0.050)\tData 0.000 (0.017)\tLoss 1.6013 (1.3923)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [432][350/391]\tTime 0.033 (0.050)\tData 0.000 (0.017)\tLoss 1.7049 (1.3843)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.8193 (0.8193)\tPrec@1 91.406 (91.406)\n",
            "Test: [50/79]\tTime 0.020 (0.016)\tLoss 0.8152 (0.8434)\tPrec@1 90.625 (88.588)\n",
            " * Prec@1 88.790\n",
            "\n",
            "Epoch: 434/600\n",
            "Learning rate: 0.018010\n",
            "Epoch: [433][0/391]\tTime 0.212 (0.212)\tData 0.155 (0.155)\tLoss 1.4547 (1.4547)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [433][50/391]\tTime 0.062 (0.055)\tData 0.030 (0.017)\tLoss 1.0136 (1.3932)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [433][100/391]\tTime 0.063 (0.051)\tData 0.031 (0.016)\tLoss 1.6475 (1.3925)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [433][150/391]\tTime 0.065 (0.050)\tData 0.034 (0.016)\tLoss 1.6397 (1.3746)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [433][200/391]\tTime 0.065 (0.050)\tData 0.032 (0.016)\tLoss 1.3297 (1.3953)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [433][250/391]\tTime 0.069 (0.050)\tData 0.037 (0.016)\tLoss 1.5960 (1.3882)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [433][300/391]\tTime 0.066 (0.049)\tData 0.033 (0.016)\tLoss 1.4769 (1.3925)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [433][350/391]\tTime 0.065 (0.049)\tData 0.033 (0.016)\tLoss 1.7459 (1.3990)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.096 (0.096)\tLoss 0.8026 (0.8026)\tPrec@1 92.188 (92.188)\n",
            "Test: [50/79]\tTime 0.019 (0.016)\tLoss 0.8342 (0.8086)\tPrec@1 89.844 (90.426)\n",
            " * Prec@1 90.360\n",
            "\n",
            "Epoch: 435/600\n",
            "Learning rate: 0.017809\n",
            "Epoch: [434][0/391]\tTime 0.202 (0.202)\tData 0.155 (0.155)\tLoss 1.0694 (1.0694)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [434][50/391]\tTime 0.063 (0.051)\tData 0.032 (0.019)\tLoss 1.6336 (1.4100)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [434][100/391]\tTime 0.065 (0.050)\tData 0.033 (0.018)\tLoss 1.4280 (1.4161)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [434][150/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 0.9473 (1.3921)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [434][200/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.5942 (1.3904)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [434][250/391]\tTime 0.033 (0.050)\tData 0.000 (0.018)\tLoss 1.3206 (1.3927)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [434][300/391]\tTime 0.033 (0.050)\tData 0.000 (0.018)\tLoss 1.5210 (1.3928)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [434][350/391]\tTime 0.051 (0.050)\tData 0.018 (0.018)\tLoss 1.2074 (1.3950)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.8288 (0.8288)\tPrec@1 92.188 (92.188)\n",
            "Test: [50/79]\tTime 0.020 (0.017)\tLoss 0.8292 (0.8319)\tPrec@1 89.062 (90.242)\n",
            " * Prec@1 90.450\n",
            "\n",
            "Epoch: 436/600\n",
            "Learning rate: 0.017610\n",
            "Epoch: [435][0/391]\tTime 0.202 (0.202)\tData 0.156 (0.156)\tLoss 1.4423 (1.4423)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [435][50/391]\tTime 0.068 (0.053)\tData 0.036 (0.020)\tLoss 1.4997 (1.3599)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [435][100/391]\tTime 0.067 (0.051)\tData 0.035 (0.019)\tLoss 1.5740 (1.3954)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [435][150/391]\tTime 0.068 (0.051)\tData 0.035 (0.018)\tLoss 0.9760 (1.3962)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [435][200/391]\tTime 0.065 (0.050)\tData 0.034 (0.018)\tLoss 1.7511 (1.3839)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [435][250/391]\tTime 0.070 (0.050)\tData 0.037 (0.018)\tLoss 1.0747 (1.3660)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [435][300/391]\tTime 0.061 (0.050)\tData 0.029 (0.018)\tLoss 1.1060 (1.3671)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [435][350/391]\tTime 0.066 (0.050)\tData 0.034 (0.018)\tLoss 1.7107 (1.3750)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.101 (0.101)\tLoss 0.7737 (0.7737)\tPrec@1 91.406 (91.406)\n",
            "Test: [50/79]\tTime 0.011 (0.016)\tLoss 0.8504 (0.8445)\tPrec@1 89.844 (89.032)\n",
            " * Prec@1 89.280\n",
            "\n",
            "Epoch: 437/600\n",
            "Learning rate: 0.017412\n",
            "Epoch: [436][0/391]\tTime 0.203 (0.203)\tData 0.158 (0.158)\tLoss 0.8723 (0.8723)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [436][50/391]\tTime 0.067 (0.052)\tData 0.034 (0.020)\tLoss 1.5728 (1.3906)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [436][100/391]\tTime 0.066 (0.051)\tData 0.034 (0.019)\tLoss 1.6017 (1.4049)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [436][150/391]\tTime 0.065 (0.051)\tData 0.022 (0.018)\tLoss 1.6756 (1.4002)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [436][200/391]\tTime 0.042 (0.052)\tData 0.000 (0.017)\tLoss 1.6143 (1.3960)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [436][250/391]\tTime 0.032 (0.052)\tData 0.000 (0.017)\tLoss 1.6761 (1.3971)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [436][300/391]\tTime 0.043 (0.052)\tData 0.011 (0.017)\tLoss 0.9298 (1.3947)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [436][350/391]\tTime 0.066 (0.051)\tData 0.032 (0.017)\tLoss 1.6860 (1.3945)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.098 (0.098)\tLoss 0.8147 (0.8147)\tPrec@1 90.625 (90.625)\n",
            "Test: [50/79]\tTime 0.020 (0.016)\tLoss 0.8056 (0.8139)\tPrec@1 92.969 (90.962)\n",
            " * Prec@1 91.150\n",
            "\n",
            "Epoch: 438/600\n",
            "Learning rate: 0.017214\n",
            "Epoch: [437][0/391]\tTime 0.205 (0.205)\tData 0.161 (0.161)\tLoss 1.3265 (1.3265)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [437][50/391]\tTime 0.067 (0.053)\tData 0.036 (0.021)\tLoss 1.2600 (1.3811)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [437][100/391]\tTime 0.063 (0.051)\tData 0.032 (0.019)\tLoss 0.9015 (1.4006)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [437][150/391]\tTime 0.065 (0.050)\tData 0.032 (0.018)\tLoss 0.9397 (1.3777)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [437][200/391]\tTime 0.066 (0.050)\tData 0.034 (0.018)\tLoss 1.3605 (1.3851)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [437][250/391]\tTime 0.032 (0.049)\tData 0.000 (0.018)\tLoss 0.8865 (1.3875)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [437][300/391]\tTime 0.033 (0.050)\tData 0.000 (0.018)\tLoss 1.4405 (1.3864)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [437][350/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.7204 (1.3951)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.096 (0.096)\tLoss 0.8240 (0.8240)\tPrec@1 90.625 (90.625)\n",
            "Test: [50/79]\tTime 0.010 (0.017)\tLoss 0.8426 (0.8409)\tPrec@1 89.844 (90.518)\n",
            " * Prec@1 90.630\n",
            "\n",
            "Epoch: 439/600\n",
            "Learning rate: 0.017017\n",
            "Epoch: [438][0/391]\tTime 0.205 (0.205)\tData 0.156 (0.156)\tLoss 1.4644 (1.4644)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [438][50/391]\tTime 0.063 (0.051)\tData 0.031 (0.019)\tLoss 1.6735 (1.4316)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [438][100/391]\tTime 0.064 (0.050)\tData 0.032 (0.018)\tLoss 1.5187 (1.4315)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [438][150/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 0.8612 (1.4158)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [438][200/391]\tTime 0.062 (0.049)\tData 0.030 (0.017)\tLoss 1.4445 (1.4086)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [438][250/391]\tTime 0.034 (0.049)\tData 0.002 (0.016)\tLoss 1.6384 (1.3993)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [438][300/391]\tTime 0.057 (0.049)\tData 0.026 (0.017)\tLoss 1.1808 (1.3977)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [438][350/391]\tTime 0.069 (0.049)\tData 0.037 (0.017)\tLoss 0.8606 (1.3953)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.095 (0.095)\tLoss 0.8110 (0.8110)\tPrec@1 90.625 (90.625)\n",
            "Test: [50/79]\tTime 0.020 (0.016)\tLoss 0.8140 (0.8394)\tPrec@1 92.188 (90.150)\n",
            " * Prec@1 90.410\n",
            "\n",
            "Epoch: 440/600\n",
            "Learning rate: 0.016822\n",
            "Epoch: [439][0/391]\tTime 0.203 (0.203)\tData 0.156 (0.156)\tLoss 0.8748 (0.8748)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [439][50/391]\tTime 0.065 (0.052)\tData 0.032 (0.019)\tLoss 0.9457 (1.3902)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [439][100/391]\tTime 0.067 (0.051)\tData 0.034 (0.019)\tLoss 1.2913 (1.3936)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [439][150/391]\tTime 0.064 (0.050)\tData 0.032 (0.018)\tLoss 0.9104 (1.3783)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [439][200/391]\tTime 0.064 (0.050)\tData 0.032 (0.018)\tLoss 1.6220 (1.3945)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [439][250/391]\tTime 0.067 (0.050)\tData 0.034 (0.018)\tLoss 1.7135 (1.4009)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [439][300/391]\tTime 0.067 (0.050)\tData 0.036 (0.017)\tLoss 1.3932 (1.4076)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [439][350/391]\tTime 0.071 (0.050)\tData 0.035 (0.017)\tLoss 1.6938 (1.4209)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.098 (0.098)\tLoss 0.7825 (0.7825)\tPrec@1 92.969 (92.969)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.8190 (0.7988)\tPrec@1 91.406 (90.656)\n",
            " * Prec@1 90.680\n",
            "\n",
            "Epoch: 441/600\n",
            "Learning rate: 0.016627\n",
            "Epoch: [440][0/391]\tTime 0.209 (0.209)\tData 0.152 (0.152)\tLoss 1.7147 (1.7147)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [440][50/391]\tTime 0.033 (0.057)\tData 0.000 (0.017)\tLoss 1.5154 (1.4037)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [440][100/391]\tTime 0.031 (0.053)\tData 0.000 (0.016)\tLoss 1.7324 (1.4076)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [440][150/391]\tTime 0.031 (0.051)\tData 0.000 (0.016)\tLoss 1.5839 (1.3848)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [440][200/391]\tTime 0.042 (0.052)\tData 0.000 (0.016)\tLoss 1.6909 (1.3608)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [440][250/391]\tTime 0.032 (0.053)\tData 0.000 (0.016)\tLoss 1.6247 (1.3659)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [440][300/391]\tTime 0.034 (0.052)\tData 0.000 (0.016)\tLoss 0.8267 (1.3723)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [440][350/391]\tTime 0.032 (0.052)\tData 0.000 (0.016)\tLoss 0.8868 (1.3690)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.7809 (0.7809)\tPrec@1 92.188 (92.188)\n",
            "Test: [50/79]\tTime 0.015 (0.016)\tLoss 0.8672 (0.8313)\tPrec@1 85.156 (89.599)\n",
            " * Prec@1 89.470\n",
            "\n",
            "Epoch: 442/600\n",
            "Learning rate: 0.016433\n",
            "Epoch: [441][0/391]\tTime 0.206 (0.206)\tData 0.156 (0.156)\tLoss 1.6415 (1.6415)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [441][50/391]\tTime 0.065 (0.051)\tData 0.034 (0.018)\tLoss 1.1915 (1.3833)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [441][100/391]\tTime 0.063 (0.049)\tData 0.029 (0.017)\tLoss 1.5813 (1.3972)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [441][150/391]\tTime 0.031 (0.049)\tData 0.000 (0.016)\tLoss 0.9919 (1.3959)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [441][200/391]\tTime 0.031 (0.049)\tData 0.000 (0.016)\tLoss 1.3143 (1.3939)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [441][250/391]\tTime 0.031 (0.049)\tData 0.000 (0.016)\tLoss 1.4527 (1.4112)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [441][300/391]\tTime 0.031 (0.049)\tData 0.000 (0.016)\tLoss 1.5474 (1.4075)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [441][350/391]\tTime 0.033 (0.049)\tData 0.000 (0.016)\tLoss 1.6833 (1.4119)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.096 (0.096)\tLoss 0.8130 (0.8130)\tPrec@1 92.969 (92.969)\n",
            "Test: [50/79]\tTime 0.016 (0.016)\tLoss 0.8392 (0.8231)\tPrec@1 87.500 (89.767)\n",
            " * Prec@1 89.940\n",
            "\n",
            "Epoch: 443/600\n",
            "Learning rate: 0.016240\n",
            "Epoch: [442][0/391]\tTime 0.202 (0.202)\tData 0.154 (0.154)\tLoss 1.6641 (1.6641)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [442][50/391]\tTime 0.066 (0.050)\tData 0.033 (0.018)\tLoss 1.6667 (1.3917)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [442][100/391]\tTime 0.064 (0.049)\tData 0.031 (0.017)\tLoss 0.9442 (1.3686)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [442][150/391]\tTime 0.061 (0.049)\tData 0.028 (0.017)\tLoss 1.5190 (1.3624)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [442][200/391]\tTime 0.062 (0.049)\tData 0.030 (0.017)\tLoss 0.9185 (1.3775)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [442][250/391]\tTime 0.063 (0.048)\tData 0.031 (0.016)\tLoss 1.4463 (1.3804)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [442][300/391]\tTime 0.064 (0.048)\tData 0.031 (0.016)\tLoss 1.4955 (1.3927)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [442][350/391]\tTime 0.064 (0.048)\tData 0.032 (0.016)\tLoss 1.0130 (1.3909)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.8925 (0.8925)\tPrec@1 89.844 (89.844)\n",
            "Test: [50/79]\tTime 0.021 (0.016)\tLoss 0.9015 (0.8847)\tPrec@1 86.719 (88.266)\n",
            " * Prec@1 88.570\n",
            "\n",
            "Epoch: 444/600\n",
            "Learning rate: 0.016048\n",
            "Epoch: [443][0/391]\tTime 0.213 (0.213)\tData 0.152 (0.152)\tLoss 0.9223 (0.9223)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [443][50/391]\tTime 0.032 (0.051)\tData 0.000 (0.019)\tLoss 1.2246 (1.3294)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [443][100/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.7125 (1.3549)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [443][150/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.7056 (1.3734)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [443][200/391]\tTime 0.037 (0.049)\tData 0.000 (0.017)\tLoss 1.6786 (1.3769)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [443][250/391]\tTime 0.034 (0.049)\tData 0.000 (0.016)\tLoss 1.6295 (1.3783)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [443][300/391]\tTime 0.074 (0.050)\tData 0.031 (0.016)\tLoss 1.7054 (1.3780)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [443][350/391]\tTime 0.066 (0.050)\tData 0.034 (0.016)\tLoss 1.5360 (1.3762)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.098 (0.098)\tLoss 0.8395 (0.8395)\tPrec@1 92.188 (92.188)\n",
            "Test: [50/79]\tTime 0.010 (0.017)\tLoss 0.8486 (0.8590)\tPrec@1 89.844 (90.732)\n",
            " * Prec@1 90.800\n",
            "\n",
            "Epoch: 445/600\n",
            "Learning rate: 0.015857\n",
            "Epoch: [444][0/391]\tTime 0.213 (0.213)\tData 0.154 (0.154)\tLoss 1.4445 (1.4445)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [444][50/391]\tTime 0.031 (0.053)\tData 0.000 (0.018)\tLoss 1.4151 (1.3981)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [444][100/391]\tTime 0.031 (0.051)\tData 0.000 (0.017)\tLoss 1.6037 (1.3945)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [444][150/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 0.9560 (1.3734)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [444][200/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.5638 (1.3792)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [444][250/391]\tTime 0.065 (0.050)\tData 0.033 (0.016)\tLoss 1.0614 (1.3921)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [444][300/391]\tTime 0.064 (0.050)\tData 0.033 (0.016)\tLoss 1.5364 (1.4012)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [444][350/391]\tTime 0.065 (0.050)\tData 0.033 (0.016)\tLoss 1.1024 (1.3938)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.096 (0.096)\tLoss 0.7718 (0.7718)\tPrec@1 92.188 (92.188)\n",
            "Test: [50/79]\tTime 0.019 (0.016)\tLoss 0.8009 (0.8018)\tPrec@1 87.500 (90.594)\n",
            " * Prec@1 90.770\n",
            "\n",
            "Epoch: 446/600\n",
            "Learning rate: 0.015667\n",
            "Epoch: [445][0/391]\tTime 0.201 (0.201)\tData 0.154 (0.154)\tLoss 1.6189 (1.6189)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [445][50/391]\tTime 0.065 (0.051)\tData 0.029 (0.018)\tLoss 1.5027 (1.3587)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [445][100/391]\tTime 0.064 (0.050)\tData 0.032 (0.017)\tLoss 1.7111 (1.3823)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [445][150/391]\tTime 0.063 (0.049)\tData 0.031 (0.017)\tLoss 1.5888 (1.4002)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [445][200/391]\tTime 0.064 (0.049)\tData 0.032 (0.016)\tLoss 1.7006 (1.3896)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [445][250/391]\tTime 0.065 (0.049)\tData 0.032 (0.016)\tLoss 1.2854 (1.3783)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [445][300/391]\tTime 0.065 (0.049)\tData 0.033 (0.016)\tLoss 1.1591 (1.3777)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [445][350/391]\tTime 0.062 (0.049)\tData 0.031 (0.016)\tLoss 1.3556 (1.3861)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.099 (0.099)\tLoss 0.8061 (0.8061)\tPrec@1 92.969 (92.969)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.7582 (0.8075)\tPrec@1 92.969 (90.579)\n",
            " * Prec@1 90.990\n",
            "\n",
            "Epoch: 447/600\n",
            "Learning rate: 0.015477\n",
            "Epoch: [446][0/391]\tTime 0.202 (0.202)\tData 0.155 (0.155)\tLoss 1.1046 (1.1046)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [446][50/391]\tTime 0.067 (0.051)\tData 0.032 (0.018)\tLoss 1.7334 (1.4262)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [446][100/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 1.5740 (1.4419)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [446][150/391]\tTime 0.061 (0.050)\tData 0.029 (0.017)\tLoss 1.4029 (1.4414)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [446][200/391]\tTime 0.063 (0.049)\tData 0.031 (0.016)\tLoss 1.4098 (1.4261)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [446][250/391]\tTime 0.063 (0.049)\tData 0.031 (0.016)\tLoss 1.2318 (1.4166)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [446][300/391]\tTime 0.065 (0.049)\tData 0.032 (0.016)\tLoss 1.6324 (1.4235)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [446][350/391]\tTime 0.073 (0.049)\tData 0.030 (0.016)\tLoss 0.9076 (1.4098)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.7886 (0.7886)\tPrec@1 91.406 (91.406)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.7876 (0.8072)\tPrec@1 92.188 (90.778)\n",
            " * Prec@1 90.960\n",
            "\n",
            "Epoch: 448/600\n",
            "Learning rate: 0.015289\n",
            "Epoch: [447][0/391]\tTime 0.208 (0.208)\tData 0.153 (0.153)\tLoss 1.5659 (1.5659)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [447][50/391]\tTime 0.076 (0.054)\tData 0.034 (0.019)\tLoss 1.0279 (1.4021)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [447][100/391]\tTime 0.065 (0.053)\tData 0.033 (0.018)\tLoss 1.3633 (1.4274)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [447][150/391]\tTime 0.057 (0.051)\tData 0.024 (0.018)\tLoss 1.6346 (1.4005)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [447][200/391]\tTime 0.066 (0.051)\tData 0.034 (0.017)\tLoss 1.4007 (1.3987)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [447][250/391]\tTime 0.068 (0.051)\tData 0.036 (0.017)\tLoss 1.6126 (1.4058)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [447][300/391]\tTime 0.067 (0.050)\tData 0.035 (0.017)\tLoss 0.9970 (1.4042)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [447][350/391]\tTime 0.067 (0.050)\tData 0.035 (0.017)\tLoss 1.4983 (1.3983)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.109 (0.109)\tLoss 0.7675 (0.7675)\tPrec@1 93.750 (93.750)\n",
            "Test: [50/79]\tTime 0.010 (0.017)\tLoss 0.7566 (0.7915)\tPrec@1 92.969 (91.575)\n",
            " * Prec@1 91.560\n",
            "\n",
            "Epoch: 449/600\n",
            "Learning rate: 0.015102\n",
            "Epoch: [448][0/391]\tTime 0.204 (0.204)\tData 0.158 (0.158)\tLoss 1.4888 (1.4888)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [448][50/391]\tTime 0.063 (0.051)\tData 0.031 (0.018)\tLoss 1.5093 (1.3930)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [448][100/391]\tTime 0.067 (0.050)\tData 0.032 (0.017)\tLoss 1.5594 (1.4177)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [448][150/391]\tTime 0.068 (0.050)\tData 0.036 (0.017)\tLoss 1.2409 (1.3859)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [448][200/391]\tTime 0.065 (0.049)\tData 0.032 (0.017)\tLoss 1.3131 (1.3764)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [448][250/391]\tTime 0.064 (0.049)\tData 0.031 (0.017)\tLoss 0.9403 (1.3647)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [448][300/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 0.9265 (1.3646)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [448][350/391]\tTime 0.033 (0.049)\tData 0.000 (0.017)\tLoss 1.6341 (1.3675)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.7803 (0.7803)\tPrec@1 92.188 (92.188)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.7657 (0.8007)\tPrec@1 93.750 (90.778)\n",
            " * Prec@1 90.810\n",
            "\n",
            "Epoch: 450/600\n",
            "Learning rate: 0.014915\n",
            "Epoch: [449][0/391]\tTime 0.212 (0.212)\tData 0.165 (0.165)\tLoss 1.6716 (1.6716)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [449][50/391]\tTime 0.066 (0.052)\tData 0.035 (0.020)\tLoss 1.1247 (1.3551)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [449][100/391]\tTime 0.070 (0.051)\tData 0.037 (0.019)\tLoss 1.6982 (1.3567)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [449][150/391]\tTime 0.066 (0.051)\tData 0.034 (0.018)\tLoss 1.4101 (1.3783)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [449][200/391]\tTime 0.060 (0.050)\tData 0.029 (0.018)\tLoss 1.6134 (1.3738)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [449][250/391]\tTime 0.059 (0.050)\tData 0.024 (0.017)\tLoss 1.5960 (1.3844)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [449][300/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.5279 (1.3859)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [449][350/391]\tTime 0.065 (0.049)\tData 0.031 (0.017)\tLoss 0.9269 (1.3820)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.8630 (0.8630)\tPrec@1 91.406 (91.406)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.8118 (0.8364)\tPrec@1 91.406 (89.997)\n",
            " * Prec@1 90.220\n",
            "\n",
            "Epoch: 451/600\n",
            "Learning rate: 0.014730\n",
            "Epoch: [450][0/391]\tTime 0.212 (0.212)\tData 0.153 (0.153)\tLoss 1.2341 (1.2341)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [450][50/391]\tTime 0.070 (0.051)\tData 0.034 (0.018)\tLoss 1.6216 (1.3358)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [450][100/391]\tTime 0.032 (0.050)\tData 0.000 (0.016)\tLoss 1.5981 (1.3416)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [450][150/391]\tTime 0.032 (0.050)\tData 0.000 (0.016)\tLoss 1.6465 (1.3466)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [450][200/391]\tTime 0.032 (0.050)\tData 0.000 (0.016)\tLoss 1.4254 (1.3511)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [450][250/391]\tTime 0.034 (0.049)\tData 0.000 (0.016)\tLoss 1.4560 (1.3656)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [450][300/391]\tTime 0.035 (0.049)\tData 0.000 (0.016)\tLoss 1.5016 (1.3689)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [450][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.7225 (1.3648)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.7841 (0.7841)\tPrec@1 92.969 (92.969)\n",
            "Test: [50/79]\tTime 0.017 (0.016)\tLoss 0.8219 (0.7993)\tPrec@1 89.844 (90.931)\n",
            " * Prec@1 90.860\n",
            "\n",
            "Epoch: 452/600\n",
            "Learning rate: 0.014546\n",
            "Epoch: [451][0/391]\tTime 0.206 (0.206)\tData 0.159 (0.159)\tLoss 1.4705 (1.4705)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [451][50/391]\tTime 0.032 (0.051)\tData 0.000 (0.019)\tLoss 1.4767 (1.4113)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [451][100/391]\tTime 0.034 (0.051)\tData 0.000 (0.018)\tLoss 1.6305 (1.3841)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [451][150/391]\tTime 0.033 (0.050)\tData 0.000 (0.017)\tLoss 1.6544 (1.3766)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [451][200/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.7143 (1.3697)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [451][250/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.3215 (1.3669)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [451][300/391]\tTime 0.035 (0.050)\tData 0.003 (0.017)\tLoss 1.6998 (1.3700)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [451][350/391]\tTime 0.044 (0.050)\tData 0.000 (0.016)\tLoss 1.3108 (1.3704)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.098 (0.098)\tLoss 0.7660 (0.7660)\tPrec@1 94.531 (94.531)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.7884 (0.8009)\tPrec@1 92.188 (91.468)\n",
            " * Prec@1 91.490\n",
            "\n",
            "Epoch: 453/600\n",
            "Learning rate: 0.014362\n",
            "Epoch: [452][0/391]\tTime 0.204 (0.204)\tData 0.156 (0.156)\tLoss 1.5948 (1.5948)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [452][50/391]\tTime 0.056 (0.051)\tData 0.023 (0.017)\tLoss 0.9475 (1.3594)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [452][100/391]\tTime 0.049 (0.050)\tData 0.017 (0.016)\tLoss 1.5905 (1.3767)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [452][150/391]\tTime 0.033 (0.049)\tData 0.000 (0.016)\tLoss 1.4033 (1.3612)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [452][200/391]\tTime 0.070 (0.049)\tData 0.037 (0.016)\tLoss 1.3496 (1.3590)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [452][250/391]\tTime 0.067 (0.049)\tData 0.034 (0.016)\tLoss 1.6199 (1.3653)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [452][300/391]\tTime 0.062 (0.049)\tData 0.029 (0.016)\tLoss 1.6613 (1.3675)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [452][350/391]\tTime 0.063 (0.049)\tData 0.030 (0.016)\tLoss 1.2628 (1.3718)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.112 (0.112)\tLoss 0.7625 (0.7625)\tPrec@1 92.188 (92.188)\n",
            "Test: [50/79]\tTime 0.019 (0.017)\tLoss 0.7722 (0.8031)\tPrec@1 95.312 (90.640)\n",
            " * Prec@1 90.870\n",
            "\n",
            "Epoch: 454/600\n",
            "Learning rate: 0.014180\n",
            "Epoch: [453][0/391]\tTime 0.202 (0.202)\tData 0.154 (0.154)\tLoss 1.6539 (1.6539)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [453][50/391]\tTime 0.066 (0.051)\tData 0.031 (0.018)\tLoss 1.7173 (1.3891)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [453][100/391]\tTime 0.064 (0.050)\tData 0.030 (0.017)\tLoss 1.0809 (1.3701)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [453][150/391]\tTime 0.063 (0.049)\tData 0.031 (0.016)\tLoss 1.6159 (1.3672)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [453][200/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 0.9703 (1.3744)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [453][250/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 0.9323 (1.3787)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [453][300/391]\tTime 0.033 (0.049)\tData 0.000 (0.016)\tLoss 1.7310 (1.3764)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [453][350/391]\tTime 0.033 (0.049)\tData 0.000 (0.016)\tLoss 1.1126 (1.3745)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.099 (0.099)\tLoss 0.7659 (0.7659)\tPrec@1 93.750 (93.750)\n",
            "Test: [50/79]\tTime 0.019 (0.016)\tLoss 0.8492 (0.8216)\tPrec@1 87.500 (89.782)\n",
            " * Prec@1 90.160\n",
            "\n",
            "Epoch: 455/600\n",
            "Learning rate: 0.013998\n",
            "Epoch: [454][0/391]\tTime 0.207 (0.207)\tData 0.160 (0.160)\tLoss 1.6812 (1.6812)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [454][50/391]\tTime 0.049 (0.051)\tData 0.017 (0.018)\tLoss 1.7015 (1.3220)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [454][100/391]\tTime 0.032 (0.051)\tData 0.000 (0.017)\tLoss 1.4396 (1.3451)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [454][150/391]\tTime 0.032 (0.051)\tData 0.000 (0.017)\tLoss 1.2506 (1.3507)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [454][200/391]\tTime 0.033 (0.050)\tData 0.000 (0.017)\tLoss 1.6206 (1.3589)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [454][250/391]\tTime 0.035 (0.050)\tData 0.000 (0.017)\tLoss 0.9381 (1.3471)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [454][300/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.4366 (1.3596)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [454][350/391]\tTime 0.033 (0.050)\tData 0.000 (0.017)\tLoss 1.3013 (1.3594)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.8160 (0.8160)\tPrec@1 91.406 (91.406)\n",
            "Test: [50/79]\tTime 0.012 (0.017)\tLoss 0.8549 (0.8286)\tPrec@1 87.500 (90.579)\n",
            " * Prec@1 90.450\n",
            "\n",
            "Epoch: 456/600\n",
            "Learning rate: 0.013818\n",
            "Epoch: [455][0/391]\tTime 0.213 (0.213)\tData 0.154 (0.154)\tLoss 1.2474 (1.2474)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [455][50/391]\tTime 0.066 (0.052)\tData 0.032 (0.019)\tLoss 1.4092 (1.4172)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [455][100/391]\tTime 0.064 (0.051)\tData 0.033 (0.017)\tLoss 1.7515 (1.4040)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [455][150/391]\tTime 0.037 (0.050)\tData 0.005 (0.017)\tLoss 1.6826 (1.3924)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [455][200/391]\tTime 0.031 (0.050)\tData 0.000 (0.017)\tLoss 1.2394 (1.3997)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [455][250/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.6194 (1.3972)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [455][300/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 0.9631 (1.3942)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [455][350/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.3324 (1.4004)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.098 (0.098)\tLoss 0.8168 (0.8168)\tPrec@1 90.625 (90.625)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.8191 (0.7992)\tPrec@1 89.062 (91.452)\n",
            " * Prec@1 91.570\n",
            "\n",
            "Epoch: 457/600\n",
            "Learning rate: 0.013638\n",
            "Epoch: [456][0/391]\tTime 0.217 (0.217)\tData 0.160 (0.160)\tLoss 1.5456 (1.5456)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [456][50/391]\tTime 0.065 (0.051)\tData 0.033 (0.019)\tLoss 0.8683 (1.3363)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [456][100/391]\tTime 0.076 (0.052)\tData 0.032 (0.018)\tLoss 0.8146 (1.3684)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [456][150/391]\tTime 0.062 (0.051)\tData 0.030 (0.017)\tLoss 1.6130 (1.3568)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [456][200/391]\tTime 0.075 (0.051)\tData 0.032 (0.017)\tLoss 1.3728 (1.3594)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [456][250/391]\tTime 0.064 (0.051)\tData 0.032 (0.016)\tLoss 1.3170 (1.3626)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [456][300/391]\tTime 0.063 (0.051)\tData 0.031 (0.016)\tLoss 1.5956 (1.3701)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [456][350/391]\tTime 0.067 (0.051)\tData 0.036 (0.016)\tLoss 1.0358 (1.3690)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.099 (0.099)\tLoss 0.8451 (0.8451)\tPrec@1 89.844 (89.844)\n",
            "Test: [50/79]\tTime 0.011 (0.016)\tLoss 0.8379 (0.8524)\tPrec@1 92.969 (90.441)\n",
            " * Prec@1 90.290\n",
            "\n",
            "Epoch: 458/600\n",
            "Learning rate: 0.013459\n",
            "Epoch: [457][0/391]\tTime 0.212 (0.212)\tData 0.153 (0.153)\tLoss 1.4771 (1.4771)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [457][50/391]\tTime 0.064 (0.052)\tData 0.031 (0.019)\tLoss 1.3963 (1.3925)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [457][100/391]\tTime 0.064 (0.050)\tData 0.031 (0.018)\tLoss 1.7720 (1.3594)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [457][150/391]\tTime 0.065 (0.051)\tData 0.033 (0.017)\tLoss 1.6932 (1.3631)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [457][200/391]\tTime 0.064 (0.050)\tData 0.032 (0.017)\tLoss 1.2666 (1.3593)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [457][250/391]\tTime 0.034 (0.050)\tData 0.002 (0.017)\tLoss 1.5085 (1.3704)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [457][300/391]\tTime 0.031 (0.050)\tData 0.000 (0.017)\tLoss 1.5066 (1.3688)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [457][350/391]\tTime 0.033 (0.050)\tData 0.000 (0.017)\tLoss 1.6598 (1.3668)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.8600 (0.8600)\tPrec@1 89.844 (89.844)\n",
            "Test: [50/79]\tTime 0.011 (0.016)\tLoss 0.8846 (0.8347)\tPrec@1 82.812 (89.476)\n",
            " * Prec@1 89.630\n",
            "\n",
            "Epoch: 459/600\n",
            "Learning rate: 0.013282\n",
            "Epoch: [458][0/391]\tTime 0.214 (0.214)\tData 0.155 (0.155)\tLoss 1.6745 (1.6745)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [458][50/391]\tTime 0.034 (0.051)\tData 0.002 (0.018)\tLoss 1.4620 (1.3512)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [458][100/391]\tTime 0.034 (0.050)\tData 0.000 (0.018)\tLoss 1.6103 (1.3999)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [458][150/391]\tTime 0.033 (0.050)\tData 0.000 (0.018)\tLoss 1.0359 (1.4063)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [458][200/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.3464 (1.4045)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [458][250/391]\tTime 0.061 (0.050)\tData 0.029 (0.017)\tLoss 0.9835 (1.4127)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [458][300/391]\tTime 0.065 (0.050)\tData 0.033 (0.018)\tLoss 0.9325 (1.4025)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [458][350/391]\tTime 0.067 (0.050)\tData 0.035 (0.018)\tLoss 1.0101 (1.4030)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.099 (0.099)\tLoss 0.7778 (0.7778)\tPrec@1 90.625 (90.625)\n",
            "Test: [50/79]\tTime 0.011 (0.016)\tLoss 0.8106 (0.8029)\tPrec@1 90.625 (90.732)\n",
            " * Prec@1 90.910\n",
            "\n",
            "Epoch: 460/600\n",
            "Learning rate: 0.013105\n",
            "Epoch: [459][0/391]\tTime 0.204 (0.204)\tData 0.158 (0.158)\tLoss 1.6158 (1.6158)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [459][50/391]\tTime 0.065 (0.052)\tData 0.033 (0.019)\tLoss 1.2911 (1.3507)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [459][100/391]\tTime 0.066 (0.050)\tData 0.034 (0.018)\tLoss 1.2400 (1.3594)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [459][150/391]\tTime 0.068 (0.050)\tData 0.037 (0.018)\tLoss 1.2614 (1.3466)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [459][200/391]\tTime 0.077 (0.050)\tData 0.042 (0.018)\tLoss 1.2364 (1.3510)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [459][250/391]\tTime 0.067 (0.050)\tData 0.036 (0.018)\tLoss 1.6761 (1.3561)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [459][300/391]\tTime 0.066 (0.050)\tData 0.034 (0.018)\tLoss 1.4953 (1.3612)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [459][350/391]\tTime 0.065 (0.050)\tData 0.033 (0.018)\tLoss 1.3148 (1.3608)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.096 (0.096)\tLoss 0.8385 (0.8385)\tPrec@1 92.188 (92.188)\n",
            "Test: [50/79]\tTime 0.020 (0.016)\tLoss 0.8117 (0.8340)\tPrec@1 89.062 (89.890)\n",
            " * Prec@1 90.000\n",
            "\n",
            "Epoch: 461/600\n",
            "Learning rate: 0.012930\n",
            "Epoch: [460][0/391]\tTime 0.200 (0.200)\tData 0.154 (0.154)\tLoss 1.0452 (1.0452)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [460][50/391]\tTime 0.067 (0.051)\tData 0.034 (0.018)\tLoss 1.5070 (1.3014)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [460][100/391]\tTime 0.067 (0.050)\tData 0.035 (0.018)\tLoss 1.6730 (1.3401)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [460][150/391]\tTime 0.063 (0.050)\tData 0.031 (0.017)\tLoss 1.5320 (1.3662)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [460][200/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.5191 (1.3618)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [460][250/391]\tTime 0.066 (0.049)\tData 0.034 (0.017)\tLoss 0.9777 (1.3602)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [460][300/391]\tTime 0.069 (0.049)\tData 0.037 (0.017)\tLoss 1.0787 (1.3628)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [460][350/391]\tTime 0.066 (0.049)\tData 0.034 (0.017)\tLoss 1.2047 (1.3578)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.098 (0.098)\tLoss 0.8078 (0.8078)\tPrec@1 91.406 (91.406)\n",
            "Test: [50/79]\tTime 0.016 (0.016)\tLoss 0.8271 (0.8250)\tPrec@1 90.625 (90.181)\n",
            " * Prec@1 90.360\n",
            "\n",
            "Epoch: 462/600\n",
            "Learning rate: 0.012755\n",
            "Epoch: [461][0/391]\tTime 0.202 (0.202)\tData 0.155 (0.155)\tLoss 1.3717 (1.3717)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [461][50/391]\tTime 0.065 (0.051)\tData 0.033 (0.018)\tLoss 1.4177 (1.3559)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [461][100/391]\tTime 0.065 (0.051)\tData 0.032 (0.017)\tLoss 1.0089 (1.3497)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [461][150/391]\tTime 0.064 (0.050)\tData 0.032 (0.017)\tLoss 1.1845 (1.3586)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [461][200/391]\tTime 0.065 (0.050)\tData 0.034 (0.017)\tLoss 1.5491 (1.3641)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [461][250/391]\tTime 0.063 (0.049)\tData 0.032 (0.017)\tLoss 1.0229 (1.3670)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [461][300/391]\tTime 0.063 (0.049)\tData 0.030 (0.017)\tLoss 1.4928 (1.3590)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [461][350/391]\tTime 0.066 (0.049)\tData 0.034 (0.017)\tLoss 1.6989 (1.3710)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.8272 (0.8272)\tPrec@1 91.406 (91.406)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.8324 (0.8132)\tPrec@1 89.844 (90.809)\n",
            " * Prec@1 90.930\n",
            "\n",
            "Epoch: 463/600\n",
            "Learning rate: 0.012582\n",
            "Epoch: [462][0/391]\tTime 0.212 (0.212)\tData 0.155 (0.155)\tLoss 1.5111 (1.5111)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [462][50/391]\tTime 0.031 (0.051)\tData 0.000 (0.019)\tLoss 1.3655 (1.3804)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [462][100/391]\tTime 0.034 (0.050)\tData 0.000 (0.018)\tLoss 0.8451 (1.3557)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [462][150/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.6656 (1.3682)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [462][200/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.2718 (1.3518)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [462][250/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.4862 (1.3604)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [462][300/391]\tTime 0.031 (0.050)\tData 0.000 (0.018)\tLoss 1.4179 (1.3581)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [462][350/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.6544 (1.3523)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.8081 (0.8081)\tPrec@1 93.750 (93.750)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.8034 (0.8227)\tPrec@1 92.188 (91.069)\n",
            " * Prec@1 91.270\n",
            "\n",
            "Epoch: 464/600\n",
            "Learning rate: 0.012410\n",
            "Epoch: [463][0/391]\tTime 0.213 (0.213)\tData 0.153 (0.153)\tLoss 1.3565 (1.3565)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [463][50/391]\tTime 0.032 (0.054)\tData 0.000 (0.018)\tLoss 1.4371 (1.3588)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [463][100/391]\tTime 0.032 (0.052)\tData 0.000 (0.018)\tLoss 1.5872 (1.3591)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [463][150/391]\tTime 0.034 (0.051)\tData 0.000 (0.017)\tLoss 0.8103 (1.3772)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [463][200/391]\tTime 0.031 (0.050)\tData 0.000 (0.017)\tLoss 1.2680 (1.3750)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [463][250/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.6947 (1.3748)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [463][300/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.4130 (1.3725)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [463][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.4984 (1.3752)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.106 (0.106)\tLoss 0.7930 (0.7930)\tPrec@1 92.969 (92.969)\n",
            "Test: [50/79]\tTime 0.020 (0.017)\tLoss 0.8336 (0.8159)\tPrec@1 90.625 (90.870)\n",
            " * Prec@1 91.160\n",
            "\n",
            "Epoch: 465/600\n",
            "Learning rate: 0.012238\n",
            "Epoch: [464][0/391]\tTime 0.199 (0.199)\tData 0.153 (0.153)\tLoss 1.2835 (1.2835)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [464][50/391]\tTime 0.065 (0.051)\tData 0.034 (0.018)\tLoss 1.5602 (1.3278)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [464][100/391]\tTime 0.064 (0.050)\tData 0.032 (0.017)\tLoss 1.0563 (1.3225)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [464][150/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.1207 (1.3416)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [464][200/391]\tTime 0.065 (0.049)\tData 0.034 (0.017)\tLoss 1.2024 (1.3635)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [464][250/391]\tTime 0.066 (0.049)\tData 0.034 (0.017)\tLoss 1.1710 (1.3661)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [464][300/391]\tTime 0.063 (0.049)\tData 0.030 (0.017)\tLoss 1.0826 (1.3620)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [464][350/391]\tTime 0.065 (0.049)\tData 0.031 (0.017)\tLoss 1.5242 (1.3667)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.096 (0.096)\tLoss 0.7789 (0.7789)\tPrec@1 93.750 (93.750)\n",
            "Test: [50/79]\tTime 0.019 (0.016)\tLoss 0.7983 (0.8034)\tPrec@1 90.625 (91.069)\n",
            " * Prec@1 91.250\n",
            "\n",
            "Epoch: 466/600\n",
            "Learning rate: 0.012068\n",
            "Epoch: [465][0/391]\tTime 0.201 (0.201)\tData 0.156 (0.156)\tLoss 1.4207 (1.4207)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [465][50/391]\tTime 0.031 (0.052)\tData 0.000 (0.019)\tLoss 0.9302 (1.3488)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [465][100/391]\tTime 0.032 (0.051)\tData 0.000 (0.018)\tLoss 1.5528 (1.3542)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [465][150/391]\tTime 0.032 (0.051)\tData 0.000 (0.018)\tLoss 1.4558 (1.3692)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [465][200/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.0328 (1.3703)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [465][250/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.0542 (1.3706)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [465][300/391]\tTime 0.033 (0.050)\tData 0.000 (0.018)\tLoss 1.6658 (1.3679)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [465][350/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.2092 (1.3678)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.7773 (0.7773)\tPrec@1 92.188 (92.188)\n",
            "Test: [50/79]\tTime 0.010 (0.016)\tLoss 0.7883 (0.8028)\tPrec@1 89.844 (90.411)\n",
            " * Prec@1 90.300\n",
            "\n",
            "Epoch: 467/600\n",
            "Learning rate: 0.011898\n",
            "Epoch: [466][0/391]\tTime 0.204 (0.204)\tData 0.156 (0.156)\tLoss 1.5666 (1.5666)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [466][50/391]\tTime 0.065 (0.051)\tData 0.033 (0.018)\tLoss 1.5635 (1.3811)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [466][100/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 1.3174 (1.3702)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [466][150/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 1.2456 (1.3852)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [466][200/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.5801 (1.3851)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [466][250/391]\tTime 0.034 (0.049)\tData 0.002 (0.017)\tLoss 1.6263 (1.3788)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [466][300/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.1457 (1.3798)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [466][350/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.6283 (1.3806)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.7462 (0.7462)\tPrec@1 93.750 (93.750)\n",
            "Test: [50/79]\tTime 0.011 (0.016)\tLoss 0.7847 (0.7946)\tPrec@1 92.188 (90.947)\n",
            " * Prec@1 91.220\n",
            "\n",
            "Epoch: 468/600\n",
            "Learning rate: 0.011730\n",
            "Epoch: [467][0/391]\tTime 0.212 (0.212)\tData 0.155 (0.155)\tLoss 1.2974 (1.2974)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [467][50/391]\tTime 0.065 (0.052)\tData 0.033 (0.018)\tLoss 0.9081 (1.3622)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [467][100/391]\tTime 0.067 (0.050)\tData 0.030 (0.017)\tLoss 1.4398 (1.3663)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [467][150/391]\tTime 0.031 (0.050)\tData 0.000 (0.017)\tLoss 1.2278 (1.3690)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [467][200/391]\tTime 0.068 (0.050)\tData 0.036 (0.017)\tLoss 1.6691 (1.3795)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [467][250/391]\tTime 0.067 (0.050)\tData 0.035 (0.017)\tLoss 1.2491 (1.3683)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [467][300/391]\tTime 0.067 (0.050)\tData 0.035 (0.017)\tLoss 1.3656 (1.3724)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [467][350/391]\tTime 0.065 (0.050)\tData 0.023 (0.017)\tLoss 1.6279 (1.3799)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.098 (0.098)\tLoss 0.8010 (0.8010)\tPrec@1 91.406 (91.406)\n",
            "Test: [50/79]\tTime 0.014 (0.016)\tLoss 0.8339 (0.8349)\tPrec@1 89.062 (89.951)\n",
            " * Prec@1 90.160\n",
            "\n",
            "Epoch: 469/600\n",
            "Learning rate: 0.011563\n",
            "Epoch: [468][0/391]\tTime 0.201 (0.201)\tData 0.155 (0.155)\tLoss 1.6118 (1.6118)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [468][50/391]\tTime 0.067 (0.051)\tData 0.034 (0.018)\tLoss 1.6187 (1.3848)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [468][100/391]\tTime 0.050 (0.050)\tData 0.009 (0.017)\tLoss 1.6715 (1.3747)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [468][150/391]\tTime 0.065 (0.050)\tData 0.031 (0.016)\tLoss 1.1089 (1.3816)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [468][200/391]\tTime 0.065 (0.050)\tData 0.033 (0.016)\tLoss 1.4881 (1.3887)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [468][250/391]\tTime 0.063 (0.050)\tData 0.031 (0.016)\tLoss 1.5495 (1.3807)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [468][300/391]\tTime 0.064 (0.049)\tData 0.032 (0.016)\tLoss 1.4258 (1.3838)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [468][350/391]\tTime 0.066 (0.049)\tData 0.033 (0.016)\tLoss 0.9429 (1.3753)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.102 (0.102)\tLoss 0.8123 (0.8123)\tPrec@1 92.188 (92.188)\n",
            "Test: [50/79]\tTime 0.010 (0.017)\tLoss 0.7676 (0.8063)\tPrec@1 92.969 (90.441)\n",
            " * Prec@1 90.680\n",
            "\n",
            "Epoch: 470/600\n",
            "Learning rate: 0.011397\n",
            "Epoch: [469][0/391]\tTime 0.200 (0.200)\tData 0.153 (0.153)\tLoss 1.5037 (1.5037)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [469][50/391]\tTime 0.064 (0.051)\tData 0.032 (0.018)\tLoss 0.9717 (1.3108)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [469][100/391]\tTime 0.063 (0.050)\tData 0.031 (0.017)\tLoss 1.5909 (1.3488)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [469][150/391]\tTime 0.063 (0.049)\tData 0.032 (0.017)\tLoss 0.9629 (1.3619)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [469][200/391]\tTime 0.063 (0.049)\tData 0.032 (0.017)\tLoss 1.6174 (1.3583)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [469][250/391]\tTime 0.062 (0.049)\tData 0.031 (0.016)\tLoss 1.5736 (1.3561)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [469][300/391]\tTime 0.065 (0.049)\tData 0.033 (0.016)\tLoss 1.5260 (1.3635)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [469][350/391]\tTime 0.063 (0.048)\tData 0.031 (0.016)\tLoss 1.2581 (1.3668)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.7780 (0.7780)\tPrec@1 92.969 (92.969)\n",
            "Test: [50/79]\tTime 0.019 (0.016)\tLoss 0.8273 (0.8017)\tPrec@1 89.062 (91.605)\n",
            " * Prec@1 91.760\n",
            "\n",
            "Epoch: 471/600\n",
            "Learning rate: 0.011232\n",
            "Epoch: [470][0/391]\tTime 0.216 (0.216)\tData 0.157 (0.157)\tLoss 1.5633 (1.5633)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [470][50/391]\tTime 0.064 (0.052)\tData 0.031 (0.018)\tLoss 1.0440 (1.3410)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [470][100/391]\tTime 0.066 (0.050)\tData 0.035 (0.017)\tLoss 1.6130 (1.3406)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [470][150/391]\tTime 0.067 (0.050)\tData 0.035 (0.017)\tLoss 1.7490 (1.3409)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [470][200/391]\tTime 0.068 (0.050)\tData 0.035 (0.017)\tLoss 1.6159 (1.3318)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [470][250/391]\tTime 0.065 (0.050)\tData 0.030 (0.017)\tLoss 1.6154 (1.3429)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [470][300/391]\tTime 0.067 (0.050)\tData 0.035 (0.017)\tLoss 1.1862 (1.3413)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [470][350/391]\tTime 0.067 (0.050)\tData 0.035 (0.017)\tLoss 1.4104 (1.3391)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.096 (0.096)\tLoss 0.8361 (0.8361)\tPrec@1 90.625 (90.625)\n",
            "Test: [50/79]\tTime 0.017 (0.016)\tLoss 0.8076 (0.8294)\tPrec@1 91.406 (90.855)\n",
            " * Prec@1 91.030\n",
            "\n",
            "Epoch: 472/600\n",
            "Learning rate: 0.011068\n",
            "Epoch: [471][0/391]\tTime 0.203 (0.203)\tData 0.157 (0.157)\tLoss 1.2269 (1.2269)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [471][50/391]\tTime 0.066 (0.051)\tData 0.029 (0.019)\tLoss 1.3485 (1.3513)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [471][100/391]\tTime 0.066 (0.050)\tData 0.035 (0.017)\tLoss 1.1775 (1.3436)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [471][150/391]\tTime 0.064 (0.049)\tData 0.033 (0.017)\tLoss 1.0646 (1.3397)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [471][200/391]\tTime 0.062 (0.049)\tData 0.030 (0.017)\tLoss 1.7106 (1.3465)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [471][250/391]\tTime 0.054 (0.049)\tData 0.021 (0.016)\tLoss 1.0042 (1.3488)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [471][300/391]\tTime 0.039 (0.049)\tData 0.005 (0.016)\tLoss 1.6153 (1.3550)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [471][350/391]\tTime 0.037 (0.049)\tData 0.005 (0.016)\tLoss 1.5629 (1.3494)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.099 (0.099)\tLoss 0.7868 (0.7868)\tPrec@1 93.750 (93.750)\n",
            "Test: [50/79]\tTime 0.017 (0.016)\tLoss 0.7933 (0.8222)\tPrec@1 92.969 (91.284)\n",
            " * Prec@1 91.360\n",
            "\n",
            "Epoch: 473/600\n",
            "Learning rate: 0.010905\n",
            "Epoch: [472][0/391]\tTime 0.201 (0.201)\tData 0.154 (0.154)\tLoss 1.5653 (1.5653)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [472][50/391]\tTime 0.064 (0.051)\tData 0.032 (0.018)\tLoss 1.4962 (1.3014)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [472][100/391]\tTime 0.057 (0.050)\tData 0.019 (0.017)\tLoss 1.5751 (1.3230)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [472][150/391]\tTime 0.051 (0.049)\tData 0.019 (0.016)\tLoss 1.6866 (1.3175)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [472][200/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.6333 (1.3421)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [472][250/391]\tTime 0.042 (0.049)\tData 0.000 (0.017)\tLoss 1.6133 (1.3603)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [472][300/391]\tTime 0.033 (0.051)\tData 0.000 (0.016)\tLoss 1.4874 (1.3574)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [472][350/391]\tTime 0.036 (0.050)\tData 0.000 (0.016)\tLoss 0.8934 (1.3607)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.8157 (0.8157)\tPrec@1 91.406 (91.406)\n",
            "Test: [50/79]\tTime 0.019 (0.016)\tLoss 0.7337 (0.7994)\tPrec@1 96.875 (91.820)\n",
            " * Prec@1 91.860\n",
            "\n",
            "Epoch: 474/600\n",
            "Learning rate: 0.010743\n",
            "Epoch: [473][0/391]\tTime 0.206 (0.206)\tData 0.159 (0.159)\tLoss 1.5029 (1.5029)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [473][50/391]\tTime 0.059 (0.050)\tData 0.027 (0.018)\tLoss 1.2828 (1.3973)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [473][100/391]\tTime 0.034 (0.049)\tData 0.003 (0.017)\tLoss 1.3674 (1.3992)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [473][150/391]\tTime 0.035 (0.049)\tData 0.000 (0.017)\tLoss 1.4178 (1.3892)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [473][200/391]\tTime 0.031 (0.049)\tData 0.000 (0.016)\tLoss 0.8711 (1.3774)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [473][250/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 1.2603 (1.3808)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [473][300/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 0.7687 (1.3668)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [473][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 0.9061 (1.3528)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.099 (0.099)\tLoss 0.7867 (0.7867)\tPrec@1 93.750 (93.750)\n",
            "Test: [50/79]\tTime 0.011 (0.018)\tLoss 0.8016 (0.8078)\tPrec@1 91.406 (91.835)\n",
            " * Prec@1 91.850\n",
            "\n",
            "Epoch: 475/600\n",
            "Learning rate: 0.010582\n",
            "Epoch: [474][0/391]\tTime 0.202 (0.202)\tData 0.155 (0.155)\tLoss 1.6058 (1.6058)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [474][50/391]\tTime 0.063 (0.051)\tData 0.031 (0.017)\tLoss 0.9522 (1.3407)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [474][100/391]\tTime 0.033 (0.051)\tData 0.000 (0.016)\tLoss 1.6637 (1.3254)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [474][150/391]\tTime 0.032 (0.050)\tData 0.000 (0.016)\tLoss 1.2245 (1.3428)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [474][200/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 0.8450 (1.3510)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [474][250/391]\tTime 0.031 (0.049)\tData 0.000 (0.016)\tLoss 1.4416 (1.3545)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [474][300/391]\tTime 0.068 (0.049)\tData 0.036 (0.016)\tLoss 1.1880 (1.3576)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [474][350/391]\tTime 0.069 (0.049)\tData 0.037 (0.016)\tLoss 1.3001 (1.3621)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.105 (0.105)\tLoss 0.7817 (0.7817)\tPrec@1 92.969 (92.969)\n",
            "Test: [50/79]\tTime 0.017 (0.017)\tLoss 0.8443 (0.8076)\tPrec@1 89.844 (91.131)\n",
            " * Prec@1 91.530\n",
            "\n",
            "Epoch: 476/600\n",
            "Learning rate: 0.010422\n",
            "Epoch: [475][0/391]\tTime 0.203 (0.203)\tData 0.156 (0.156)\tLoss 1.3988 (1.3988)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [475][50/391]\tTime 0.054 (0.051)\tData 0.022 (0.018)\tLoss 1.0152 (1.3713)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [475][100/391]\tTime 0.059 (0.050)\tData 0.026 (0.017)\tLoss 1.5751 (1.3789)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [475][150/391]\tTime 0.059 (0.049)\tData 0.026 (0.017)\tLoss 1.3103 (1.3705)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [475][200/391]\tTime 0.042 (0.049)\tData 0.011 (0.017)\tLoss 1.3944 (1.3599)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [475][250/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.4210 (1.3696)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [475][300/391]\tTime 0.031 (0.049)\tData 0.000 (0.016)\tLoss 1.1893 (1.3765)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [475][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.3334 (1.3793)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.8005 (0.8005)\tPrec@1 91.406 (91.406)\n",
            "Test: [50/79]\tTime 0.011 (0.016)\tLoss 0.7727 (0.7795)\tPrec@1 92.188 (91.452)\n",
            " * Prec@1 91.780\n",
            "\n",
            "Epoch: 477/600\n",
            "Learning rate: 0.010263\n",
            "Epoch: [476][0/391]\tTime 0.201 (0.201)\tData 0.155 (0.155)\tLoss 1.0705 (1.0705)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [476][50/391]\tTime 0.075 (0.051)\tData 0.030 (0.018)\tLoss 0.7606 (1.3418)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [476][100/391]\tTime 0.032 (0.050)\tData 0.000 (0.016)\tLoss 1.5752 (1.3344)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [476][150/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.6641 (1.3524)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [476][200/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.4992 (1.3571)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [476][250/391]\tTime 0.031 (0.049)\tData 0.000 (0.016)\tLoss 1.3524 (1.3613)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [476][300/391]\tTime 0.033 (0.049)\tData 0.000 (0.016)\tLoss 0.9545 (1.3522)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [476][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.6471 (1.3550)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.8178 (0.8178)\tPrec@1 92.188 (92.188)\n",
            "Test: [50/79]\tTime 0.010 (0.016)\tLoss 0.7445 (0.7821)\tPrec@1 91.406 (91.544)\n",
            " * Prec@1 91.690\n",
            "\n",
            "Epoch: 478/600\n",
            "Learning rate: 0.010106\n",
            "Epoch: [477][0/391]\tTime 0.215 (0.215)\tData 0.154 (0.154)\tLoss 0.7985 (0.7985)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [477][50/391]\tTime 0.065 (0.051)\tData 0.033 (0.019)\tLoss 1.3999 (1.2766)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [477][100/391]\tTime 0.039 (0.050)\tData 0.007 (0.018)\tLoss 1.5963 (1.3173)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [477][150/391]\tTime 0.031 (0.050)\tData 0.000 (0.018)\tLoss 1.2166 (1.3473)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [477][200/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.2227 (1.3571)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [477][250/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.5693 (1.3560)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [477][300/391]\tTime 0.031 (0.050)\tData 0.000 (0.018)\tLoss 1.5504 (1.3561)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [477][350/391]\tTime 0.033 (0.050)\tData 0.000 (0.018)\tLoss 1.2756 (1.3662)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.098 (0.098)\tLoss 0.7860 (0.7860)\tPrec@1 92.969 (92.969)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.7397 (0.7757)\tPrec@1 93.750 (92.218)\n",
            " * Prec@1 92.430\n",
            "\n",
            "Epoch: 479/600\n",
            "Learning rate: 0.009949\n",
            "Epoch: [478][0/391]\tTime 0.219 (0.219)\tData 0.163 (0.163)\tLoss 1.6842 (1.6842)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [478][50/391]\tTime 0.032 (0.054)\tData 0.000 (0.017)\tLoss 0.8525 (1.3872)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [478][100/391]\tTime 0.034 (0.051)\tData 0.000 (0.017)\tLoss 1.4333 (1.3491)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [478][150/391]\tTime 0.033 (0.050)\tData 0.000 (0.016)\tLoss 1.2528 (1.3482)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [478][200/391]\tTime 0.032 (0.050)\tData 0.000 (0.016)\tLoss 0.9159 (1.3626)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [478][250/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.6642 (1.3670)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [478][300/391]\tTime 0.031 (0.049)\tData 0.000 (0.016)\tLoss 1.1148 (1.3726)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [478][350/391]\tTime 0.033 (0.049)\tData 0.000 (0.016)\tLoss 1.5204 (1.3654)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.8102 (0.8102)\tPrec@1 92.969 (92.969)\n",
            "Test: [50/79]\tTime 0.014 (0.016)\tLoss 0.7480 (0.7787)\tPrec@1 95.312 (92.984)\n",
            " * Prec@1 92.910\n",
            "\n",
            "Epoch: 480/600\n",
            "Learning rate: 0.009794\n",
            "Epoch: [479][0/391]\tTime 0.205 (0.205)\tData 0.157 (0.157)\tLoss 1.7033 (1.7033)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [479][50/391]\tTime 0.069 (0.052)\tData 0.036 (0.019)\tLoss 1.6075 (1.3566)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [479][100/391]\tTime 0.068 (0.051)\tData 0.035 (0.018)\tLoss 1.4429 (1.3627)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [479][150/391]\tTime 0.065 (0.050)\tData 0.033 (0.018)\tLoss 0.9868 (1.3739)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [479][200/391]\tTime 0.069 (0.050)\tData 0.037 (0.018)\tLoss 0.8697 (1.3683)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [479][250/391]\tTime 0.068 (0.050)\tData 0.037 (0.018)\tLoss 1.3045 (1.3679)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [479][300/391]\tTime 0.066 (0.050)\tData 0.035 (0.018)\tLoss 1.6435 (1.3741)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [479][350/391]\tTime 0.066 (0.050)\tData 0.034 (0.018)\tLoss 1.1195 (1.3621)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.098 (0.098)\tLoss 0.7689 (0.7689)\tPrec@1 92.188 (92.188)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.7483 (0.7765)\tPrec@1 93.750 (91.697)\n",
            " * Prec@1 91.630\n",
            "\n",
            "Epoch: 481/600\n",
            "Learning rate: 0.009640\n",
            "Epoch: [480][0/391]\tTime 0.209 (0.209)\tData 0.161 (0.161)\tLoss 1.1653 (1.1653)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [480][50/391]\tTime 0.066 (0.052)\tData 0.033 (0.019)\tLoss 1.5988 (1.4113)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [480][100/391]\tTime 0.067 (0.050)\tData 0.035 (0.018)\tLoss 1.5920 (1.3847)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [480][150/391]\tTime 0.068 (0.051)\tData 0.036 (0.018)\tLoss 1.4914 (1.3626)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [480][200/391]\tTime 0.068 (0.050)\tData 0.036 (0.018)\tLoss 1.4328 (1.3545)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [480][250/391]\tTime 0.065 (0.050)\tData 0.031 (0.018)\tLoss 1.0135 (1.3524)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [480][300/391]\tTime 0.067 (0.050)\tData 0.035 (0.018)\tLoss 1.3994 (1.3407)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [480][350/391]\tTime 0.070 (0.050)\tData 0.038 (0.018)\tLoss 0.8563 (1.3392)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.8059 (0.8059)\tPrec@1 92.188 (92.188)\n",
            "Test: [50/79]\tTime 0.010 (0.016)\tLoss 0.8031 (0.8083)\tPrec@1 91.406 (91.422)\n",
            " * Prec@1 91.270\n",
            "\n",
            "Epoch: 482/600\n",
            "Learning rate: 0.009486\n",
            "Epoch: [481][0/391]\tTime 0.203 (0.203)\tData 0.157 (0.157)\tLoss 1.0009 (1.0009)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [481][50/391]\tTime 0.060 (0.051)\tData 0.028 (0.018)\tLoss 1.3647 (1.3547)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [481][100/391]\tTime 0.065 (0.050)\tData 0.034 (0.017)\tLoss 1.0426 (1.3463)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [481][150/391]\tTime 0.063 (0.049)\tData 0.031 (0.017)\tLoss 1.5171 (1.3697)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [481][200/391]\tTime 0.063 (0.049)\tData 0.031 (0.017)\tLoss 0.9181 (1.3773)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [481][250/391]\tTime 0.063 (0.049)\tData 0.031 (0.017)\tLoss 1.5555 (1.3735)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [481][300/391]\tTime 0.063 (0.049)\tData 0.030 (0.017)\tLoss 1.6396 (1.3754)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [481][350/391]\tTime 0.066 (0.049)\tData 0.034 (0.017)\tLoss 0.8862 (1.3684)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.100 (0.100)\tLoss 0.7761 (0.7761)\tPrec@1 93.750 (93.750)\n",
            "Test: [50/79]\tTime 0.020 (0.016)\tLoss 0.7704 (0.8044)\tPrec@1 92.188 (91.468)\n",
            " * Prec@1 91.330\n",
            "\n",
            "Epoch: 483/600\n",
            "Learning rate: 0.009334\n",
            "Epoch: [482][0/391]\tTime 0.202 (0.202)\tData 0.154 (0.154)\tLoss 1.0583 (1.0583)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [482][50/391]\tTime 0.065 (0.051)\tData 0.032 (0.018)\tLoss 0.9761 (1.3210)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [482][100/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.6276 (1.3360)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [482][150/391]\tTime 0.054 (0.049)\tData 0.022 (0.017)\tLoss 1.6516 (1.3621)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [482][200/391]\tTime 0.064 (0.049)\tData 0.033 (0.017)\tLoss 1.0067 (1.3594)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [482][250/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 1.6245 (1.3700)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [482][300/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.6226 (1.3729)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [482][350/391]\tTime 0.034 (0.049)\tData 0.000 (0.017)\tLoss 1.2481 (1.3642)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.099 (0.099)\tLoss 0.8210 (0.8210)\tPrec@1 90.625 (90.625)\n",
            "Test: [50/79]\tTime 0.019 (0.016)\tLoss 0.8306 (0.8151)\tPrec@1 90.625 (91.376)\n",
            " * Prec@1 91.450\n",
            "\n",
            "Epoch: 484/600\n",
            "Learning rate: 0.009183\n",
            "Epoch: [483][0/391]\tTime 0.198 (0.198)\tData 0.152 (0.152)\tLoss 1.6797 (1.6797)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [483][50/391]\tTime 0.063 (0.051)\tData 0.031 (0.018)\tLoss 1.4632 (1.3359)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [483][100/391]\tTime 0.067 (0.050)\tData 0.034 (0.018)\tLoss 1.3723 (1.3583)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [483][150/391]\tTime 0.065 (0.050)\tData 0.028 (0.017)\tLoss 0.9952 (1.3694)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [483][200/391]\tTime 0.068 (0.050)\tData 0.035 (0.017)\tLoss 1.6515 (1.3747)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [483][250/391]\tTime 0.062 (0.049)\tData 0.030 (0.017)\tLoss 1.4733 (1.3689)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [483][300/391]\tTime 0.063 (0.049)\tData 0.032 (0.017)\tLoss 0.9695 (1.3627)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [483][350/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.6317 (1.3640)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.102 (0.102)\tLoss 0.7553 (0.7553)\tPrec@1 94.531 (94.531)\n",
            "Test: [50/79]\tTime 0.020 (0.017)\tLoss 0.8085 (0.7857)\tPrec@1 89.062 (91.268)\n",
            " * Prec@1 91.360\n",
            "\n",
            "Epoch: 485/600\n",
            "Learning rate: 0.009034\n",
            "Epoch: [484][0/391]\tTime 0.211 (0.211)\tData 0.153 (0.153)\tLoss 1.4466 (1.4466)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [484][50/391]\tTime 0.033 (0.059)\tData 0.000 (0.018)\tLoss 0.8442 (1.3716)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [484][100/391]\tTime 0.032 (0.054)\tData 0.000 (0.017)\tLoss 1.6134 (1.4013)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [484][150/391]\tTime 0.033 (0.052)\tData 0.000 (0.017)\tLoss 1.2351 (1.3845)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [484][200/391]\tTime 0.032 (0.051)\tData 0.000 (0.016)\tLoss 1.6088 (1.3629)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [484][250/391]\tTime 0.074 (0.052)\tData 0.031 (0.016)\tLoss 0.7499 (1.3621)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [484][300/391]\tTime 0.059 (0.052)\tData 0.027 (0.016)\tLoss 1.1984 (1.3560)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [484][350/391]\tTime 0.062 (0.052)\tData 0.030 (0.016)\tLoss 1.5031 (1.3610)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.098 (0.098)\tLoss 0.7514 (0.7514)\tPrec@1 94.531 (94.531)\n",
            "Test: [50/79]\tTime 0.022 (0.017)\tLoss 0.7917 (0.7752)\tPrec@1 88.281 (91.805)\n",
            " * Prec@1 92.000\n",
            "\n",
            "Epoch: 486/600\n",
            "Learning rate: 0.008885\n",
            "Epoch: [485][0/391]\tTime 0.209 (0.209)\tData 0.160 (0.160)\tLoss 1.3873 (1.3873)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [485][50/391]\tTime 0.065 (0.051)\tData 0.032 (0.018)\tLoss 1.0997 (1.3391)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [485][100/391]\tTime 0.064 (0.050)\tData 0.032 (0.017)\tLoss 1.3067 (1.3296)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [485][150/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.0429 (1.3416)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [485][200/391]\tTime 0.031 (0.049)\tData 0.000 (0.016)\tLoss 1.6014 (1.3404)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [485][250/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.6571 (1.3358)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [485][300/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 0.9599 (1.3408)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [485][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.3346 (1.3428)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.099 (0.099)\tLoss 0.7674 (0.7674)\tPrec@1 91.406 (91.406)\n",
            "Test: [50/79]\tTime 0.020 (0.016)\tLoss 0.7739 (0.7643)\tPrec@1 92.969 (92.341)\n",
            " * Prec@1 92.480\n",
            "\n",
            "Epoch: 487/600\n",
            "Learning rate: 0.008737\n",
            "Epoch: [486][0/391]\tTime 0.210 (0.210)\tData 0.154 (0.154)\tLoss 1.5857 (1.5857)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [486][50/391]\tTime 0.034 (0.054)\tData 0.000 (0.016)\tLoss 1.1954 (1.3282)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [486][100/391]\tTime 0.046 (0.052)\tData 0.014 (0.017)\tLoss 1.4328 (1.3375)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [486][150/391]\tTime 0.065 (0.051)\tData 0.033 (0.017)\tLoss 1.6504 (1.3390)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [486][200/391]\tTime 0.065 (0.050)\tData 0.033 (0.016)\tLoss 1.5942 (1.3514)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [486][250/391]\tTime 0.065 (0.050)\tData 0.033 (0.016)\tLoss 0.9455 (1.3600)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [486][300/391]\tTime 0.066 (0.050)\tData 0.034 (0.016)\tLoss 1.5295 (1.3607)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [486][350/391]\tTime 0.064 (0.049)\tData 0.032 (0.016)\tLoss 1.6210 (1.3613)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.098 (0.098)\tLoss 0.7594 (0.7594)\tPrec@1 92.188 (92.188)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.7252 (0.7724)\tPrec@1 95.312 (92.877)\n",
            " * Prec@1 92.750\n",
            "\n",
            "Epoch: 488/600\n",
            "Learning rate: 0.008591\n",
            "Epoch: [487][0/391]\tTime 0.198 (0.198)\tData 0.153 (0.153)\tLoss 1.5819 (1.5819)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [487][50/391]\tTime 0.065 (0.051)\tData 0.031 (0.017)\tLoss 0.9584 (1.4105)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [487][100/391]\tTime 0.066 (0.050)\tData 0.032 (0.017)\tLoss 1.2605 (1.3882)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [487][150/391]\tTime 0.065 (0.049)\tData 0.031 (0.017)\tLoss 1.5128 (1.3761)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [487][200/391]\tTime 0.065 (0.049)\tData 0.034 (0.017)\tLoss 1.3646 (1.3750)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [487][250/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.2536 (1.3637)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [487][300/391]\tTime 0.042 (0.050)\tData 0.000 (0.016)\tLoss 1.2560 (1.3652)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [487][350/391]\tTime 0.063 (0.050)\tData 0.031 (0.016)\tLoss 1.6718 (1.3626)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.098 (0.098)\tLoss 0.7585 (0.7585)\tPrec@1 94.531 (94.531)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.7820 (0.7831)\tPrec@1 92.969 (92.050)\n",
            " * Prec@1 92.110\n",
            "\n",
            "Epoch: 489/600\n",
            "Learning rate: 0.008446\n",
            "Epoch: [488][0/391]\tTime 0.198 (0.198)\tData 0.152 (0.152)\tLoss 0.7845 (0.7845)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [488][50/391]\tTime 0.031 (0.051)\tData 0.000 (0.018)\tLoss 1.4969 (1.2840)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [488][100/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 0.9109 (1.3015)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [488][150/391]\tTime 0.042 (0.050)\tData 0.000 (0.017)\tLoss 1.5961 (1.3330)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [488][200/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.1337 (1.3219)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [488][250/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.4452 (1.3298)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [488][300/391]\tTime 0.031 (0.050)\tData 0.000 (0.017)\tLoss 1.6053 (1.3313)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [488][350/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.0906 (1.3239)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.102 (0.102)\tLoss 0.7549 (0.7549)\tPrec@1 93.750 (93.750)\n",
            "Test: [50/79]\tTime 0.017 (0.016)\tLoss 0.7427 (0.7676)\tPrec@1 92.969 (92.356)\n",
            " * Prec@1 92.580\n",
            "\n",
            "Epoch: 490/600\n",
            "Learning rate: 0.008301\n",
            "Epoch: [489][0/391]\tTime 0.213 (0.213)\tData 0.156 (0.156)\tLoss 1.5669 (1.5669)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [489][50/391]\tTime 0.065 (0.051)\tData 0.034 (0.018)\tLoss 1.2969 (1.3676)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [489][100/391]\tTime 0.063 (0.049)\tData 0.031 (0.017)\tLoss 1.5900 (1.3477)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [489][150/391]\tTime 0.066 (0.049)\tData 0.034 (0.017)\tLoss 1.0372 (1.3473)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [489][200/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 0.8892 (1.3353)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [489][250/391]\tTime 0.063 (0.049)\tData 0.031 (0.016)\tLoss 1.5174 (1.3345)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [489][300/391]\tTime 0.064 (0.049)\tData 0.032 (0.016)\tLoss 1.5761 (1.3420)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [489][350/391]\tTime 0.064 (0.049)\tData 0.032 (0.016)\tLoss 1.0260 (1.3465)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.099 (0.099)\tLoss 0.7910 (0.7910)\tPrec@1 89.062 (89.062)\n",
            "Test: [50/79]\tTime 0.022 (0.017)\tLoss 0.7642 (0.7795)\tPrec@1 92.188 (92.218)\n",
            " * Prec@1 92.390\n",
            "\n",
            "Epoch: 491/600\n",
            "Learning rate: 0.008158\n",
            "Epoch: [490][0/391]\tTime 0.212 (0.212)\tData 0.153 (0.153)\tLoss 1.5942 (1.5942)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [490][50/391]\tTime 0.065 (0.051)\tData 0.033 (0.018)\tLoss 1.7031 (1.3675)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [490][100/391]\tTime 0.066 (0.050)\tData 0.034 (0.017)\tLoss 1.6237 (1.3233)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [490][150/391]\tTime 0.065 (0.049)\tData 0.034 (0.017)\tLoss 1.1941 (1.3073)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [490][200/391]\tTime 0.065 (0.049)\tData 0.032 (0.017)\tLoss 1.6102 (1.3257)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [490][250/391]\tTime 0.063 (0.049)\tData 0.032 (0.016)\tLoss 1.5121 (1.3211)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [490][300/391]\tTime 0.065 (0.049)\tData 0.033 (0.016)\tLoss 1.3174 (1.3258)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [490][350/391]\tTime 0.065 (0.049)\tData 0.033 (0.016)\tLoss 0.8419 (1.3333)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.098 (0.098)\tLoss 0.7041 (0.7041)\tPrec@1 93.750 (93.750)\n",
            "Test: [50/79]\tTime 0.016 (0.016)\tLoss 0.7539 (0.7470)\tPrec@1 92.969 (92.448)\n",
            " * Prec@1 92.650\n",
            "\n",
            "Epoch: 492/600\n",
            "Learning rate: 0.008017\n",
            "Epoch: [491][0/391]\tTime 0.204 (0.204)\tData 0.158 (0.158)\tLoss 0.9012 (0.9012)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [491][50/391]\tTime 0.059 (0.051)\tData 0.028 (0.018)\tLoss 0.7771 (1.3809)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [491][100/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.4251 (1.3563)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [491][150/391]\tTime 0.067 (0.049)\tData 0.035 (0.017)\tLoss 0.8905 (1.3597)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [491][200/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.6111 (1.3685)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [491][250/391]\tTime 0.066 (0.049)\tData 0.033 (0.017)\tLoss 0.8924 (1.3653)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [491][300/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.6739 (1.3644)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [491][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.5940 (1.3587)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.098 (0.098)\tLoss 0.7582 (0.7582)\tPrec@1 92.969 (92.969)\n",
            "Test: [50/79]\tTime 0.010 (0.017)\tLoss 0.7550 (0.7717)\tPrec@1 92.969 (92.111)\n",
            " * Prec@1 92.070\n",
            "\n",
            "Epoch: 493/600\n",
            "Learning rate: 0.007876\n",
            "Epoch: [492][0/391]\tTime 0.199 (0.199)\tData 0.152 (0.152)\tLoss 0.9220 (0.9220)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [492][50/391]\tTime 0.031 (0.051)\tData 0.000 (0.018)\tLoss 1.6487 (1.3475)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [492][100/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.4951 (1.3351)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [492][150/391]\tTime 0.064 (0.050)\tData 0.032 (0.017)\tLoss 1.2995 (1.3405)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [492][200/391]\tTime 0.062 (0.050)\tData 0.031 (0.017)\tLoss 1.5404 (1.3270)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [492][250/391]\tTime 0.063 (0.050)\tData 0.031 (0.017)\tLoss 1.5839 (1.3383)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [492][300/391]\tTime 0.066 (0.049)\tData 0.035 (0.017)\tLoss 1.6810 (1.3317)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [492][350/391]\tTime 0.062 (0.049)\tData 0.029 (0.017)\tLoss 0.8388 (1.3273)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.099 (0.099)\tLoss 0.7698 (0.7698)\tPrec@1 92.969 (92.969)\n",
            "Test: [50/79]\tTime 0.010 (0.017)\tLoss 0.7562 (0.7776)\tPrec@1 94.531 (92.525)\n",
            " * Prec@1 92.630\n",
            "\n",
            "Epoch: 494/600\n",
            "Learning rate: 0.007736\n",
            "Epoch: [493][0/391]\tTime 0.200 (0.200)\tData 0.155 (0.155)\tLoss 0.7898 (0.7898)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [493][50/391]\tTime 0.065 (0.051)\tData 0.033 (0.018)\tLoss 1.6061 (1.3130)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [493][100/391]\tTime 0.065 (0.049)\tData 0.031 (0.017)\tLoss 1.3421 (1.3465)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [493][150/391]\tTime 0.060 (0.049)\tData 0.027 (0.016)\tLoss 1.3352 (1.3199)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [493][200/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.6298 (1.3172)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [493][250/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.4598 (1.3240)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [493][300/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 0.9508 (1.3277)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [493][350/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 1.3613 (1.3227)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.101 (0.101)\tLoss 0.7938 (0.7938)\tPrec@1 91.406 (91.406)\n",
            "Test: [50/79]\tTime 0.010 (0.016)\tLoss 0.7196 (0.7682)\tPrec@1 95.312 (92.034)\n",
            " * Prec@1 91.950\n",
            "\n",
            "Epoch: 495/600\n",
            "Learning rate: 0.007598\n",
            "Epoch: [494][0/391]\tTime 0.218 (0.218)\tData 0.157 (0.157)\tLoss 1.3548 (1.3548)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [494][50/391]\tTime 0.048 (0.051)\tData 0.016 (0.018)\tLoss 1.3230 (1.4040)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [494][100/391]\tTime 0.046 (0.049)\tData 0.014 (0.017)\tLoss 1.3057 (1.3676)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [494][150/391]\tTime 0.063 (0.050)\tData 0.031 (0.017)\tLoss 1.0116 (1.3650)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [494][200/391]\tTime 0.069 (0.049)\tData 0.034 (0.017)\tLoss 1.0059 (1.3371)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [494][250/391]\tTime 0.064 (0.050)\tData 0.032 (0.017)\tLoss 1.5176 (1.3356)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [494][300/391]\tTime 0.063 (0.049)\tData 0.030 (0.017)\tLoss 1.6486 (1.3412)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [494][350/391]\tTime 0.054 (0.049)\tData 0.012 (0.017)\tLoss 1.4589 (1.3445)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.106 (0.106)\tLoss 0.7257 (0.7257)\tPrec@1 96.094 (96.094)\n",
            "Test: [50/79]\tTime 0.011 (0.016)\tLoss 0.7420 (0.7660)\tPrec@1 95.312 (92.662)\n",
            " * Prec@1 92.720\n",
            "\n",
            "Epoch: 496/600\n",
            "Learning rate: 0.007461\n",
            "Epoch: [495][0/391]\tTime 0.200 (0.200)\tData 0.153 (0.153)\tLoss 1.4337 (1.4337)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [495][50/391]\tTime 0.051 (0.052)\tData 0.018 (0.018)\tLoss 1.6762 (1.3065)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [495][100/391]\tTime 0.032 (0.051)\tData 0.000 (0.018)\tLoss 0.9731 (1.3242)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [495][150/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.3571 (1.3371)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [495][200/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.5422 (1.3421)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [495][250/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.5928 (1.3378)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [495][300/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.5972 (1.3484)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [495][350/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.6154 (1.3506)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.7964 (0.7964)\tPrec@1 92.969 (92.969)\n",
            "Test: [50/79]\tTime 0.011 (0.016)\tLoss 0.7875 (0.8048)\tPrec@1 92.969 (92.800)\n",
            " * Prec@1 92.780\n",
            "\n",
            "Epoch: 497/600\n",
            "Learning rate: 0.007325\n",
            "Epoch: [496][0/391]\tTime 0.212 (0.212)\tData 0.155 (0.155)\tLoss 1.6706 (1.6706)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [496][50/391]\tTime 0.077 (0.054)\tData 0.034 (0.018)\tLoss 1.6248 (1.3383)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [496][100/391]\tTime 0.069 (0.053)\tData 0.038 (0.018)\tLoss 1.3054 (1.3738)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [496][150/391]\tTime 0.077 (0.053)\tData 0.034 (0.018)\tLoss 1.5128 (1.3699)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [496][200/391]\tTime 0.065 (0.052)\tData 0.033 (0.018)\tLoss 1.3952 (1.3675)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [496][250/391]\tTime 0.071 (0.052)\tData 0.038 (0.018)\tLoss 1.6767 (1.3689)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [496][300/391]\tTime 0.069 (0.052)\tData 0.036 (0.018)\tLoss 1.5690 (1.3706)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [496][350/391]\tTime 0.067 (0.051)\tData 0.036 (0.018)\tLoss 1.1948 (1.3712)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.098 (0.098)\tLoss 0.8008 (0.8008)\tPrec@1 92.969 (92.969)\n",
            "Test: [50/79]\tTime 0.019 (0.016)\tLoss 0.8024 (0.8044)\tPrec@1 90.625 (92.065)\n",
            " * Prec@1 92.170\n",
            "\n",
            "Epoch: 498/600\n",
            "Learning rate: 0.007190\n",
            "Epoch: [497][0/391]\tTime 0.203 (0.203)\tData 0.155 (0.155)\tLoss 1.6240 (1.6240)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [497][50/391]\tTime 0.044 (0.051)\tData 0.012 (0.019)\tLoss 1.5689 (1.4075)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [497][100/391]\tTime 0.067 (0.051)\tData 0.035 (0.018)\tLoss 1.2713 (1.3682)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [497][150/391]\tTime 0.065 (0.050)\tData 0.034 (0.018)\tLoss 1.2584 (1.3416)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [497][200/391]\tTime 0.067 (0.050)\tData 0.036 (0.018)\tLoss 1.5294 (1.3457)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [497][250/391]\tTime 0.064 (0.050)\tData 0.031 (0.017)\tLoss 1.6220 (1.3414)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [497][300/391]\tTime 0.064 (0.050)\tData 0.032 (0.017)\tLoss 1.5911 (1.3466)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [497][350/391]\tTime 0.064 (0.050)\tData 0.032 (0.017)\tLoss 1.3820 (1.3482)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.099 (0.099)\tLoss 0.7969 (0.7969)\tPrec@1 92.188 (92.188)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.7532 (0.7777)\tPrec@1 95.312 (92.785)\n",
            " * Prec@1 92.750\n",
            "\n",
            "Epoch: 499/600\n",
            "Learning rate: 0.007056\n",
            "Epoch: [498][0/391]\tTime 0.200 (0.200)\tData 0.154 (0.154)\tLoss 1.5570 (1.5570)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [498][50/391]\tTime 0.065 (0.051)\tData 0.032 (0.019)\tLoss 1.3283 (1.3399)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [498][100/391]\tTime 0.064 (0.050)\tData 0.032 (0.017)\tLoss 1.6225 (1.3426)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [498][150/391]\tTime 0.075 (0.051)\tData 0.032 (0.017)\tLoss 1.5087 (1.3437)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [498][200/391]\tTime 0.073 (0.053)\tData 0.030 (0.017)\tLoss 1.0685 (1.3310)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [498][250/391]\tTime 0.063 (0.052)\tData 0.031 (0.017)\tLoss 1.5636 (1.3434)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [498][300/391]\tTime 0.064 (0.051)\tData 0.032 (0.016)\tLoss 1.5650 (1.3400)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [498][350/391]\tTime 0.055 (0.051)\tData 0.013 (0.016)\tLoss 1.2481 (1.3441)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.099 (0.099)\tLoss 0.7660 (0.7660)\tPrec@1 94.531 (94.531)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.7590 (0.7768)\tPrec@1 94.531 (92.770)\n",
            " * Prec@1 93.050\n",
            "\n",
            "Epoch: 500/600\n",
            "Learning rate: 0.006923\n",
            "Epoch: [499][0/391]\tTime 0.205 (0.205)\tData 0.157 (0.157)\tLoss 1.6009 (1.6009)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [499][50/391]\tTime 0.065 (0.051)\tData 0.031 (0.018)\tLoss 1.0710 (1.3319)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [499][100/391]\tTime 0.068 (0.050)\tData 0.035 (0.017)\tLoss 1.6854 (1.3579)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [499][150/391]\tTime 0.064 (0.050)\tData 0.031 (0.017)\tLoss 0.9063 (1.3439)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [499][200/391]\tTime 0.063 (0.049)\tData 0.031 (0.017)\tLoss 1.1808 (1.3517)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [499][250/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.1758 (1.3496)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [499][300/391]\tTime 0.063 (0.049)\tData 0.031 (0.017)\tLoss 1.4467 (1.3461)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [499][350/391]\tTime 0.064 (0.049)\tData 0.032 (0.016)\tLoss 1.5674 (1.3486)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.099 (0.099)\tLoss 0.7736 (0.7736)\tPrec@1 92.188 (92.188)\n",
            "Test: [50/79]\tTime 0.011 (0.017)\tLoss 0.7994 (0.7705)\tPrec@1 88.281 (92.371)\n",
            " * Prec@1 92.210\n",
            "\n",
            "Epoch: 501/600\n",
            "Learning rate: 0.006792\n",
            "Epoch: [500][0/391]\tTime 0.220 (0.220)\tData 0.160 (0.160)\tLoss 1.6274 (1.6274)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [500][50/391]\tTime 0.065 (0.051)\tData 0.032 (0.018)\tLoss 1.5707 (1.3344)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [500][100/391]\tTime 0.064 (0.050)\tData 0.032 (0.018)\tLoss 1.6024 (1.3277)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [500][150/391]\tTime 0.064 (0.049)\tData 0.033 (0.017)\tLoss 1.4613 (1.3319)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [500][200/391]\tTime 0.080 (0.049)\tData 0.034 (0.017)\tLoss 1.5535 (1.3327)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [500][250/391]\tTime 0.042 (0.049)\tData 0.000 (0.016)\tLoss 1.2638 (1.3334)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [500][300/391]\tTime 0.033 (0.050)\tData 0.000 (0.016)\tLoss 1.0479 (1.3365)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [500][350/391]\tTime 0.032 (0.050)\tData 0.000 (0.016)\tLoss 1.4150 (1.3205)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.100 (0.100)\tLoss 0.7612 (0.7612)\tPrec@1 94.531 (94.531)\n",
            "Test: [50/79]\tTime 0.019 (0.016)\tLoss 0.7386 (0.7718)\tPrec@1 94.531 (92.984)\n",
            " * Prec@1 93.110\n",
            "\n",
            "Epoch: 502/600\n",
            "Learning rate: 0.006662\n",
            "Epoch: [501][0/391]\tTime 0.218 (0.218)\tData 0.161 (0.161)\tLoss 1.6073 (1.6073)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [501][50/391]\tTime 0.065 (0.052)\tData 0.033 (0.019)\tLoss 1.6727 (1.3093)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [501][100/391]\tTime 0.065 (0.050)\tData 0.033 (0.018)\tLoss 1.1966 (1.3165)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [501][150/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.5361 (1.3137)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [501][200/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.4016 (1.2981)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [501][250/391]\tTime 0.067 (0.049)\tData 0.034 (0.017)\tLoss 1.6550 (1.3047)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [501][300/391]\tTime 0.065 (0.049)\tData 0.032 (0.016)\tLoss 1.5349 (1.3064)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [501][350/391]\tTime 0.063 (0.049)\tData 0.032 (0.016)\tLoss 1.5015 (1.3104)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.098 (0.098)\tLoss 0.7450 (0.7450)\tPrec@1 92.188 (92.188)\n",
            "Test: [50/79]\tTime 0.014 (0.016)\tLoss 0.7422 (0.7649)\tPrec@1 94.531 (93.061)\n",
            " * Prec@1 93.270\n",
            "\n",
            "Epoch: 503/600\n",
            "Learning rate: 0.006533\n",
            "Epoch: [502][0/391]\tTime 0.204 (0.204)\tData 0.157 (0.157)\tLoss 1.0737 (1.0737)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [502][50/391]\tTime 0.043 (0.051)\tData 0.000 (0.018)\tLoss 1.4939 (1.3619)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [502][100/391]\tTime 0.034 (0.051)\tData 0.000 (0.017)\tLoss 1.0974 (1.3341)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [502][150/391]\tTime 0.031 (0.050)\tData 0.000 (0.017)\tLoss 0.8628 (1.3264)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [502][200/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.2164 (1.3327)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [502][250/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.5771 (1.3188)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [502][300/391]\tTime 0.063 (0.050)\tData 0.031 (0.017)\tLoss 1.5112 (1.3282)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [502][350/391]\tTime 0.045 (0.050)\tData 0.013 (0.017)\tLoss 1.4693 (1.3235)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.110 (0.110)\tLoss 0.7601 (0.7601)\tPrec@1 93.750 (93.750)\n",
            "Test: [50/79]\tTime 0.019 (0.016)\tLoss 0.7976 (0.7828)\tPrec@1 93.750 (93.076)\n",
            " * Prec@1 92.900\n",
            "\n",
            "Epoch: 504/600\n",
            "Learning rate: 0.006405\n",
            "Epoch: [503][0/391]\tTime 0.204 (0.204)\tData 0.157 (0.157)\tLoss 0.8821 (0.8821)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [503][50/391]\tTime 0.063 (0.051)\tData 0.031 (0.018)\tLoss 1.5760 (1.2933)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [503][100/391]\tTime 0.066 (0.050)\tData 0.033 (0.017)\tLoss 1.5898 (1.3106)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [503][150/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.0793 (1.3162)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [503][200/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.5043 (1.3164)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [503][250/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 0.8295 (1.3131)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [503][300/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 1.6149 (1.3273)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [503][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.5715 (1.3295)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.7286 (0.7286)\tPrec@1 92.188 (92.188)\n",
            "Test: [50/79]\tTime 0.014 (0.016)\tLoss 0.7220 (0.7407)\tPrec@1 94.531 (92.862)\n",
            " * Prec@1 92.890\n",
            "\n",
            "Epoch: 505/600\n",
            "Learning rate: 0.006278\n",
            "Epoch: [504][0/391]\tTime 0.201 (0.201)\tData 0.154 (0.154)\tLoss 1.5654 (1.5654)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [504][50/391]\tTime 0.064 (0.052)\tData 0.033 (0.019)\tLoss 1.3029 (1.3309)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [504][100/391]\tTime 0.065 (0.050)\tData 0.034 (0.018)\tLoss 1.2293 (1.3433)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [504][150/391]\tTime 0.067 (0.050)\tData 0.035 (0.017)\tLoss 1.5922 (1.3252)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [504][200/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.2130 (1.3251)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [504][250/391]\tTime 0.040 (0.049)\tData 0.008 (0.017)\tLoss 1.6311 (1.3225)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [504][300/391]\tTime 0.076 (0.049)\tData 0.034 (0.017)\tLoss 0.9523 (1.3154)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [504][350/391]\tTime 0.063 (0.049)\tData 0.032 (0.017)\tLoss 1.4830 (1.3162)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.101 (0.101)\tLoss 0.7768 (0.7768)\tPrec@1 93.750 (93.750)\n",
            "Test: [50/79]\tTime 0.010 (0.016)\tLoss 0.7993 (0.8056)\tPrec@1 92.188 (92.494)\n",
            " * Prec@1 92.470\n",
            "\n",
            "Epoch: 506/600\n",
            "Learning rate: 0.006153\n",
            "Epoch: [505][0/391]\tTime 0.203 (0.203)\tData 0.157 (0.157)\tLoss 1.4337 (1.4337)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [505][50/391]\tTime 0.069 (0.052)\tData 0.035 (0.020)\tLoss 1.3073 (1.2986)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [505][100/391]\tTime 0.068 (0.051)\tData 0.035 (0.019)\tLoss 0.9569 (1.2960)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [505][150/391]\tTime 0.068 (0.050)\tData 0.036 (0.018)\tLoss 1.0149 (1.3076)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [505][200/391]\tTime 0.071 (0.050)\tData 0.034 (0.018)\tLoss 1.4667 (1.3106)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [505][250/391]\tTime 0.062 (0.050)\tData 0.031 (0.018)\tLoss 1.0638 (1.3096)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [505][300/391]\tTime 0.065 (0.050)\tData 0.033 (0.017)\tLoss 1.0942 (1.3139)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [505][350/391]\tTime 0.062 (0.050)\tData 0.030 (0.017)\tLoss 1.0412 (1.3178)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.098 (0.098)\tLoss 0.7391 (0.7391)\tPrec@1 92.188 (92.188)\n",
            "Test: [50/79]\tTime 0.015 (0.016)\tLoss 0.7631 (0.7605)\tPrec@1 92.969 (92.724)\n",
            " * Prec@1 92.650\n",
            "\n",
            "Epoch: 507/600\n",
            "Learning rate: 0.006029\n",
            "Epoch: [506][0/391]\tTime 0.227 (0.227)\tData 0.161 (0.161)\tLoss 1.5285 (1.5285)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [506][50/391]\tTime 0.065 (0.052)\tData 0.033 (0.018)\tLoss 0.9177 (1.2987)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [506][100/391]\tTime 0.066 (0.050)\tData 0.034 (0.017)\tLoss 1.1084 (1.3225)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [506][150/391]\tTime 0.064 (0.050)\tData 0.032 (0.017)\tLoss 1.5884 (1.3509)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [506][200/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 0.8342 (1.3313)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [506][250/391]\tTime 0.042 (0.049)\tData 0.000 (0.016)\tLoss 1.5953 (1.3325)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [506][300/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.6105 (1.3272)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [506][350/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.3158 (1.3337)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.099 (0.099)\tLoss 0.6972 (0.6972)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.022 (0.016)\tLoss 0.7112 (0.7366)\tPrec@1 93.750 (93.168)\n",
            " * Prec@1 93.280\n",
            "\n",
            "Epoch: 508/600\n",
            "Learning rate: 0.005906\n",
            "Epoch: [507][0/391]\tTime 0.209 (0.209)\tData 0.163 (0.163)\tLoss 1.0825 (1.0825)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [507][50/391]\tTime 0.067 (0.053)\tData 0.035 (0.019)\tLoss 1.6456 (1.2669)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [507][100/391]\tTime 0.066 (0.051)\tData 0.033 (0.018)\tLoss 1.1386 (1.2982)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [507][150/391]\tTime 0.064 (0.050)\tData 0.033 (0.017)\tLoss 0.8808 (1.3153)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [507][200/391]\tTime 0.063 (0.050)\tData 0.032 (0.017)\tLoss 1.1020 (1.3266)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [507][250/391]\tTime 0.063 (0.049)\tData 0.031 (0.017)\tLoss 1.5311 (1.3133)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [507][300/391]\tTime 0.063 (0.049)\tData 0.031 (0.017)\tLoss 1.3837 (1.3121)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [507][350/391]\tTime 0.064 (0.049)\tData 0.033 (0.017)\tLoss 1.4308 (1.3135)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.098 (0.098)\tLoss 0.7212 (0.7212)\tPrec@1 96.094 (96.094)\n",
            "Test: [50/79]\tTime 0.015 (0.016)\tLoss 0.7389 (0.7552)\tPrec@1 94.531 (92.831)\n",
            " * Prec@1 92.920\n",
            "\n",
            "Epoch: 509/600\n",
            "Learning rate: 0.005784\n",
            "Epoch: [508][0/391]\tTime 0.202 (0.202)\tData 0.156 (0.156)\tLoss 1.1327 (1.1327)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [508][50/391]\tTime 0.066 (0.052)\tData 0.034 (0.020)\tLoss 1.4275 (1.3030)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [508][100/391]\tTime 0.068 (0.051)\tData 0.036 (0.019)\tLoss 1.2398 (1.2835)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [508][150/391]\tTime 0.069 (0.051)\tData 0.037 (0.018)\tLoss 0.9579 (1.3065)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [508][200/391]\tTime 0.070 (0.051)\tData 0.038 (0.018)\tLoss 1.6157 (1.2960)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [508][250/391]\tTime 0.066 (0.050)\tData 0.035 (0.018)\tLoss 1.0962 (1.2982)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [508][300/391]\tTime 0.069 (0.050)\tData 0.037 (0.018)\tLoss 1.5467 (1.3066)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [508][350/391]\tTime 0.069 (0.051)\tData 0.037 (0.018)\tLoss 1.2680 (1.3061)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.099 (0.099)\tLoss 0.7471 (0.7471)\tPrec@1 94.531 (94.531)\n",
            "Test: [50/79]\tTime 0.019 (0.016)\tLoss 0.7588 (0.7811)\tPrec@1 92.188 (92.708)\n",
            " * Prec@1 92.940\n",
            "\n",
            "Epoch: 510/600\n",
            "Learning rate: 0.005664\n",
            "Epoch: [509][0/391]\tTime 0.202 (0.202)\tData 0.155 (0.155)\tLoss 1.6677 (1.6677)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [509][50/391]\tTime 0.065 (0.051)\tData 0.033 (0.019)\tLoss 1.6651 (1.2299)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [509][100/391]\tTime 0.064 (0.049)\tData 0.031 (0.017)\tLoss 1.5719 (1.2612)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [509][150/391]\tTime 0.063 (0.049)\tData 0.028 (0.017)\tLoss 1.3324 (1.2873)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [509][200/391]\tTime 0.067 (0.049)\tData 0.035 (0.017)\tLoss 1.6373 (1.3067)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [509][250/391]\tTime 0.068 (0.049)\tData 0.036 (0.017)\tLoss 1.1388 (1.2991)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [509][300/391]\tTime 0.066 (0.049)\tData 0.034 (0.017)\tLoss 0.7799 (1.3008)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [509][350/391]\tTime 0.067 (0.050)\tData 0.035 (0.017)\tLoss 1.5990 (1.2999)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.109 (0.109)\tLoss 0.7754 (0.7754)\tPrec@1 92.969 (92.969)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.7551 (0.7810)\tPrec@1 93.750 (92.264)\n",
            " * Prec@1 92.520\n",
            "\n",
            "Epoch: 511/600\n",
            "Learning rate: 0.005544\n",
            "Epoch: [510][0/391]\tTime 0.215 (0.215)\tData 0.156 (0.156)\tLoss 1.4796 (1.4796)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [510][50/391]\tTime 0.065 (0.051)\tData 0.033 (0.019)\tLoss 1.4564 (1.3009)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [510][100/391]\tTime 0.065 (0.050)\tData 0.032 (0.018)\tLoss 0.8717 (1.2894)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [510][150/391]\tTime 0.067 (0.049)\tData 0.035 (0.017)\tLoss 1.0364 (1.3121)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [510][200/391]\tTime 0.065 (0.050)\tData 0.033 (0.017)\tLoss 0.9938 (1.3219)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [510][250/391]\tTime 0.065 (0.050)\tData 0.033 (0.017)\tLoss 1.2094 (1.3158)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [510][300/391]\tTime 0.065 (0.050)\tData 0.034 (0.017)\tLoss 1.1968 (1.3238)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [510][350/391]\tTime 0.066 (0.050)\tData 0.034 (0.017)\tLoss 1.2468 (1.3172)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.098 (0.098)\tLoss 0.7695 (0.7695)\tPrec@1 94.531 (94.531)\n",
            "Test: [50/79]\tTime 0.011 (0.016)\tLoss 0.7472 (0.7806)\tPrec@1 93.750 (92.586)\n",
            " * Prec@1 92.490\n",
            "\n",
            "Epoch: 512/600\n",
            "Learning rate: 0.005426\n",
            "Epoch: [511][0/391]\tTime 0.215 (0.215)\tData 0.168 (0.168)\tLoss 1.4266 (1.4266)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [511][50/391]\tTime 0.064 (0.051)\tData 0.033 (0.019)\tLoss 1.3826 (1.2864)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [511][100/391]\tTime 0.064 (0.050)\tData 0.033 (0.018)\tLoss 1.3797 (1.3136)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [511][150/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.5531 (1.3218)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [511][200/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.6061 (1.3142)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [511][250/391]\tTime 0.064 (0.049)\tData 0.033 (0.017)\tLoss 0.9931 (1.3171)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [511][300/391]\tTime 0.064 (0.049)\tData 0.033 (0.017)\tLoss 0.8578 (1.3053)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [511][350/391]\tTime 0.065 (0.049)\tData 0.034 (0.017)\tLoss 1.3268 (1.3063)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.099 (0.099)\tLoss 0.7367 (0.7367)\tPrec@1 94.531 (94.531)\n",
            "Test: [50/79]\tTime 0.016 (0.016)\tLoss 0.7340 (0.7596)\tPrec@1 93.750 (93.183)\n",
            " * Prec@1 93.090\n",
            "\n",
            "Epoch: 513/600\n",
            "Learning rate: 0.005309\n",
            "Epoch: [512][0/391]\tTime 0.202 (0.202)\tData 0.157 (0.157)\tLoss 1.1489 (1.1489)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [512][50/391]\tTime 0.041 (0.051)\tData 0.000 (0.018)\tLoss 1.4222 (1.3637)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [512][100/391]\tTime 0.032 (0.051)\tData 0.000 (0.017)\tLoss 1.6665 (1.3672)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [512][150/391]\tTime 0.033 (0.050)\tData 0.000 (0.016)\tLoss 1.6663 (1.3754)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [512][200/391]\tTime 0.036 (0.049)\tData 0.004 (0.016)\tLoss 1.0205 (1.3461)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [512][250/391]\tTime 0.068 (0.049)\tData 0.037 (0.017)\tLoss 1.0837 (1.3356)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [512][300/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.5720 (1.3351)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [512][350/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.2846 (1.3356)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.7515 (0.7515)\tPrec@1 92.188 (92.188)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.7405 (0.7625)\tPrec@1 94.531 (93.061)\n",
            " * Prec@1 93.380\n",
            "\n",
            "Epoch: 514/600\n",
            "Learning rate: 0.005194\n",
            "Epoch: [513][0/391]\tTime 0.221 (0.221)\tData 0.157 (0.157)\tLoss 1.2367 (1.2367)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [513][50/391]\tTime 0.066 (0.052)\tData 0.034 (0.019)\tLoss 1.4668 (1.3502)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [513][100/391]\tTime 0.062 (0.051)\tData 0.030 (0.018)\tLoss 1.6237 (1.3866)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [513][150/391]\tTime 0.063 (0.050)\tData 0.031 (0.017)\tLoss 1.4833 (1.3533)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [513][200/391]\tTime 0.064 (0.049)\tData 0.033 (0.017)\tLoss 1.3884 (1.3488)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [513][250/391]\tTime 0.166 (0.050)\tData 0.123 (0.017)\tLoss 1.5835 (1.3394)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [513][300/391]\tTime 0.067 (0.051)\tData 0.035 (0.017)\tLoss 1.0252 (1.3443)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [513][350/391]\tTime 0.067 (0.051)\tData 0.035 (0.017)\tLoss 1.4505 (1.3368)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.099 (0.099)\tLoss 0.7955 (0.7955)\tPrec@1 92.969 (92.969)\n",
            "Test: [50/79]\tTime 0.014 (0.016)\tLoss 0.7460 (0.7934)\tPrec@1 95.312 (92.096)\n",
            " * Prec@1 92.020\n",
            "\n",
            "Epoch: 515/600\n",
            "Learning rate: 0.005079\n",
            "Epoch: [514][0/391]\tTime 0.206 (0.206)\tData 0.158 (0.158)\tLoss 1.4426 (1.4426)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [514][50/391]\tTime 0.065 (0.051)\tData 0.034 (0.019)\tLoss 1.3590 (1.3379)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [514][100/391]\tTime 0.064 (0.050)\tData 0.031 (0.018)\tLoss 0.8767 (1.2943)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [514][150/391]\tTime 0.033 (0.050)\tData 0.000 (0.017)\tLoss 0.9203 (1.2890)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [514][200/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.5075 (1.2896)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [514][250/391]\tTime 0.033 (0.049)\tData 0.000 (0.017)\tLoss 1.6026 (1.2847)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [514][300/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 1.4218 (1.2826)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [514][350/391]\tTime 0.034 (0.049)\tData 0.000 (0.016)\tLoss 1.2071 (1.2805)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.099 (0.099)\tLoss 0.6990 (0.6990)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.7130 (0.7495)\tPrec@1 94.531 (93.122)\n",
            " * Prec@1 93.210\n",
            "\n",
            "Epoch: 516/600\n",
            "Learning rate: 0.004966\n",
            "Epoch: [515][0/391]\tTime 0.211 (0.211)\tData 0.153 (0.153)\tLoss 0.8053 (0.8053)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [515][50/391]\tTime 0.033 (0.060)\tData 0.000 (0.018)\tLoss 1.2619 (1.3228)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [515][100/391]\tTime 0.032 (0.054)\tData 0.000 (0.017)\tLoss 0.7795 (1.3090)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [515][150/391]\tTime 0.033 (0.052)\tData 0.000 (0.016)\tLoss 1.3879 (1.3013)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [515][200/391]\tTime 0.032 (0.051)\tData 0.000 (0.016)\tLoss 1.3876 (1.3132)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [515][250/391]\tTime 0.032 (0.051)\tData 0.000 (0.016)\tLoss 1.5976 (1.3236)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [515][300/391]\tTime 0.032 (0.051)\tData 0.000 (0.016)\tLoss 1.5203 (1.3175)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [515][350/391]\tTime 0.033 (0.050)\tData 0.000 (0.016)\tLoss 1.1385 (1.3144)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.101 (0.101)\tLoss 0.7219 (0.7219)\tPrec@1 93.750 (93.750)\n",
            "Test: [50/79]\tTime 0.010 (0.017)\tLoss 0.7131 (0.7435)\tPrec@1 96.094 (93.382)\n",
            " * Prec@1 93.360\n",
            "\n",
            "Epoch: 517/600\n",
            "Learning rate: 0.004854\n",
            "Epoch: [516][0/391]\tTime 0.205 (0.205)\tData 0.159 (0.159)\tLoss 1.4822 (1.4822)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [516][50/391]\tTime 0.031 (0.051)\tData 0.000 (0.019)\tLoss 1.3661 (1.2895)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [516][100/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.2208 (1.3137)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [516][150/391]\tTime 0.033 (0.050)\tData 0.000 (0.018)\tLoss 1.1337 (1.3040)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [516][200/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.5321 (1.3094)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [516][250/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.5402 (1.3088)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [516][300/391]\tTime 0.033 (0.049)\tData 0.000 (0.017)\tLoss 1.4605 (1.3163)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [516][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 0.8111 (1.3133)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.107 (0.107)\tLoss 0.7606 (0.7606)\tPrec@1 93.750 (93.750)\n",
            "Test: [50/79]\tTime 0.015 (0.016)\tLoss 0.7451 (0.7852)\tPrec@1 94.531 (93.275)\n",
            " * Prec@1 93.430\n",
            "\n",
            "Epoch: 518/600\n",
            "Learning rate: 0.004743\n",
            "Epoch: [517][0/391]\tTime 0.204 (0.204)\tData 0.158 (0.158)\tLoss 0.9724 (0.9724)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [517][50/391]\tTime 0.054 (0.051)\tData 0.022 (0.018)\tLoss 1.3919 (1.3319)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [517][100/391]\tTime 0.034 (0.049)\tData 0.002 (0.017)\tLoss 1.5693 (1.3235)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [517][150/391]\tTime 0.031 (0.049)\tData 0.000 (0.016)\tLoss 1.4797 (1.3164)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [517][200/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.2843 (1.3150)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [517][250/391]\tTime 0.032 (0.048)\tData 0.000 (0.016)\tLoss 1.4659 (1.3173)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [517][300/391]\tTime 0.064 (0.049)\tData 0.032 (0.016)\tLoss 1.3801 (1.3195)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [517][350/391]\tTime 0.064 (0.049)\tData 0.032 (0.016)\tLoss 0.8551 (1.3100)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.099 (0.099)\tLoss 0.7480 (0.7480)\tPrec@1 92.969 (92.969)\n",
            "Test: [50/79]\tTime 0.010 (0.016)\tLoss 0.7586 (0.7735)\tPrec@1 94.531 (92.846)\n",
            " * Prec@1 92.810\n",
            "\n",
            "Epoch: 519/600\n",
            "Learning rate: 0.004634\n",
            "Epoch: [518][0/391]\tTime 0.199 (0.199)\tData 0.153 (0.153)\tLoss 0.9526 (0.9526)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [518][50/391]\tTime 0.064 (0.051)\tData 0.032 (0.018)\tLoss 0.9690 (1.3554)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [518][100/391]\tTime 0.063 (0.050)\tData 0.030 (0.017)\tLoss 1.2975 (1.3417)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [518][150/391]\tTime 0.053 (0.049)\tData 0.021 (0.017)\tLoss 1.1278 (1.3423)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [518][200/391]\tTime 0.033 (0.049)\tData 0.002 (0.016)\tLoss 1.6308 (1.3446)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [518][250/391]\tTime 0.076 (0.050)\tData 0.033 (0.016)\tLoss 0.9257 (1.3443)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [518][300/391]\tTime 0.066 (0.050)\tData 0.033 (0.016)\tLoss 0.8060 (1.3407)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [518][350/391]\tTime 0.064 (0.050)\tData 0.032 (0.016)\tLoss 1.4169 (1.3327)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.099 (0.099)\tLoss 0.7600 (0.7600)\tPrec@1 93.750 (93.750)\n",
            "Test: [50/79]\tTime 0.010 (0.016)\tLoss 0.7355 (0.7662)\tPrec@1 96.875 (93.214)\n",
            " * Prec@1 93.300\n",
            "\n",
            "Epoch: 520/600\n",
            "Learning rate: 0.004525\n",
            "Epoch: [519][0/391]\tTime 0.200 (0.200)\tData 0.154 (0.154)\tLoss 1.0193 (1.0193)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [519][50/391]\tTime 0.064 (0.051)\tData 0.032 (0.018)\tLoss 0.6738 (1.2520)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [519][100/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.5695 (1.2865)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [519][150/391]\tTime 0.065 (0.049)\tData 0.034 (0.016)\tLoss 1.2551 (1.2879)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [519][200/391]\tTime 0.065 (0.049)\tData 0.033 (0.016)\tLoss 1.5166 (1.2890)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [519][250/391]\tTime 0.064 (0.049)\tData 0.032 (0.016)\tLoss 1.2442 (1.2772)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [519][300/391]\tTime 0.064 (0.049)\tData 0.033 (0.016)\tLoss 1.3232 (1.2776)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [519][350/391]\tTime 0.063 (0.048)\tData 0.031 (0.016)\tLoss 1.5692 (1.2839)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.110 (0.110)\tLoss 0.7295 (0.7295)\tPrec@1 96.094 (96.094)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.7310 (0.7603)\tPrec@1 94.531 (93.413)\n",
            " * Prec@1 93.360\n",
            "\n",
            "Epoch: 521/600\n",
            "Learning rate: 0.004418\n",
            "Epoch: [520][0/391]\tTime 0.201 (0.201)\tData 0.155 (0.155)\tLoss 1.6639 (1.6639)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [520][50/391]\tTime 0.032 (0.051)\tData 0.000 (0.018)\tLoss 1.2382 (1.3874)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [520][100/391]\tTime 0.033 (0.050)\tData 0.000 (0.017)\tLoss 1.1458 (1.3311)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [520][150/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.2682 (1.3383)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [520][200/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 1.1943 (1.3333)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [520][250/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 1.6226 (1.3266)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [520][300/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.1761 (1.3277)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [520][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.4805 (1.3250)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.098 (0.098)\tLoss 0.7611 (0.7611)\tPrec@1 92.188 (92.188)\n",
            "Test: [50/79]\tTime 0.010 (0.016)\tLoss 0.7385 (0.7491)\tPrec@1 93.750 (92.724)\n",
            " * Prec@1 93.070\n",
            "\n",
            "Epoch: 522/600\n",
            "Learning rate: 0.004313\n",
            "Epoch: [521][0/391]\tTime 0.206 (0.206)\tData 0.157 (0.157)\tLoss 1.2925 (1.2925)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [521][50/391]\tTime 0.065 (0.051)\tData 0.032 (0.018)\tLoss 1.1397 (1.3366)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [521][100/391]\tTime 0.064 (0.050)\tData 0.031 (0.017)\tLoss 1.3297 (1.3074)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [521][150/391]\tTime 0.063 (0.049)\tData 0.031 (0.017)\tLoss 0.9028 (1.3212)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [521][200/391]\tTime 0.056 (0.049)\tData 0.024 (0.017)\tLoss 1.6053 (1.3241)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [521][250/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.0741 (1.3366)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [521][300/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.6580 (1.3264)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [521][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.5511 (1.3180)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.101 (0.101)\tLoss 0.7412 (0.7412)\tPrec@1 93.750 (93.750)\n",
            "Test: [50/79]\tTime 0.023 (0.017)\tLoss 0.7031 (0.7494)\tPrec@1 96.875 (93.566)\n",
            " * Prec@1 93.770\n",
            "\n",
            "Epoch: 523/600\n",
            "Learning rate: 0.004208\n",
            "Epoch: [522][0/391]\tTime 0.214 (0.214)\tData 0.158 (0.158)\tLoss 1.5073 (1.5073)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [522][50/391]\tTime 0.032 (0.052)\tData 0.000 (0.018)\tLoss 1.1131 (1.3229)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [522][100/391]\tTime 0.067 (0.051)\tData 0.035 (0.018)\tLoss 1.5946 (1.3260)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [522][150/391]\tTime 0.066 (0.050)\tData 0.035 (0.018)\tLoss 0.9313 (1.3234)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [522][200/391]\tTime 0.064 (0.050)\tData 0.033 (0.018)\tLoss 1.5836 (1.3110)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [522][250/391]\tTime 0.067 (0.050)\tData 0.035 (0.018)\tLoss 1.4832 (1.3077)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [522][300/391]\tTime 0.066 (0.050)\tData 0.035 (0.017)\tLoss 1.1381 (1.3071)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [522][350/391]\tTime 0.065 (0.050)\tData 0.033 (0.017)\tLoss 1.4683 (1.3138)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.098 (0.098)\tLoss 0.7098 (0.7098)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.011 (0.016)\tLoss 0.6985 (0.7365)\tPrec@1 96.094 (93.750)\n",
            " * Prec@1 93.880\n",
            "\n",
            "Epoch: 524/600\n",
            "Learning rate: 0.004105\n",
            "Epoch: [523][0/391]\tTime 0.203 (0.203)\tData 0.158 (0.158)\tLoss 1.2770 (1.2770)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [523][50/391]\tTime 0.065 (0.052)\tData 0.033 (0.019)\tLoss 1.0082 (1.2890)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [523][100/391]\tTime 0.065 (0.050)\tData 0.033 (0.017)\tLoss 1.3198 (1.2621)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [523][150/391]\tTime 0.036 (0.050)\tData 0.004 (0.017)\tLoss 1.6006 (1.2768)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [523][200/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.4791 (1.2709)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [523][250/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 0.8507 (1.2754)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [523][300/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 1.5850 (1.2876)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [523][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.1832 (1.2956)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.100 (0.100)\tLoss 0.7403 (0.7403)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.019 (0.016)\tLoss 0.7341 (0.7637)\tPrec@1 94.531 (93.827)\n",
            " * Prec@1 93.900\n",
            "\n",
            "Epoch: 525/600\n",
            "Learning rate: 0.004003\n",
            "Epoch: [524][0/391]\tTime 0.212 (0.212)\tData 0.166 (0.166)\tLoss 1.2265 (1.2265)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [524][50/391]\tTime 0.068 (0.053)\tData 0.036 (0.020)\tLoss 1.4001 (1.2288)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [524][100/391]\tTime 0.067 (0.051)\tData 0.035 (0.019)\tLoss 1.2101 (1.2611)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [524][150/391]\tTime 0.068 (0.051)\tData 0.036 (0.018)\tLoss 1.5419 (1.3005)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [524][200/391]\tTime 0.066 (0.050)\tData 0.034 (0.018)\tLoss 0.9595 (1.3067)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [524][250/391]\tTime 0.066 (0.050)\tData 0.035 (0.018)\tLoss 1.2821 (1.3074)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [524][300/391]\tTime 0.067 (0.050)\tData 0.035 (0.018)\tLoss 1.1548 (1.2978)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [524][350/391]\tTime 0.067 (0.050)\tData 0.034 (0.018)\tLoss 1.4940 (1.3097)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.098 (0.098)\tLoss 0.7597 (0.7597)\tPrec@1 93.750 (93.750)\n",
            "Test: [50/79]\tTime 0.010 (0.016)\tLoss 0.7274 (0.7693)\tPrec@1 93.750 (93.091)\n",
            " * Prec@1 93.010\n",
            "\n",
            "Epoch: 526/600\n",
            "Learning rate: 0.003902\n",
            "Epoch: [525][0/391]\tTime 0.204 (0.204)\tData 0.154 (0.154)\tLoss 1.3631 (1.3631)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [525][50/391]\tTime 0.033 (0.053)\tData 0.000 (0.017)\tLoss 1.5684 (1.3827)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [525][100/391]\tTime 0.074 (0.053)\tData 0.032 (0.016)\tLoss 0.9350 (1.3513)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [525][150/391]\tTime 0.067 (0.054)\tData 0.034 (0.016)\tLoss 1.6141 (1.3558)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [525][200/391]\tTime 0.043 (0.052)\tData 0.012 (0.015)\tLoss 1.4443 (1.3453)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [525][250/391]\tTime 0.038 (0.051)\tData 0.006 (0.016)\tLoss 1.4139 (1.3502)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [525][300/391]\tTime 0.032 (0.051)\tData 0.000 (0.016)\tLoss 0.7185 (1.3327)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [525][350/391]\tTime 0.032 (0.051)\tData 0.000 (0.016)\tLoss 1.2001 (1.3256)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.101 (0.101)\tLoss 0.7475 (0.7475)\tPrec@1 93.750 (93.750)\n",
            "Test: [50/79]\tTime 0.022 (0.017)\tLoss 0.6947 (0.7591)\tPrec@1 97.656 (93.627)\n",
            " * Prec@1 93.590\n",
            "\n",
            "Epoch: 527/600\n",
            "Learning rate: 0.003803\n",
            "Epoch: [526][0/391]\tTime 0.202 (0.202)\tData 0.156 (0.156)\tLoss 1.2871 (1.2871)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [526][50/391]\tTime 0.031 (0.050)\tData 0.000 (0.018)\tLoss 0.9763 (1.2769)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [526][100/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 0.7893 (1.3089)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [526][150/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 1.5805 (1.3118)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [526][200/391]\tTime 0.033 (0.049)\tData 0.000 (0.017)\tLoss 1.5315 (1.3016)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [526][250/391]\tTime 0.062 (0.049)\tData 0.028 (0.017)\tLoss 1.0594 (1.2960)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [526][300/391]\tTime 0.066 (0.049)\tData 0.034 (0.017)\tLoss 1.2634 (1.2951)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [526][350/391]\tTime 0.039 (0.049)\tData 0.008 (0.017)\tLoss 1.5205 (1.2968)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.100 (0.100)\tLoss 0.7220 (0.7220)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.014 (0.016)\tLoss 0.7233 (0.7475)\tPrec@1 94.531 (93.934)\n",
            " * Prec@1 93.980\n",
            "\n",
            "Epoch: 528/600\n",
            "Learning rate: 0.003705\n",
            "Epoch: [527][0/391]\tTime 0.206 (0.206)\tData 0.159 (0.159)\tLoss 0.9173 (0.9173)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [527][50/391]\tTime 0.065 (0.052)\tData 0.032 (0.019)\tLoss 1.3374 (1.3046)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [527][100/391]\tTime 0.062 (0.050)\tData 0.030 (0.018)\tLoss 1.1877 (1.3144)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [527][150/391]\tTime 0.064 (0.050)\tData 0.031 (0.017)\tLoss 1.5269 (1.3153)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [527][200/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.5039 (1.3065)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [527][250/391]\tTime 0.060 (0.049)\tData 0.027 (0.017)\tLoss 1.2481 (1.2924)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [527][300/391]\tTime 0.064 (0.049)\tData 0.032 (0.016)\tLoss 0.8608 (1.2880)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [527][350/391]\tTime 0.063 (0.049)\tData 0.030 (0.016)\tLoss 1.3771 (1.2877)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.101 (0.101)\tLoss 0.6966 (0.6966)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.020 (0.017)\tLoss 0.6527 (0.7100)\tPrec@1 96.875 (94.148)\n",
            " * Prec@1 94.260\n",
            "\n",
            "Epoch: 529/600\n",
            "Learning rate: 0.003608\n",
            "Epoch: [528][0/391]\tTime 0.209 (0.209)\tData 0.163 (0.163)\tLoss 1.4377 (1.4377)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [528][50/391]\tTime 0.068 (0.052)\tData 0.032 (0.020)\tLoss 1.6614 (1.3312)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [528][100/391]\tTime 0.062 (0.051)\tData 0.030 (0.018)\tLoss 1.6140 (1.3365)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [528][150/391]\tTime 0.064 (0.050)\tData 0.032 (0.017)\tLoss 0.8134 (1.3135)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [528][200/391]\tTime 0.045 (0.050)\tData 0.013 (0.017)\tLoss 1.5064 (1.3271)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [528][250/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.3884 (1.3221)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [528][300/391]\tTime 0.033 (0.049)\tData 0.000 (0.016)\tLoss 1.3413 (1.3153)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [528][350/391]\tTime 0.033 (0.049)\tData 0.000 (0.016)\tLoss 1.5248 (1.3178)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.104 (0.104)\tLoss 0.7562 (0.7562)\tPrec@1 93.750 (93.750)\n",
            "Test: [50/79]\tTime 0.011 (0.016)\tLoss 0.7505 (0.7744)\tPrec@1 95.312 (93.444)\n",
            " * Prec@1 93.480\n",
            "\n",
            "Epoch: 530/600\n",
            "Learning rate: 0.003512\n",
            "Epoch: [529][0/391]\tTime 0.206 (0.206)\tData 0.156 (0.156)\tLoss 1.5223 (1.5223)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [529][50/391]\tTime 0.063 (0.051)\tData 0.031 (0.018)\tLoss 1.0481 (1.2915)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [529][100/391]\tTime 0.059 (0.050)\tData 0.024 (0.017)\tLoss 1.5309 (1.3390)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [529][150/391]\tTime 0.068 (0.050)\tData 0.035 (0.017)\tLoss 1.1818 (1.3285)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [529][200/391]\tTime 0.067 (0.050)\tData 0.034 (0.017)\tLoss 0.9065 (1.3095)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [529][250/391]\tTime 0.069 (0.050)\tData 0.036 (0.017)\tLoss 1.4102 (1.3195)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [529][300/391]\tTime 0.067 (0.050)\tData 0.034 (0.017)\tLoss 1.4836 (1.3062)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [529][350/391]\tTime 0.067 (0.050)\tData 0.034 (0.017)\tLoss 1.6134 (1.3118)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.099 (0.099)\tLoss 0.7542 (0.7542)\tPrec@1 92.969 (92.969)\n",
            "Test: [50/79]\tTime 0.019 (0.016)\tLoss 0.7116 (0.7493)\tPrec@1 97.656 (94.271)\n",
            " * Prec@1 94.200\n",
            "\n",
            "Epoch: 531/600\n",
            "Learning rate: 0.003418\n",
            "Epoch: [530][0/391]\tTime 0.206 (0.206)\tData 0.160 (0.160)\tLoss 1.4338 (1.4338)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [530][50/391]\tTime 0.033 (0.054)\tData 0.000 (0.017)\tLoss 1.5125 (1.3294)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [530][100/391]\tTime 0.049 (0.051)\tData 0.016 (0.016)\tLoss 1.0797 (1.3102)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [530][150/391]\tTime 0.062 (0.051)\tData 0.029 (0.016)\tLoss 1.4874 (1.2954)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [530][200/391]\tTime 0.032 (0.050)\tData 0.000 (0.016)\tLoss 1.3564 (1.2885)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [530][250/391]\tTime 0.033 (0.050)\tData 0.000 (0.016)\tLoss 1.4934 (1.2870)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [530][300/391]\tTime 0.036 (0.050)\tData 0.000 (0.016)\tLoss 1.3965 (1.2984)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [530][350/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.3174 (1.2956)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.102 (0.102)\tLoss 0.6942 (0.6942)\tPrec@1 96.094 (96.094)\n",
            "Test: [50/79]\tTime 0.011 (0.016)\tLoss 0.7073 (0.7448)\tPrec@1 96.875 (93.873)\n",
            " * Prec@1 94.040\n",
            "\n",
            "Epoch: 532/600\n",
            "Learning rate: 0.003325\n",
            "Epoch: [531][0/391]\tTime 0.216 (0.216)\tData 0.157 (0.157)\tLoss 0.7561 (0.7561)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [531][50/391]\tTime 0.032 (0.051)\tData 0.001 (0.019)\tLoss 0.8960 (1.2360)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [531][100/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.0358 (1.2624)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [531][150/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.2452 (1.2800)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [531][200/391]\tTime 0.034 (0.050)\tData 0.000 (0.017)\tLoss 1.6126 (1.2765)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [531][250/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.3444 (1.2845)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [531][300/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.0968 (1.2891)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [531][350/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 0.9750 (1.2921)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.098 (0.098)\tLoss 0.7638 (0.7638)\tPrec@1 92.188 (92.188)\n",
            "Test: [50/79]\tTime 0.020 (0.016)\tLoss 0.6993 (0.7387)\tPrec@1 96.094 (94.179)\n",
            " * Prec@1 94.300\n",
            "\n",
            "Epoch: 533/600\n",
            "Learning rate: 0.003233\n",
            "Epoch: [532][0/391]\tTime 0.206 (0.206)\tData 0.157 (0.157)\tLoss 1.0285 (1.0285)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [532][50/391]\tTime 0.063 (0.051)\tData 0.030 (0.018)\tLoss 1.5909 (1.3056)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [532][100/391]\tTime 0.066 (0.050)\tData 0.033 (0.017)\tLoss 1.2874 (1.3176)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [532][150/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.0855 (1.3114)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [532][200/391]\tTime 0.033 (0.049)\tData 0.000 (0.016)\tLoss 1.6033 (1.3105)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [532][250/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.6370 (1.3241)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [532][300/391]\tTime 0.033 (0.049)\tData 0.000 (0.016)\tLoss 1.5608 (1.3154)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [532][350/391]\tTime 0.031 (0.049)\tData 0.000 (0.016)\tLoss 1.3861 (1.3064)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.112 (0.112)\tLoss 0.7574 (0.7574)\tPrec@1 94.531 (94.531)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.7483 (0.7736)\tPrec@1 93.750 (93.520)\n",
            " * Prec@1 93.670\n",
            "\n",
            "Epoch: 534/600\n",
            "Learning rate: 0.003142\n",
            "Epoch: [533][0/391]\tTime 0.214 (0.214)\tData 0.155 (0.155)\tLoss 1.2147 (1.2147)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [533][50/391]\tTime 0.032 (0.052)\tData 0.000 (0.018)\tLoss 1.5617 (1.3119)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [533][100/391]\tTime 0.037 (0.050)\tData 0.005 (0.017)\tLoss 1.6253 (1.2875)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [533][150/391]\tTime 0.032 (0.050)\tData 0.001 (0.017)\tLoss 1.2765 (1.3013)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [533][200/391]\tTime 0.059 (0.049)\tData 0.028 (0.017)\tLoss 0.9571 (1.2785)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [533][250/391]\tTime 0.075 (0.050)\tData 0.032 (0.017)\tLoss 1.5447 (1.2749)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [533][300/391]\tTime 0.033 (0.050)\tData 0.001 (0.016)\tLoss 0.8779 (1.2733)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [533][350/391]\tTime 0.052 (0.050)\tData 0.021 (0.016)\tLoss 1.1785 (1.2766)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.101 (0.101)\tLoss 0.7516 (0.7516)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.7344 (0.7645)\tPrec@1 95.312 (93.704)\n",
            " * Prec@1 93.800\n",
            "\n",
            "Epoch: 535/600\n",
            "Learning rate: 0.003053\n",
            "Epoch: [534][0/391]\tTime 0.202 (0.202)\tData 0.155 (0.155)\tLoss 1.5472 (1.5472)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [534][50/391]\tTime 0.064 (0.051)\tData 0.032 (0.018)\tLoss 1.4053 (1.3264)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [534][100/391]\tTime 0.064 (0.050)\tData 0.032 (0.017)\tLoss 1.4917 (1.3371)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [534][150/391]\tTime 0.042 (0.049)\tData 0.011 (0.017)\tLoss 1.5940 (1.3402)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [534][200/391]\tTime 0.037 (0.049)\tData 0.003 (0.017)\tLoss 0.6791 (1.3259)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [534][250/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.2298 (1.3107)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [534][300/391]\tTime 0.034 (0.049)\tData 0.001 (0.017)\tLoss 1.5541 (1.3109)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [534][350/391]\tTime 0.053 (0.049)\tData 0.011 (0.016)\tLoss 0.7781 (1.3070)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.097 (0.097)\tLoss 0.7202 (0.7202)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.011 (0.016)\tLoss 0.7044 (0.7367)\tPrec@1 96.094 (94.102)\n",
            " * Prec@1 94.220\n",
            "\n",
            "Epoch: 536/600\n",
            "Learning rate: 0.002965\n",
            "Epoch: [535][0/391]\tTime 0.203 (0.203)\tData 0.156 (0.156)\tLoss 1.6106 (1.6106)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [535][50/391]\tTime 0.064 (0.051)\tData 0.031 (0.018)\tLoss 1.1529 (1.3020)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [535][100/391]\tTime 0.063 (0.050)\tData 0.030 (0.017)\tLoss 1.4884 (1.3187)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [535][150/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 0.7639 (1.2976)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [535][200/391]\tTime 0.033 (0.050)\tData 0.000 (0.017)\tLoss 1.3217 (1.2854)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [535][250/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.3304 (1.2939)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [535][300/391]\tTime 0.043 (0.049)\tData 0.011 (0.017)\tLoss 1.5481 (1.3007)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [535][350/391]\tTime 0.063 (0.049)\tData 0.030 (0.017)\tLoss 1.5679 (1.3021)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.101 (0.101)\tLoss 0.7523 (0.7523)\tPrec@1 93.750 (93.750)\n",
            "Test: [50/79]\tTime 0.019 (0.017)\tLoss 0.7108 (0.7516)\tPrec@1 97.656 (93.903)\n",
            " * Prec@1 93.920\n",
            "\n",
            "Epoch: 537/600\n",
            "Learning rate: 0.002878\n",
            "Epoch: [536][0/391]\tTime 0.211 (0.211)\tData 0.153 (0.153)\tLoss 1.4183 (1.4183)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [536][50/391]\tTime 0.065 (0.051)\tData 0.034 (0.018)\tLoss 1.4798 (1.3268)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [536][100/391]\tTime 0.066 (0.050)\tData 0.033 (0.017)\tLoss 1.0425 (1.3363)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [536][150/391]\tTime 0.063 (0.049)\tData 0.031 (0.017)\tLoss 1.4851 (1.3153)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [536][200/391]\tTime 0.067 (0.049)\tData 0.034 (0.017)\tLoss 0.8174 (1.3042)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [536][250/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.5123 (1.2980)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [536][300/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 0.9379 (1.2969)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [536][350/391]\tTime 0.063 (0.049)\tData 0.030 (0.016)\tLoss 1.5793 (1.2984)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.100 (0.100)\tLoss 0.7338 (0.7338)\tPrec@1 93.750 (93.750)\n",
            "Test: [50/79]\tTime 0.019 (0.016)\tLoss 0.7254 (0.7429)\tPrec@1 93.750 (94.531)\n",
            " * Prec@1 94.450\n",
            "\n",
            "Epoch: 538/600\n",
            "Learning rate: 0.002793\n",
            "Epoch: [537][0/391]\tTime 0.211 (0.211)\tData 0.159 (0.159)\tLoss 1.3088 (1.3088)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [537][50/391]\tTime 0.066 (0.051)\tData 0.033 (0.018)\tLoss 1.4861 (1.2909)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [537][100/391]\tTime 0.059 (0.050)\tData 0.027 (0.017)\tLoss 0.7981 (1.2718)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [537][150/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 0.9867 (1.2767)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [537][200/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.6258 (1.2865)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [537][250/391]\tTime 0.059 (0.049)\tData 0.015 (0.017)\tLoss 1.5946 (1.2825)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [537][300/391]\tTime 0.065 (0.049)\tData 0.032 (0.017)\tLoss 1.4956 (1.2860)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [537][350/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.5089 (1.2907)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.099 (0.099)\tLoss 0.7459 (0.7459)\tPrec@1 94.531 (94.531)\n",
            "Test: [50/79]\tTime 0.019 (0.016)\tLoss 0.7181 (0.7583)\tPrec@1 96.875 (94.148)\n",
            " * Prec@1 94.030\n",
            "\n",
            "Epoch: 539/600\n",
            "Learning rate: 0.002709\n",
            "Epoch: [538][0/391]\tTime 0.200 (0.200)\tData 0.155 (0.155)\tLoss 1.4860 (1.4860)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [538][50/391]\tTime 0.054 (0.051)\tData 0.021 (0.018)\tLoss 1.5156 (1.2948)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [538][100/391]\tTime 0.052 (0.050)\tData 0.020 (0.017)\tLoss 1.4735 (1.3110)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [538][150/391]\tTime 0.067 (0.050)\tData 0.035 (0.017)\tLoss 1.0179 (1.2986)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [538][200/391]\tTime 0.067 (0.050)\tData 0.035 (0.017)\tLoss 1.4779 (1.2929)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [538][250/391]\tTime 0.066 (0.050)\tData 0.033 (0.017)\tLoss 0.7831 (1.2911)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [538][300/391]\tTime 0.067 (0.050)\tData 0.033 (0.017)\tLoss 1.6221 (1.2951)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [538][350/391]\tTime 0.062 (0.050)\tData 0.030 (0.017)\tLoss 0.8224 (1.2813)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.100 (0.100)\tLoss 0.7109 (0.7109)\tPrec@1 94.531 (94.531)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.6929 (0.7335)\tPrec@1 97.656 (94.593)\n",
            " * Prec@1 94.620\n",
            "\n",
            "Epoch: 540/600\n",
            "Learning rate: 0.002626\n",
            "Epoch: [539][0/391]\tTime 0.215 (0.215)\tData 0.158 (0.158)\tLoss 1.6618 (1.6618)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [539][50/391]\tTime 0.033 (0.054)\tData 0.000 (0.017)\tLoss 1.1932 (1.3104)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [539][100/391]\tTime 0.032 (0.051)\tData 0.000 (0.017)\tLoss 0.7905 (1.2806)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [539][150/391]\tTime 0.031 (0.051)\tData 0.000 (0.017)\tLoss 1.1581 (1.2693)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [539][200/391]\tTime 0.068 (0.050)\tData 0.035 (0.017)\tLoss 1.3375 (1.2583)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [539][250/391]\tTime 0.069 (0.050)\tData 0.037 (0.017)\tLoss 1.5192 (1.2719)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [539][300/391]\tTime 0.071 (0.050)\tData 0.038 (0.017)\tLoss 1.5640 (1.2771)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [539][350/391]\tTime 0.067 (0.050)\tData 0.035 (0.017)\tLoss 1.4214 (1.2797)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.101 (0.101)\tLoss 0.7639 (0.7639)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.011 (0.016)\tLoss 0.7349 (0.7802)\tPrec@1 96.875 (94.179)\n",
            " * Prec@1 94.200\n",
            "\n",
            "Epoch: 541/600\n",
            "Learning rate: 0.002545\n",
            "Epoch: [540][0/391]\tTime 0.200 (0.200)\tData 0.154 (0.154)\tLoss 1.5046 (1.5046)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [540][50/391]\tTime 0.066 (0.052)\tData 0.034 (0.019)\tLoss 1.3737 (1.3213)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [540][100/391]\tTime 0.063 (0.050)\tData 0.031 (0.017)\tLoss 1.4762 (1.3057)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [540][150/391]\tTime 0.066 (0.050)\tData 0.033 (0.017)\tLoss 0.8892 (1.3221)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [540][200/391]\tTime 0.066 (0.049)\tData 0.035 (0.017)\tLoss 1.5097 (1.3206)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [540][250/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.5553 (1.3150)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [540][300/391]\tTime 0.064 (0.049)\tData 0.031 (0.017)\tLoss 1.4482 (1.3064)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [540][350/391]\tTime 0.064 (0.049)\tData 0.031 (0.016)\tLoss 0.8587 (1.3031)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.113 (0.113)\tLoss 0.7730 (0.7730)\tPrec@1 93.750 (93.750)\n",
            "Test: [50/79]\tTime 0.022 (0.018)\tLoss 0.7417 (0.7686)\tPrec@1 94.531 (94.439)\n",
            " * Prec@1 94.560\n",
            "\n",
            "Epoch: 542/600\n",
            "Learning rate: 0.002465\n",
            "Epoch: [541][0/391]\tTime 0.206 (0.206)\tData 0.159 (0.159)\tLoss 0.7434 (0.7434)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [541][50/391]\tTime 0.053 (0.051)\tData 0.020 (0.018)\tLoss 0.9346 (1.2342)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [541][100/391]\tTime 0.042 (0.051)\tData 0.000 (0.017)\tLoss 1.3166 (1.2733)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [541][150/391]\tTime 0.065 (0.051)\tData 0.032 (0.016)\tLoss 1.6156 (1.2786)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [541][200/391]\tTime 0.073 (0.050)\tData 0.038 (0.016)\tLoss 1.0668 (1.2652)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [541][250/391]\tTime 0.068 (0.050)\tData 0.036 (0.016)\tLoss 0.8647 (1.2636)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [541][300/391]\tTime 0.065 (0.050)\tData 0.032 (0.017)\tLoss 1.0976 (1.2698)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [541][350/391]\tTime 0.064 (0.050)\tData 0.032 (0.016)\tLoss 0.7618 (1.2629)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.099 (0.099)\tLoss 0.7488 (0.7488)\tPrec@1 94.531 (94.531)\n",
            "Test: [50/79]\tTime 0.019 (0.016)\tLoss 0.7429 (0.7672)\tPrec@1 94.531 (93.888)\n",
            " * Prec@1 94.140\n",
            "\n",
            "Epoch: 543/600\n",
            "Learning rate: 0.002386\n",
            "Epoch: [542][0/391]\tTime 0.206 (0.206)\tData 0.156 (0.156)\tLoss 1.0767 (1.0767)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [542][50/391]\tTime 0.032 (0.052)\tData 0.000 (0.019)\tLoss 1.0191 (1.3195)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [542][100/391]\tTime 0.032 (0.051)\tData 0.000 (0.018)\tLoss 1.6174 (1.3252)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [542][150/391]\tTime 0.033 (0.050)\tData 0.000 (0.018)\tLoss 0.9144 (1.2947)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [542][200/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 0.8557 (1.2845)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [542][250/391]\tTime 0.034 (0.050)\tData 0.000 (0.018)\tLoss 1.2725 (1.2773)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [542][300/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.0014 (1.2849)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [542][350/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.5298 (1.2842)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.099 (0.099)\tLoss 0.7649 (0.7649)\tPrec@1 92.969 (92.969)\n",
            "Test: [50/79]\tTime 0.017 (0.016)\tLoss 0.7335 (0.7615)\tPrec@1 93.750 (93.949)\n",
            " * Prec@1 93.950\n",
            "\n",
            "Epoch: 544/600\n",
            "Learning rate: 0.002308\n",
            "Epoch: [543][0/391]\tTime 0.201 (0.201)\tData 0.155 (0.155)\tLoss 1.6001 (1.6001)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [543][50/391]\tTime 0.068 (0.054)\tData 0.034 (0.019)\tLoss 0.8928 (1.2455)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [543][100/391]\tTime 0.066 (0.051)\tData 0.034 (0.018)\tLoss 1.4396 (1.2650)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [543][150/391]\tTime 0.068 (0.051)\tData 0.036 (0.018)\tLoss 0.9522 (1.2672)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [543][200/391]\tTime 0.066 (0.050)\tData 0.034 (0.018)\tLoss 1.3337 (1.2845)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [543][250/391]\tTime 0.068 (0.050)\tData 0.036 (0.018)\tLoss 1.0977 (1.2848)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [543][300/391]\tTime 0.064 (0.050)\tData 0.032 (0.017)\tLoss 1.5735 (1.2825)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [543][350/391]\tTime 0.064 (0.050)\tData 0.032 (0.017)\tLoss 1.2900 (1.2815)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.099 (0.099)\tLoss 0.7390 (0.7390)\tPrec@1 94.531 (94.531)\n",
            "Test: [50/79]\tTime 0.019 (0.016)\tLoss 0.7309 (0.7561)\tPrec@1 94.531 (94.056)\n",
            " * Prec@1 94.190\n",
            "\n",
            "Epoch: 545/600\n",
            "Learning rate: 0.002232\n",
            "Epoch: [544][0/391]\tTime 0.204 (0.204)\tData 0.156 (0.156)\tLoss 1.2004 (1.2004)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [544][50/391]\tTime 0.064 (0.051)\tData 0.031 (0.018)\tLoss 1.2069 (1.2666)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [544][100/391]\tTime 0.073 (0.050)\tData 0.032 (0.017)\tLoss 1.3196 (1.2707)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [544][150/391]\tTime 0.065 (0.049)\tData 0.032 (0.017)\tLoss 1.0417 (1.2938)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [544][200/391]\tTime 0.063 (0.049)\tData 0.031 (0.017)\tLoss 1.2419 (1.2978)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [544][250/391]\tTime 0.066 (0.049)\tData 0.034 (0.017)\tLoss 1.5741 (1.2978)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [544][300/391]\tTime 0.066 (0.049)\tData 0.032 (0.017)\tLoss 1.4936 (1.2994)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [544][350/391]\tTime 0.063 (0.049)\tData 0.031 (0.016)\tLoss 1.5881 (1.2982)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.099 (0.099)\tLoss 0.7413 (0.7413)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.015 (0.016)\tLoss 0.7004 (0.7401)\tPrec@1 96.094 (94.577)\n",
            " * Prec@1 94.780\n",
            "\n",
            "Epoch: 546/600\n",
            "Learning rate: 0.002157\n",
            "Epoch: [545][0/391]\tTime 0.213 (0.213)\tData 0.155 (0.155)\tLoss 1.1664 (1.1664)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [545][50/391]\tTime 0.064 (0.051)\tData 0.032 (0.018)\tLoss 0.7737 (1.2852)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [545][100/391]\tTime 0.061 (0.049)\tData 0.030 (0.017)\tLoss 1.5075 (1.2804)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [545][150/391]\tTime 0.054 (0.049)\tData 0.021 (0.017)\tLoss 1.0712 (1.2794)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [545][200/391]\tTime 0.055 (0.049)\tData 0.024 (0.016)\tLoss 1.4905 (1.2904)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [545][250/391]\tTime 0.031 (0.049)\tData 0.000 (0.016)\tLoss 1.1172 (1.2934)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [545][300/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.4439 (1.2919)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [545][350/391]\tTime 0.031 (0.048)\tData 0.000 (0.016)\tLoss 1.4974 (1.2938)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.099 (0.099)\tLoss 0.7033 (0.7033)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.019 (0.016)\tLoss 0.6766 (0.7108)\tPrec@1 96.875 (94.562)\n",
            " * Prec@1 94.720\n",
            "\n",
            "Epoch: 547/600\n",
            "Learning rate: 0.002083\n",
            "Epoch: [546][0/391]\tTime 0.212 (0.212)\tData 0.154 (0.154)\tLoss 1.5602 (1.5602)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [546][50/391]\tTime 0.032 (0.053)\tData 0.000 (0.018)\tLoss 1.0481 (1.2447)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [546][100/391]\tTime 0.032 (0.051)\tData 0.000 (0.017)\tLoss 1.4903 (1.2659)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [546][150/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.2974 (1.2696)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [546][200/391]\tTime 0.033 (0.050)\tData 0.000 (0.017)\tLoss 0.7530 (1.2592)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [546][250/391]\tTime 0.031 (0.050)\tData 0.000 (0.017)\tLoss 1.0461 (1.2554)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [546][300/391]\tTime 0.066 (0.050)\tData 0.033 (0.017)\tLoss 1.5466 (1.2739)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [546][350/391]\tTime 0.063 (0.050)\tData 0.032 (0.017)\tLoss 1.4032 (1.2713)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.102 (0.102)\tLoss 0.7101 (0.7101)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.020 (0.017)\tLoss 0.7050 (0.7396)\tPrec@1 95.312 (94.332)\n",
            " * Prec@1 94.450\n",
            "\n",
            "Epoch: 548/600\n",
            "Learning rate: 0.002011\n",
            "Epoch: [547][0/391]\tTime 0.216 (0.216)\tData 0.157 (0.157)\tLoss 1.3755 (1.3755)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [547][50/391]\tTime 0.032 (0.052)\tData 0.000 (0.019)\tLoss 1.2104 (1.2446)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [547][100/391]\tTime 0.034 (0.051)\tData 0.000 (0.018)\tLoss 1.4396 (1.2584)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [547][150/391]\tTime 0.031 (0.050)\tData 0.000 (0.017)\tLoss 1.4123 (1.2611)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [547][200/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 1.0464 (1.2688)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [547][250/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 0.9683 (1.2781)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [547][300/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.4662 (1.2861)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [547][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 0.9918 (1.2835)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.100 (0.100)\tLoss 0.6901 (0.6901)\tPrec@1 94.531 (94.531)\n",
            "Test: [50/79]\tTime 0.010 (0.017)\tLoss 0.6678 (0.7098)\tPrec@1 96.875 (94.562)\n",
            " * Prec@1 94.670\n",
            "\n",
            "Epoch: 549/600\n",
            "Learning rate: 0.001940\n",
            "Epoch: [548][0/391]\tTime 0.213 (0.213)\tData 0.167 (0.167)\tLoss 1.1773 (1.1773)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [548][50/391]\tTime 0.065 (0.052)\tData 0.033 (0.020)\tLoss 1.5516 (1.3146)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [548][100/391]\tTime 0.066 (0.051)\tData 0.033 (0.019)\tLoss 0.7781 (1.2671)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [548][150/391]\tTime 0.066 (0.051)\tData 0.034 (0.018)\tLoss 1.5474 (1.2912)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [548][200/391]\tTime 0.068 (0.050)\tData 0.036 (0.018)\tLoss 1.4044 (1.2731)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [548][250/391]\tTime 0.067 (0.050)\tData 0.035 (0.018)\tLoss 1.6303 (1.2648)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [548][300/391]\tTime 0.069 (0.050)\tData 0.036 (0.018)\tLoss 1.5061 (1.2675)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [548][350/391]\tTime 0.067 (0.050)\tData 0.035 (0.018)\tLoss 0.9469 (1.2726)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.099 (0.099)\tLoss 0.7240 (0.7240)\tPrec@1 94.531 (94.531)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.7223 (0.7546)\tPrec@1 95.312 (94.102)\n",
            " * Prec@1 94.220\n",
            "\n",
            "Epoch: 550/600\n",
            "Learning rate: 0.001870\n",
            "Epoch: [549][0/391]\tTime 0.213 (0.213)\tData 0.156 (0.156)\tLoss 1.3600 (1.3600)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [549][50/391]\tTime 0.064 (0.053)\tData 0.032 (0.018)\tLoss 1.1357 (1.3109)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [549][100/391]\tTime 0.062 (0.052)\tData 0.030 (0.018)\tLoss 0.8988 (1.2776)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [549][150/391]\tTime 0.064 (0.051)\tData 0.033 (0.017)\tLoss 1.4318 (1.3054)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [549][200/391]\tTime 0.064 (0.050)\tData 0.032 (0.017)\tLoss 1.3900 (1.3058)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [549][250/391]\tTime 0.065 (0.050)\tData 0.033 (0.017)\tLoss 1.2080 (1.2915)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [549][300/391]\tTime 0.063 (0.049)\tData 0.031 (0.017)\tLoss 1.6335 (1.2960)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [549][350/391]\tTime 0.051 (0.049)\tData 0.019 (0.017)\tLoss 1.4251 (1.3021)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.101 (0.101)\tLoss 0.6830 (0.6830)\tPrec@1 96.094 (96.094)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.6551 (0.7094)\tPrec@1 97.656 (94.409)\n",
            " * Prec@1 94.510\n",
            "\n",
            "Epoch: 551/600\n",
            "Learning rate: 0.001802\n",
            "Epoch: [550][0/391]\tTime 0.219 (0.219)\tData 0.161 (0.161)\tLoss 0.8138 (0.8138)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [550][50/391]\tTime 0.065 (0.053)\tData 0.034 (0.020)\tLoss 1.5801 (1.2742)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [550][100/391]\tTime 0.067 (0.051)\tData 0.035 (0.019)\tLoss 1.1594 (1.2719)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [550][150/391]\tTime 0.064 (0.050)\tData 0.033 (0.018)\tLoss 0.9573 (1.2663)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [550][200/391]\tTime 0.064 (0.050)\tData 0.031 (0.017)\tLoss 1.3867 (1.2623)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [550][250/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.6161 (1.2492)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [550][300/391]\tTime 0.061 (0.049)\tData 0.029 (0.017)\tLoss 1.5573 (1.2597)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [550][350/391]\tTime 0.066 (0.049)\tData 0.033 (0.017)\tLoss 0.7536 (1.2672)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.110 (0.110)\tLoss 0.7157 (0.7157)\tPrec@1 96.094 (96.094)\n",
            "Test: [50/79]\tTime 0.019 (0.017)\tLoss 0.6953 (0.7281)\tPrec@1 96.094 (94.562)\n",
            " * Prec@1 94.700\n",
            "\n",
            "Epoch: 552/600\n",
            "Learning rate: 0.001735\n",
            "Epoch: [551][0/391]\tTime 0.204 (0.204)\tData 0.158 (0.158)\tLoss 0.8378 (0.8378)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [551][50/391]\tTime 0.068 (0.052)\tData 0.034 (0.020)\tLoss 1.6311 (1.3070)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [551][100/391]\tTime 0.067 (0.051)\tData 0.035 (0.018)\tLoss 1.4263 (1.2827)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [551][150/391]\tTime 0.066 (0.050)\tData 0.034 (0.018)\tLoss 1.3207 (1.2764)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [551][200/391]\tTime 0.063 (0.050)\tData 0.031 (0.018)\tLoss 1.4676 (1.2837)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [551][250/391]\tTime 0.069 (0.050)\tData 0.037 (0.018)\tLoss 0.9278 (1.2776)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [551][300/391]\tTime 0.066 (0.050)\tData 0.034 (0.018)\tLoss 1.1569 (1.2747)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [551][350/391]\tTime 0.067 (0.050)\tData 0.035 (0.018)\tLoss 1.4095 (1.2729)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.100 (0.100)\tLoss 0.7076 (0.7076)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.018 (0.017)\tLoss 0.6808 (0.7160)\tPrec@1 95.312 (94.638)\n",
            " * Prec@1 94.760\n",
            "\n",
            "Epoch: 553/600\n",
            "Learning rate: 0.001669\n",
            "Epoch: [552][0/391]\tTime 0.204 (0.204)\tData 0.156 (0.156)\tLoss 0.9882 (0.9882)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [552][50/391]\tTime 0.042 (0.061)\tData 0.000 (0.018)\tLoss 1.5070 (1.2130)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [552][100/391]\tTime 0.032 (0.057)\tData 0.000 (0.017)\tLoss 1.2269 (1.2719)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [552][150/391]\tTime 0.032 (0.054)\tData 0.000 (0.017)\tLoss 1.5777 (1.2676)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [552][200/391]\tTime 0.034 (0.053)\tData 0.000 (0.017)\tLoss 1.4010 (1.2600)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [552][250/391]\tTime 0.032 (0.052)\tData 0.000 (0.016)\tLoss 1.1710 (1.2836)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [552][300/391]\tTime 0.032 (0.051)\tData 0.000 (0.016)\tLoss 1.0357 (1.2867)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [552][350/391]\tTime 0.031 (0.051)\tData 0.000 (0.016)\tLoss 1.5738 (1.2857)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.100 (0.100)\tLoss 0.6799 (0.6799)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.011 (0.017)\tLoss 0.6633 (0.6995)\tPrec@1 95.312 (94.776)\n",
            " * Prec@1 94.840\n",
            "\n",
            "Epoch: 554/600\n",
            "Learning rate: 0.001605\n",
            "Epoch: [553][0/391]\tTime 0.215 (0.215)\tData 0.157 (0.157)\tLoss 1.3786 (1.3786)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [553][50/391]\tTime 0.062 (0.051)\tData 0.030 (0.018)\tLoss 1.4065 (1.2544)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [553][100/391]\tTime 0.031 (0.050)\tData 0.000 (0.017)\tLoss 1.2503 (1.2613)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [553][150/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.5317 (1.2597)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [553][200/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.5435 (1.2571)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [553][250/391]\tTime 0.033 (0.049)\tData 0.000 (0.017)\tLoss 0.8870 (1.2663)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [553][300/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 1.2720 (1.2738)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [553][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.0940 (1.2781)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.101 (0.101)\tLoss 0.7154 (0.7154)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.017 (0.016)\tLoss 0.6939 (0.7261)\tPrec@1 96.094 (94.792)\n",
            " * Prec@1 94.890\n",
            "\n",
            "Epoch: 555/600\n",
            "Learning rate: 0.001542\n",
            "Epoch: [554][0/391]\tTime 0.207 (0.207)\tData 0.160 (0.160)\tLoss 1.3315 (1.3315)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [554][50/391]\tTime 0.065 (0.051)\tData 0.034 (0.019)\tLoss 1.3338 (1.3260)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [554][100/391]\tTime 0.065 (0.050)\tData 0.033 (0.018)\tLoss 1.1423 (1.2740)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [554][150/391]\tTime 0.065 (0.050)\tData 0.032 (0.017)\tLoss 1.5592 (1.2796)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [554][200/391]\tTime 0.065 (0.049)\tData 0.034 (0.017)\tLoss 1.0341 (1.2828)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [554][250/391]\tTime 0.065 (0.049)\tData 0.034 (0.017)\tLoss 1.4607 (1.2920)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [554][300/391]\tTime 0.062 (0.049)\tData 0.030 (0.017)\tLoss 1.5119 (1.2803)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [554][350/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.4643 (1.2687)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.110 (0.110)\tLoss 0.7307 (0.7307)\tPrec@1 96.875 (96.875)\n",
            "Test: [50/79]\tTime 0.017 (0.016)\tLoss 0.7287 (0.7636)\tPrec@1 96.094 (94.700)\n",
            " * Prec@1 94.740\n",
            "\n",
            "Epoch: 556/600\n",
            "Learning rate: 0.001480\n",
            "Epoch: [555][0/391]\tTime 0.216 (0.216)\tData 0.156 (0.156)\tLoss 1.4918 (1.4918)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [555][50/391]\tTime 0.062 (0.055)\tData 0.030 (0.018)\tLoss 1.4182 (1.2976)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [555][100/391]\tTime 0.068 (0.052)\tData 0.034 (0.017)\tLoss 1.5473 (1.2693)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [555][150/391]\tTime 0.067 (0.051)\tData 0.035 (0.017)\tLoss 1.2761 (1.2679)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [555][200/391]\tTime 0.066 (0.051)\tData 0.034 (0.017)\tLoss 1.3431 (1.2605)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [555][250/391]\tTime 0.064 (0.050)\tData 0.032 (0.017)\tLoss 0.6895 (1.2564)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [555][300/391]\tTime 0.064 (0.050)\tData 0.032 (0.017)\tLoss 1.5153 (1.2557)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [555][350/391]\tTime 0.065 (0.050)\tData 0.031 (0.017)\tLoss 1.0830 (1.2585)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.099 (0.099)\tLoss 0.6780 (0.6780)\tPrec@1 96.094 (96.094)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.6695 (0.7076)\tPrec@1 96.875 (95.052)\n",
            " * Prec@1 95.030\n",
            "\n",
            "Epoch: 557/600\n",
            "Learning rate: 0.001420\n",
            "Epoch: [556][0/391]\tTime 0.203 (0.203)\tData 0.157 (0.157)\tLoss 0.7365 (0.7365)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [556][50/391]\tTime 0.064 (0.051)\tData 0.032 (0.019)\tLoss 0.8984 (1.2470)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [556][100/391]\tTime 0.063 (0.050)\tData 0.032 (0.017)\tLoss 0.8380 (1.2370)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [556][150/391]\tTime 0.060 (0.049)\tData 0.026 (0.017)\tLoss 1.4210 (1.2352)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [556][200/391]\tTime 0.034 (0.049)\tData 0.003 (0.016)\tLoss 1.4046 (1.2436)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [556][250/391]\tTime 0.064 (0.049)\tData 0.032 (0.016)\tLoss 0.8992 (1.2399)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [556][300/391]\tTime 0.066 (0.049)\tData 0.033 (0.016)\tLoss 1.5798 (1.2481)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [556][350/391]\tTime 0.063 (0.049)\tData 0.031 (0.016)\tLoss 1.0690 (1.2485)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.100 (0.100)\tLoss 0.7184 (0.7184)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.017 (0.016)\tLoss 0.6799 (0.7253)\tPrec@1 97.656 (94.822)\n",
            " * Prec@1 94.940\n",
            "\n",
            "Epoch: 558/600\n",
            "Learning rate: 0.001361\n",
            "Epoch: [557][0/391]\tTime 0.203 (0.203)\tData 0.157 (0.157)\tLoss 1.4795 (1.4795)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [557][50/391]\tTime 0.032 (0.051)\tData 0.000 (0.019)\tLoss 1.6024 (1.2697)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [557][100/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.3658 (1.2622)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [557][150/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 0.7878 (1.2628)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [557][200/391]\tTime 0.034 (0.049)\tData 0.000 (0.017)\tLoss 1.3967 (1.2636)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [557][250/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.5448 (1.2685)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [557][300/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.5363 (1.2685)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [557][350/391]\tTime 0.033 (0.049)\tData 0.000 (0.017)\tLoss 1.4817 (1.2709)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.100 (0.100)\tLoss 0.7046 (0.7046)\tPrec@1 96.875 (96.875)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.6956 (0.7335)\tPrec@1 96.094 (95.205)\n",
            " * Prec@1 95.260\n",
            "\n",
            "Epoch: 559/600\n",
            "Learning rate: 0.001303\n",
            "Epoch: [558][0/391]\tTime 0.205 (0.205)\tData 0.157 (0.157)\tLoss 1.3791 (1.3791)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [558][50/391]\tTime 0.033 (0.052)\tData 0.000 (0.016)\tLoss 1.4460 (1.2451)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [558][100/391]\tTime 0.032 (0.050)\tData 0.000 (0.016)\tLoss 1.5226 (1.2816)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [558][150/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 0.7410 (1.2522)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [558][200/391]\tTime 0.034 (0.049)\tData 0.000 (0.016)\tLoss 1.3434 (1.2567)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [558][250/391]\tTime 0.031 (0.049)\tData 0.000 (0.016)\tLoss 1.6122 (1.2626)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [558][300/391]\tTime 0.033 (0.049)\tData 0.000 (0.016)\tLoss 0.7882 (1.2602)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [558][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 0.9369 (1.2500)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.105 (0.105)\tLoss 0.7118 (0.7118)\tPrec@1 96.875 (96.875)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.7153 (0.7409)\tPrec@1 95.312 (94.470)\n",
            " * Prec@1 94.680\n",
            "\n",
            "Epoch: 560/600\n",
            "Learning rate: 0.001247\n",
            "Epoch: [559][0/391]\tTime 0.200 (0.200)\tData 0.155 (0.155)\tLoss 1.3305 (1.3305)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [559][50/391]\tTime 0.032 (0.051)\tData 0.000 (0.019)\tLoss 1.5420 (1.2555)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [559][100/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.3533 (1.2727)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [559][150/391]\tTime 0.031 (0.050)\tData 0.000 (0.017)\tLoss 1.5222 (1.2677)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [559][200/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 1.5495 (1.2794)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [559][250/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.0708 (1.2794)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [559][300/391]\tTime 0.033 (0.049)\tData 0.000 (0.017)\tLoss 1.2510 (1.2735)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [559][350/391]\tTime 0.063 (0.049)\tData 0.030 (0.017)\tLoss 1.1294 (1.2741)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.100 (0.100)\tLoss 0.7275 (0.7275)\tPrec@1 96.094 (96.094)\n",
            "Test: [50/79]\tTime 0.012 (0.016)\tLoss 0.6986 (0.7414)\tPrec@1 97.656 (94.945)\n",
            " * Prec@1 95.030\n",
            "\n",
            "Epoch: 561/600\n",
            "Learning rate: 0.001192\n",
            "Epoch: [560][0/391]\tTime 0.203 (0.203)\tData 0.156 (0.156)\tLoss 1.1458 (1.1458)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [560][50/391]\tTime 0.065 (0.051)\tData 0.033 (0.018)\tLoss 1.3862 (1.2604)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [560][100/391]\tTime 0.075 (0.050)\tData 0.033 (0.017)\tLoss 0.9707 (1.2459)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [560][150/391]\tTime 0.032 (0.050)\tData 0.000 (0.016)\tLoss 1.5044 (1.2543)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [560][200/391]\tTime 0.032 (0.050)\tData 0.000 (0.016)\tLoss 1.3305 (1.2658)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [560][250/391]\tTime 0.032 (0.050)\tData 0.000 (0.016)\tLoss 1.1920 (1.2693)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [560][300/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 0.7990 (1.2598)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [560][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.3711 (1.2627)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.109 (0.109)\tLoss 0.7132 (0.7132)\tPrec@1 94.531 (94.531)\n",
            "Test: [50/79]\tTime 0.018 (0.017)\tLoss 0.6976 (0.7331)\tPrec@1 96.094 (94.914)\n",
            " * Prec@1 95.020\n",
            "\n",
            "Epoch: 562/600\n",
            "Learning rate: 0.001138\n",
            "Epoch: [561][0/391]\tTime 0.208 (0.208)\tData 0.158 (0.158)\tLoss 1.1625 (1.1625)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [561][50/391]\tTime 0.065 (0.051)\tData 0.033 (0.019)\tLoss 1.5019 (1.2826)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [561][100/391]\tTime 0.062 (0.049)\tData 0.029 (0.017)\tLoss 1.2486 (1.2733)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [561][150/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 1.5874 (1.2736)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [561][200/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.2968 (1.2662)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [561][250/391]\tTime 0.039 (0.049)\tData 0.008 (0.017)\tLoss 0.9305 (1.2573)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [561][300/391]\tTime 0.039 (0.049)\tData 0.008 (0.017)\tLoss 1.2309 (1.2623)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [561][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.4648 (1.2475)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.104 (0.104)\tLoss 0.6933 (0.6933)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.012 (0.017)\tLoss 0.6874 (0.7106)\tPrec@1 94.531 (94.930)\n",
            " * Prec@1 95.030\n",
            "\n",
            "Epoch: 563/600\n",
            "Learning rate: 0.001085\n",
            "Epoch: [562][0/391]\tTime 0.206 (0.206)\tData 0.159 (0.159)\tLoss 0.7336 (0.7336)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [562][50/391]\tTime 0.067 (0.052)\tData 0.034 (0.020)\tLoss 1.3987 (1.2204)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [562][100/391]\tTime 0.065 (0.051)\tData 0.033 (0.018)\tLoss 1.2310 (1.2637)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [562][150/391]\tTime 0.063 (0.050)\tData 0.032 (0.018)\tLoss 1.3684 (1.2624)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [562][200/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.4246 (1.2643)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [562][250/391]\tTime 0.068 (0.049)\tData 0.036 (0.017)\tLoss 1.2317 (1.2630)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [562][300/391]\tTime 0.064 (0.049)\tData 0.031 (0.017)\tLoss 1.5076 (1.2750)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [562][350/391]\tTime 0.063 (0.049)\tData 0.031 (0.017)\tLoss 1.3295 (1.2734)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.099 (0.099)\tLoss 0.7073 (0.7073)\tPrec@1 93.750 (93.750)\n",
            "Test: [50/79]\tTime 0.020 (0.016)\tLoss 0.6791 (0.7113)\tPrec@1 96.094 (94.761)\n",
            " * Prec@1 94.900\n",
            "\n",
            "Epoch: 564/600\n",
            "Learning rate: 0.001034\n",
            "Epoch: [563][0/391]\tTime 0.212 (0.212)\tData 0.155 (0.155)\tLoss 1.0059 (1.0059)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [563][50/391]\tTime 0.069 (0.052)\tData 0.035 (0.018)\tLoss 1.1882 (1.2838)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [563][100/391]\tTime 0.064 (0.050)\tData 0.032 (0.017)\tLoss 1.1270 (1.2782)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [563][150/391]\tTime 0.055 (0.049)\tData 0.023 (0.017)\tLoss 0.6129 (1.2741)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [563][200/391]\tTime 0.058 (0.049)\tData 0.026 (0.016)\tLoss 0.9846 (1.2692)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [563][250/391]\tTime 0.056 (0.049)\tData 0.023 (0.016)\tLoss 1.4830 (1.2696)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [563][300/391]\tTime 0.065 (0.049)\tData 0.034 (0.016)\tLoss 1.5721 (1.2687)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [563][350/391]\tTime 0.055 (0.049)\tData 0.023 (0.016)\tLoss 1.1250 (1.2643)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.102 (0.102)\tLoss 0.7043 (0.7043)\tPrec@1 96.094 (96.094)\n",
            "Test: [50/79]\tTime 0.010 (0.016)\tLoss 0.7037 (0.7377)\tPrec@1 95.312 (94.684)\n",
            " * Prec@1 94.830\n",
            "\n",
            "Epoch: 565/600\n",
            "Learning rate: 0.000985\n",
            "Epoch: [564][0/391]\tTime 0.207 (0.207)\tData 0.160 (0.160)\tLoss 1.1009 (1.1009)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [564][50/391]\tTime 0.066 (0.051)\tData 0.034 (0.019)\tLoss 0.9533 (1.1950)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [564][100/391]\tTime 0.033 (0.051)\tData 0.000 (0.017)\tLoss 1.5763 (1.1746)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [564][150/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.2797 (1.2240)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [564][200/391]\tTime 0.034 (0.050)\tData 0.000 (0.017)\tLoss 1.5111 (1.2269)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [564][250/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.0229 (1.2249)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [564][300/391]\tTime 0.031 (0.049)\tData 0.000 (0.016)\tLoss 1.5436 (1.2382)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [564][350/391]\tTime 0.042 (0.049)\tData 0.000 (0.016)\tLoss 1.5682 (1.2437)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.108 (0.108)\tLoss 0.7262 (0.7262)\tPrec@1 96.094 (96.094)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.7150 (0.7474)\tPrec@1 96.875 (95.129)\n",
            " * Prec@1 95.120\n",
            "\n",
            "Epoch: 566/600\n",
            "Learning rate: 0.000936\n",
            "Epoch: [565][0/391]\tTime 0.203 (0.203)\tData 0.156 (0.156)\tLoss 1.5227 (1.5227)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [565][50/391]\tTime 0.037 (0.051)\tData 0.005 (0.018)\tLoss 1.5335 (1.2117)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [565][100/391]\tTime 0.065 (0.050)\tData 0.033 (0.017)\tLoss 1.1519 (1.2375)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [565][150/391]\tTime 0.054 (0.050)\tData 0.022 (0.017)\tLoss 0.6957 (1.2327)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [565][200/391]\tTime 0.053 (0.049)\tData 0.022 (0.017)\tLoss 1.4486 (1.2387)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [565][250/391]\tTime 0.057 (0.049)\tData 0.025 (0.017)\tLoss 1.5380 (1.2400)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [565][300/391]\tTime 0.045 (0.049)\tData 0.014 (0.017)\tLoss 1.3159 (1.2410)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [565][350/391]\tTime 0.065 (0.049)\tData 0.032 (0.017)\tLoss 1.3218 (1.2439)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.109 (0.109)\tLoss 0.7191 (0.7191)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.010 (0.017)\tLoss 0.6979 (0.7337)\tPrec@1 96.094 (94.945)\n",
            " * Prec@1 94.980\n",
            "\n",
            "Epoch: 567/600\n",
            "Learning rate: 0.000889\n",
            "Epoch: [566][0/391]\tTime 0.209 (0.209)\tData 0.152 (0.152)\tLoss 1.4994 (1.4994)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [566][50/391]\tTime 0.066 (0.052)\tData 0.035 (0.019)\tLoss 0.9535 (1.2340)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [566][100/391]\tTime 0.066 (0.050)\tData 0.034 (0.018)\tLoss 1.0479 (1.2144)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [566][150/391]\tTime 0.068 (0.050)\tData 0.037 (0.018)\tLoss 1.2794 (1.2518)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [566][200/391]\tTime 0.066 (0.050)\tData 0.035 (0.017)\tLoss 1.5492 (1.2447)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [566][250/391]\tTime 0.062 (0.050)\tData 0.030 (0.017)\tLoss 1.3116 (1.2409)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [566][300/391]\tTime 0.062 (0.050)\tData 0.030 (0.017)\tLoss 1.4319 (1.2486)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [566][350/391]\tTime 0.067 (0.050)\tData 0.035 (0.017)\tLoss 1.2072 (1.2463)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.117 (0.117)\tLoss 0.7059 (0.7059)\tPrec@1 96.875 (96.875)\n",
            "Test: [50/79]\tTime 0.011 (0.017)\tLoss 0.6900 (0.7267)\tPrec@1 96.875 (95.190)\n",
            " * Prec@1 95.150\n",
            "\n",
            "Epoch: 568/600\n",
            "Learning rate: 0.000844\n",
            "Epoch: [567][0/391]\tTime 0.205 (0.205)\tData 0.158 (0.158)\tLoss 0.9582 (0.9582)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [567][50/391]\tTime 0.065 (0.051)\tData 0.033 (0.019)\tLoss 1.2326 (1.2271)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [567][100/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 1.5414 (1.2720)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [567][150/391]\tTime 0.033 (0.050)\tData 0.000 (0.018)\tLoss 1.4667 (1.2532)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [567][200/391]\tTime 0.033 (0.049)\tData 0.000 (0.018)\tLoss 0.7601 (1.2436)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [567][250/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 0.9338 (1.2465)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [567][300/391]\tTime 0.034 (0.049)\tData 0.000 (0.017)\tLoss 1.5389 (1.2484)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [567][350/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.4873 (1.2511)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.101 (0.101)\tLoss 0.7171 (0.7171)\tPrec@1 96.094 (96.094)\n",
            "Test: [50/79]\tTime 0.016 (0.016)\tLoss 0.6944 (0.7346)\tPrec@1 96.875 (94.884)\n",
            " * Prec@1 94.990\n",
            "\n",
            "Epoch: 569/600\n",
            "Learning rate: 0.000799\n",
            "Epoch: [568][0/391]\tTime 0.200 (0.200)\tData 0.155 (0.155)\tLoss 1.5551 (1.5551)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [568][50/391]\tTime 0.063 (0.052)\tData 0.031 (0.019)\tLoss 0.7605 (1.2469)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [568][100/391]\tTime 0.064 (0.050)\tData 0.032 (0.017)\tLoss 1.5461 (1.2713)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [568][150/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.5811 (1.2853)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [568][200/391]\tTime 0.063 (0.049)\tData 0.031 (0.016)\tLoss 1.1762 (1.2779)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [568][250/391]\tTime 0.064 (0.049)\tData 0.032 (0.016)\tLoss 0.8969 (1.2607)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [568][300/391]\tTime 0.066 (0.049)\tData 0.035 (0.016)\tLoss 0.7820 (1.2523)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [568][350/391]\tTime 0.065 (0.049)\tData 0.034 (0.016)\tLoss 0.7470 (1.2468)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.101 (0.101)\tLoss 0.7067 (0.7067)\tPrec@1 96.094 (96.094)\n",
            "Test: [50/79]\tTime 0.019 (0.016)\tLoss 0.6897 (0.7216)\tPrec@1 96.875 (95.113)\n",
            " * Prec@1 95.160\n",
            "\n",
            "Epoch: 570/600\n",
            "Learning rate: 0.000757\n",
            "Epoch: [569][0/391]\tTime 0.202 (0.202)\tData 0.154 (0.154)\tLoss 1.4927 (1.4927)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [569][50/391]\tTime 0.064 (0.051)\tData 0.031 (0.018)\tLoss 1.2181 (1.1960)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [569][100/391]\tTime 0.065 (0.050)\tData 0.031 (0.017)\tLoss 1.4633 (1.2224)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [569][150/391]\tTime 0.065 (0.049)\tData 0.032 (0.017)\tLoss 1.5153 (1.2351)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [569][200/391]\tTime 0.065 (0.049)\tData 0.033 (0.016)\tLoss 1.3157 (1.2426)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [569][250/391]\tTime 0.064 (0.049)\tData 0.033 (0.017)\tLoss 1.5738 (1.2380)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [569][300/391]\tTime 0.063 (0.049)\tData 0.032 (0.016)\tLoss 1.3941 (1.2365)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [569][350/391]\tTime 0.063 (0.049)\tData 0.032 (0.016)\tLoss 1.6139 (1.2434)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.100 (0.100)\tLoss 0.6817 (0.6817)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.010 (0.016)\tLoss 0.6719 (0.7003)\tPrec@1 96.875 (95.221)\n",
            " * Prec@1 95.320\n",
            "\n",
            "Epoch: 571/600\n",
            "Learning rate: 0.000715\n",
            "Epoch: [570][0/391]\tTime 0.203 (0.203)\tData 0.157 (0.157)\tLoss 1.2668 (1.2668)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [570][50/391]\tTime 0.066 (0.051)\tData 0.034 (0.019)\tLoss 0.7949 (1.2174)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [570][100/391]\tTime 0.068 (0.050)\tData 0.036 (0.018)\tLoss 1.1103 (1.2092)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [570][150/391]\tTime 0.065 (0.050)\tData 0.033 (0.018)\tLoss 1.4218 (1.2413)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [570][200/391]\tTime 0.062 (0.050)\tData 0.030 (0.017)\tLoss 1.4074 (1.2377)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [570][250/391]\tTime 0.067 (0.049)\tData 0.033 (0.017)\tLoss 0.9771 (1.2196)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [570][300/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 0.7865 (1.2269)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [570][350/391]\tTime 0.069 (0.049)\tData 0.033 (0.017)\tLoss 1.0197 (1.2218)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.101 (0.101)\tLoss 0.7201 (0.7201)\tPrec@1 94.531 (94.531)\n",
            "Test: [50/79]\tTime 0.019 (0.017)\tLoss 0.7088 (0.7396)\tPrec@1 97.656 (94.914)\n",
            " * Prec@1 95.150\n",
            "\n",
            "Epoch: 572/600\n",
            "Learning rate: 0.000675\n",
            "Epoch: [571][0/391]\tTime 0.213 (0.213)\tData 0.157 (0.157)\tLoss 1.4935 (1.4935)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [571][50/391]\tTime 0.032 (0.051)\tData 0.000 (0.017)\tLoss 1.5969 (1.1728)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [571][100/391]\tTime 0.067 (0.051)\tData 0.034 (0.016)\tLoss 1.4592 (1.2105)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [571][150/391]\tTime 0.064 (0.050)\tData 0.031 (0.016)\tLoss 1.5414 (1.2244)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [571][200/391]\tTime 0.077 (0.051)\tData 0.033 (0.016)\tLoss 1.1330 (1.2147)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [571][250/391]\tTime 0.077 (0.053)\tData 0.032 (0.016)\tLoss 0.8642 (1.2256)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [571][300/391]\tTime 0.065 (0.053)\tData 0.033 (0.016)\tLoss 1.0654 (1.2369)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [571][350/391]\tTime 0.064 (0.052)\tData 0.032 (0.016)\tLoss 1.5473 (1.2456)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.098 (0.098)\tLoss 0.7450 (0.7450)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.012 (0.017)\tLoss 0.7306 (0.7617)\tPrec@1 97.656 (94.960)\n",
            " * Prec@1 95.100\n",
            "\n",
            "Epoch: 573/600\n",
            "Learning rate: 0.000636\n",
            "Epoch: [572][0/391]\tTime 0.200 (0.200)\tData 0.154 (0.154)\tLoss 1.5133 (1.5133)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [572][50/391]\tTime 0.067 (0.051)\tData 0.034 (0.018)\tLoss 1.0609 (1.2441)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [572][100/391]\tTime 0.065 (0.049)\tData 0.034 (0.017)\tLoss 0.8694 (1.2200)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [572][150/391]\tTime 0.066 (0.049)\tData 0.033 (0.017)\tLoss 1.0056 (1.2488)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [572][200/391]\tTime 0.065 (0.049)\tData 0.034 (0.017)\tLoss 1.4810 (1.2613)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [572][250/391]\tTime 0.065 (0.049)\tData 0.034 (0.017)\tLoss 1.2263 (1.2553)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [572][300/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 0.8210 (1.2655)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [572][350/391]\tTime 0.063 (0.049)\tData 0.031 (0.017)\tLoss 1.4343 (1.2619)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.099 (0.099)\tLoss 0.6763 (0.6763)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.012 (0.016)\tLoss 0.6695 (0.6982)\tPrec@1 97.656 (95.389)\n",
            " * Prec@1 95.460\n",
            "\n",
            "Epoch: 574/600\n",
            "Learning rate: 0.000598\n",
            "Epoch: [573][0/391]\tTime 0.205 (0.205)\tData 0.159 (0.159)\tLoss 1.0011 (1.0011)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [573][50/391]\tTime 0.063 (0.051)\tData 0.031 (0.018)\tLoss 1.2925 (1.2902)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [573][100/391]\tTime 0.064 (0.050)\tData 0.032 (0.017)\tLoss 0.6595 (1.2478)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [573][150/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.1150 (1.2515)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [573][200/391]\tTime 0.066 (0.049)\tData 0.032 (0.017)\tLoss 1.3153 (1.2538)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [573][250/391]\tTime 0.069 (0.049)\tData 0.037 (0.016)\tLoss 1.1925 (1.2619)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [573][300/391]\tTime 0.074 (0.049)\tData 0.032 (0.016)\tLoss 1.5942 (1.2639)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [573][350/391]\tTime 0.033 (0.049)\tData 0.000 (0.016)\tLoss 0.9582 (1.2599)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.100 (0.100)\tLoss 0.7398 (0.7398)\tPrec@1 96.094 (96.094)\n",
            "Test: [50/79]\tTime 0.010 (0.016)\tLoss 0.7263 (0.7490)\tPrec@1 96.875 (95.343)\n",
            " * Prec@1 95.380\n",
            "\n",
            "Epoch: 575/600\n",
            "Learning rate: 0.000562\n",
            "Epoch: [574][0/391]\tTime 0.201 (0.201)\tData 0.155 (0.155)\tLoss 1.0333 (1.0333)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [574][50/391]\tTime 0.065 (0.051)\tData 0.032 (0.018)\tLoss 1.4919 (1.2202)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [574][100/391]\tTime 0.064 (0.050)\tData 0.033 (0.018)\tLoss 1.3737 (1.2295)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [574][150/391]\tTime 0.062 (0.050)\tData 0.030 (0.017)\tLoss 1.3547 (1.2534)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [574][200/391]\tTime 0.063 (0.050)\tData 0.031 (0.017)\tLoss 0.6813 (1.2446)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [574][250/391]\tTime 0.065 (0.049)\tData 0.034 (0.017)\tLoss 1.4848 (1.2450)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [574][300/391]\tTime 0.062 (0.049)\tData 0.026 (0.017)\tLoss 0.9203 (1.2387)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [574][350/391]\tTime 0.065 (0.049)\tData 0.034 (0.017)\tLoss 0.9687 (1.2332)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.109 (0.109)\tLoss 0.7098 (0.7098)\tPrec@1 96.875 (96.875)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.7008 (0.7266)\tPrec@1 95.312 (95.420)\n",
            " * Prec@1 95.440\n",
            "\n",
            "Epoch: 576/600\n",
            "Learning rate: 0.000527\n",
            "Epoch: [575][0/391]\tTime 0.201 (0.201)\tData 0.154 (0.154)\tLoss 0.9825 (0.9825)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [575][50/391]\tTime 0.031 (0.051)\tData 0.000 (0.019)\tLoss 1.5376 (1.2264)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [575][100/391]\tTime 0.034 (0.050)\tData 0.000 (0.018)\tLoss 1.4299 (1.2456)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [575][150/391]\tTime 0.033 (0.050)\tData 0.000 (0.017)\tLoss 1.4163 (1.2630)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [575][200/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.5486 (1.2588)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [575][250/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.4294 (1.2633)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [575][300/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 1.0259 (1.2475)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [575][350/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 1.4103 (1.2581)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.100 (0.100)\tLoss 0.7277 (0.7277)\tPrec@1 96.094 (96.094)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.7081 (0.7447)\tPrec@1 96.875 (95.098)\n",
            " * Prec@1 95.140\n",
            "\n",
            "Epoch: 577/600\n",
            "Learning rate: 0.000494\n",
            "Epoch: [576][0/391]\tTime 0.204 (0.204)\tData 0.157 (0.157)\tLoss 1.0951 (1.0951)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [576][50/391]\tTime 0.064 (0.051)\tData 0.032 (0.018)\tLoss 0.8587 (1.2412)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [576][100/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 0.7487 (1.2344)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [576][150/391]\tTime 0.066 (0.049)\tData 0.033 (0.017)\tLoss 1.0792 (1.2387)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [576][200/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.2911 (1.2438)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [576][250/391]\tTime 0.031 (0.049)\tData 0.000 (0.016)\tLoss 0.6846 (1.2432)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [576][300/391]\tTime 0.067 (0.049)\tData 0.034 (0.016)\tLoss 1.2692 (1.2528)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [576][350/391]\tTime 0.066 (0.049)\tData 0.034 (0.016)\tLoss 1.5238 (1.2522)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.100 (0.100)\tLoss 0.6949 (0.6949)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.011 (0.016)\tLoss 0.6825 (0.7145)\tPrec@1 96.875 (95.435)\n",
            " * Prec@1 95.450\n",
            "\n",
            "Epoch: 578/600\n",
            "Learning rate: 0.000462\n",
            "Epoch: [577][0/391]\tTime 0.203 (0.203)\tData 0.157 (0.157)\tLoss 0.8128 (0.8128)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [577][50/391]\tTime 0.068 (0.053)\tData 0.036 (0.020)\tLoss 0.9706 (1.2398)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [577][100/391]\tTime 0.065 (0.050)\tData 0.033 (0.018)\tLoss 1.0552 (1.2527)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [577][150/391]\tTime 0.062 (0.050)\tData 0.030 (0.018)\tLoss 0.8940 (1.2329)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [577][200/391]\tTime 0.070 (0.049)\tData 0.037 (0.017)\tLoss 0.6675 (1.2277)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [577][250/391]\tTime 0.062 (0.049)\tData 0.031 (0.017)\tLoss 0.6489 (1.2189)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [577][300/391]\tTime 0.064 (0.049)\tData 0.031 (0.017)\tLoss 1.3579 (1.2236)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [577][350/391]\tTime 0.059 (0.049)\tData 0.028 (0.017)\tLoss 0.7396 (1.2338)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.102 (0.102)\tLoss 0.7306 (0.7306)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.019 (0.017)\tLoss 0.7188 (0.7476)\tPrec@1 96.875 (95.129)\n",
            " * Prec@1 95.210\n",
            "\n",
            "Epoch: 579/600\n",
            "Learning rate: 0.000431\n",
            "Epoch: [578][0/391]\tTime 0.200 (0.200)\tData 0.156 (0.156)\tLoss 1.4546 (1.4546)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [578][50/391]\tTime 0.066 (0.052)\tData 0.035 (0.018)\tLoss 1.4885 (1.2341)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [578][100/391]\tTime 0.064 (0.051)\tData 0.032 (0.018)\tLoss 1.5847 (1.2463)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [578][150/391]\tTime 0.042 (0.051)\tData 0.000 (0.017)\tLoss 1.5476 (1.2261)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [578][200/391]\tTime 0.042 (0.053)\tData 0.000 (0.017)\tLoss 0.7609 (1.2471)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [578][250/391]\tTime 0.035 (0.054)\tData 0.000 (0.017)\tLoss 1.4948 (1.2468)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [578][300/391]\tTime 0.032 (0.053)\tData 0.000 (0.016)\tLoss 1.0540 (1.2404)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [578][350/391]\tTime 0.033 (0.052)\tData 0.000 (0.017)\tLoss 0.9587 (1.2445)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.101 (0.101)\tLoss 0.6845 (0.6845)\tPrec@1 96.875 (96.875)\n",
            "Test: [50/79]\tTime 0.010 (0.016)\tLoss 0.6727 (0.7066)\tPrec@1 98.438 (95.665)\n",
            " * Prec@1 95.670\n",
            "\n",
            "Epoch: 580/600\n",
            "Learning rate: 0.000402\n",
            "Epoch: [579][0/391]\tTime 0.215 (0.215)\tData 0.159 (0.159)\tLoss 1.1280 (1.1280)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [579][50/391]\tTime 0.067 (0.052)\tData 0.035 (0.019)\tLoss 1.2722 (1.2726)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [579][100/391]\tTime 0.065 (0.050)\tData 0.033 (0.018)\tLoss 1.4417 (1.2526)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [579][150/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.2868 (1.2479)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [579][200/391]\tTime 0.064 (0.049)\tData 0.031 (0.017)\tLoss 0.8475 (1.2452)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [579][250/391]\tTime 0.041 (0.049)\tData 0.010 (0.017)\tLoss 1.2651 (1.2316)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [579][300/391]\tTime 0.033 (0.049)\tData 0.000 (0.017)\tLoss 0.9660 (1.2388)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [579][350/391]\tTime 0.031 (0.049)\tData 0.000 (0.017)\tLoss 0.7312 (1.2361)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.099 (0.099)\tLoss 0.6934 (0.6934)\tPrec@1 96.094 (96.094)\n",
            "Test: [50/79]\tTime 0.011 (0.016)\tLoss 0.6959 (0.7245)\tPrec@1 97.656 (95.389)\n",
            " * Prec@1 95.470\n",
            "\n",
            "Epoch: 581/600\n",
            "Learning rate: 0.000374\n",
            "Epoch: [580][0/391]\tTime 0.217 (0.217)\tData 0.160 (0.160)\tLoss 1.5158 (1.5158)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [580][50/391]\tTime 0.063 (0.051)\tData 0.030 (0.018)\tLoss 1.1513 (1.1779)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [580][100/391]\tTime 0.057 (0.050)\tData 0.025 (0.017)\tLoss 1.6046 (1.1853)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [580][150/391]\tTime 0.054 (0.049)\tData 0.023 (0.017)\tLoss 1.3904 (1.2193)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [580][200/391]\tTime 0.064 (0.049)\tData 0.031 (0.017)\tLoss 0.8915 (1.2147)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [580][250/391]\tTime 0.066 (0.049)\tData 0.034 (0.017)\tLoss 1.1076 (1.2148)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [580][300/391]\tTime 0.063 (0.049)\tData 0.031 (0.017)\tLoss 0.9151 (1.2206)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [580][350/391]\tTime 0.063 (0.049)\tData 0.030 (0.016)\tLoss 1.5134 (1.2344)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.101 (0.101)\tLoss 0.7035 (0.7035)\tPrec@1 96.094 (96.094)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.6861 (0.7188)\tPrec@1 97.656 (95.450)\n",
            " * Prec@1 95.500\n",
            "\n",
            "Epoch: 582/600\n",
            "Learning rate: 0.000347\n",
            "Epoch: [581][0/391]\tTime 0.202 (0.202)\tData 0.156 (0.156)\tLoss 1.0761 (1.0761)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [581][50/391]\tTime 0.062 (0.051)\tData 0.031 (0.018)\tLoss 1.5562 (1.2736)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [581][100/391]\tTime 0.072 (0.050)\tData 0.038 (0.017)\tLoss 1.4996 (1.2689)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [581][150/391]\tTime 0.062 (0.050)\tData 0.031 (0.017)\tLoss 1.3194 (1.2545)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [581][200/391]\tTime 0.042 (0.051)\tData 0.000 (0.016)\tLoss 1.1918 (1.2421)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [581][250/391]\tTime 0.061 (0.051)\tData 0.030 (0.016)\tLoss 0.8886 (1.2444)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [581][300/391]\tTime 0.066 (0.051)\tData 0.033 (0.016)\tLoss 1.0549 (1.2509)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [581][350/391]\tTime 0.064 (0.050)\tData 0.032 (0.016)\tLoss 1.5440 (1.2462)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.103 (0.103)\tLoss 0.6976 (0.6976)\tPrec@1 96.094 (96.094)\n",
            "Test: [50/79]\tTime 0.014 (0.016)\tLoss 0.6860 (0.7162)\tPrec@1 97.656 (95.435)\n",
            " * Prec@1 95.450\n",
            "\n",
            "Epoch: 583/600\n",
            "Learning rate: 0.000322\n",
            "Epoch: [582][0/391]\tTime 0.203 (0.203)\tData 0.156 (0.156)\tLoss 1.5415 (1.5415)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [582][50/391]\tTime 0.064 (0.051)\tData 0.030 (0.018)\tLoss 0.6635 (1.2275)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [582][100/391]\tTime 0.063 (0.050)\tData 0.029 (0.017)\tLoss 1.5437 (1.2544)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [582][150/391]\tTime 0.065 (0.049)\tData 0.032 (0.016)\tLoss 0.9957 (1.2554)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [582][200/391]\tTime 0.057 (0.049)\tData 0.020 (0.016)\tLoss 0.7472 (1.2602)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [582][250/391]\tTime 0.064 (0.049)\tData 0.031 (0.016)\tLoss 1.3882 (1.2587)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [582][300/391]\tTime 0.045 (0.049)\tData 0.013 (0.016)\tLoss 0.9793 (1.2540)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [582][350/391]\tTime 0.033 (0.049)\tData 0.000 (0.016)\tLoss 1.0692 (1.2414)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.102 (0.102)\tLoss 0.7017 (0.7017)\tPrec@1 96.094 (96.094)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.6830 (0.7199)\tPrec@1 97.656 (95.466)\n",
            " * Prec@1 95.460\n",
            "\n",
            "Epoch: 584/600\n",
            "Learning rate: 0.000298\n",
            "Epoch: [583][0/391]\tTime 0.214 (0.214)\tData 0.164 (0.164)\tLoss 1.4022 (1.4022)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [583][50/391]\tTime 0.064 (0.053)\tData 0.031 (0.018)\tLoss 1.3641 (1.2597)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [583][100/391]\tTime 0.065 (0.051)\tData 0.032 (0.017)\tLoss 1.4870 (1.2729)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [583][150/391]\tTime 0.064 (0.050)\tData 0.031 (0.016)\tLoss 1.4652 (1.2537)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [583][200/391]\tTime 0.066 (0.049)\tData 0.033 (0.016)\tLoss 1.5835 (1.2384)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [583][250/391]\tTime 0.071 (0.049)\tData 0.035 (0.016)\tLoss 1.5339 (1.2420)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [583][300/391]\tTime 0.064 (0.049)\tData 0.032 (0.016)\tLoss 1.4670 (1.2423)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [583][350/391]\tTime 0.061 (0.049)\tData 0.030 (0.016)\tLoss 1.1880 (1.2492)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.102 (0.102)\tLoss 0.7037 (0.7037)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.011 (0.016)\tLoss 0.6803 (0.7216)\tPrec@1 97.656 (95.450)\n",
            " * Prec@1 95.480\n",
            "\n",
            "Epoch: 585/600\n",
            "Learning rate: 0.000275\n",
            "Epoch: [584][0/391]\tTime 0.209 (0.209)\tData 0.160 (0.160)\tLoss 0.8448 (0.8448)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [584][50/391]\tTime 0.064 (0.052)\tData 0.030 (0.018)\tLoss 1.5244 (1.2509)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [584][100/391]\tTime 0.043 (0.051)\tData 0.000 (0.016)\tLoss 1.4868 (1.2110)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [584][150/391]\tTime 0.043 (0.054)\tData 0.000 (0.016)\tLoss 1.2774 (1.2169)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [584][200/391]\tTime 0.034 (0.055)\tData 0.000 (0.015)\tLoss 1.5390 (1.2230)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [584][250/391]\tTime 0.032 (0.053)\tData 0.000 (0.015)\tLoss 1.5232 (1.2356)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [584][300/391]\tTime 0.033 (0.053)\tData 0.000 (0.015)\tLoss 1.1037 (1.2439)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [584][350/391]\tTime 0.033 (0.052)\tData 0.000 (0.015)\tLoss 1.4530 (1.2412)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.102 (0.102)\tLoss 0.6827 (0.6827)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.011 (0.016)\tLoss 0.6554 (0.6963)\tPrec@1 97.656 (95.558)\n",
            " * Prec@1 95.610\n",
            "\n",
            "Epoch: 586/600\n",
            "Learning rate: 0.000254\n",
            "Epoch: [585][0/391]\tTime 0.217 (0.217)\tData 0.158 (0.158)\tLoss 1.5365 (1.5365)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [585][50/391]\tTime 0.049 (0.051)\tData 0.017 (0.017)\tLoss 1.3141 (1.2495)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [585][100/391]\tTime 0.044 (0.050)\tData 0.009 (0.016)\tLoss 1.2649 (1.2524)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [585][150/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.4465 (1.2704)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [585][200/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.3658 (1.2480)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [585][250/391]\tTime 0.032 (0.049)\tData 0.000 (0.015)\tLoss 0.7730 (1.2412)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [585][300/391]\tTime 0.033 (0.049)\tData 0.000 (0.016)\tLoss 1.4820 (1.2464)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [585][350/391]\tTime 0.077 (0.050)\tData 0.032 (0.015)\tLoss 0.6206 (1.2439)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.101 (0.101)\tLoss 0.7274 (0.7274)\tPrec@1 94.531 (94.531)\n",
            "Test: [50/79]\tTime 0.020 (0.016)\tLoss 0.7136 (0.7431)\tPrec@1 96.875 (95.113)\n",
            " * Prec@1 95.250\n",
            "\n",
            "Epoch: 587/600\n",
            "Learning rate: 0.000234\n",
            "Epoch: [586][0/391]\tTime 0.203 (0.203)\tData 0.157 (0.157)\tLoss 1.1548 (1.1548)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [586][50/391]\tTime 0.069 (0.052)\tData 0.036 (0.019)\tLoss 1.4407 (1.2292)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [586][100/391]\tTime 0.062 (0.051)\tData 0.029 (0.018)\tLoss 0.7049 (1.2341)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [586][150/391]\tTime 0.068 (0.050)\tData 0.035 (0.017)\tLoss 1.4978 (1.2397)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [586][200/391]\tTime 0.066 (0.050)\tData 0.033 (0.017)\tLoss 1.4065 (1.2469)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [586][250/391]\tTime 0.065 (0.050)\tData 0.033 (0.017)\tLoss 1.2494 (1.2458)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [586][300/391]\tTime 0.066 (0.050)\tData 0.034 (0.017)\tLoss 1.5641 (1.2450)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [586][350/391]\tTime 0.063 (0.050)\tData 0.030 (0.017)\tLoss 1.4891 (1.2376)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.101 (0.101)\tLoss 0.7016 (0.7016)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.011 (0.016)\tLoss 0.6854 (0.7215)\tPrec@1 97.656 (95.496)\n",
            " * Prec@1 95.540\n",
            "\n",
            "Epoch: 588/600\n",
            "Learning rate: 0.000216\n",
            "Epoch: [587][0/391]\tTime 0.202 (0.202)\tData 0.156 (0.156)\tLoss 1.5816 (1.5816)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [587][50/391]\tTime 0.064 (0.051)\tData 0.031 (0.018)\tLoss 1.4957 (1.2665)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [587][100/391]\tTime 0.065 (0.050)\tData 0.032 (0.017)\tLoss 0.9881 (1.2277)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [587][150/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.5414 (1.2189)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [587][200/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.4588 (1.2272)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [587][250/391]\tTime 0.063 (0.049)\tData 0.031 (0.016)\tLoss 1.2007 (1.2195)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [587][300/391]\tTime 0.064 (0.049)\tData 0.031 (0.016)\tLoss 1.4345 (1.2177)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [587][350/391]\tTime 0.063 (0.049)\tData 0.030 (0.016)\tLoss 1.4782 (1.2244)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.099 (0.099)\tLoss 0.7045 (0.7045)\tPrec@1 94.531 (94.531)\n",
            "Test: [50/79]\tTime 0.011 (0.017)\tLoss 0.6791 (0.7185)\tPrec@1 97.656 (95.435)\n",
            " * Prec@1 95.540\n",
            "\n",
            "Epoch: 589/600\n",
            "Learning rate: 0.000199\n",
            "Epoch: [588][0/391]\tTime 0.217 (0.217)\tData 0.158 (0.158)\tLoss 1.2971 (1.2971)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [588][50/391]\tTime 0.032 (0.052)\tData 0.000 (0.018)\tLoss 1.3956 (1.1212)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [588][100/391]\tTime 0.032 (0.050)\tData 0.000 (0.018)\tLoss 0.8600 (1.1800)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [588][150/391]\tTime 0.034 (0.050)\tData 0.000 (0.017)\tLoss 1.5288 (1.2117)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [588][200/391]\tTime 0.033 (0.049)\tData 0.000 (0.017)\tLoss 1.5308 (1.2110)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [588][250/391]\tTime 0.031 (0.049)\tData 0.000 (0.016)\tLoss 0.6667 (1.2146)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [588][300/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.0721 (1.2262)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [588][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.0647 (1.2245)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.102 (0.102)\tLoss 0.7222 (0.7222)\tPrec@1 96.094 (96.094)\n",
            "Test: [50/79]\tTime 0.014 (0.016)\tLoss 0.7059 (0.7432)\tPrec@1 97.656 (95.297)\n",
            " * Prec@1 95.370\n",
            "\n",
            "Epoch: 590/600\n",
            "Learning rate: 0.000183\n",
            "Epoch: [589][0/391]\tTime 0.203 (0.203)\tData 0.157 (0.157)\tLoss 0.7881 (0.7881)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [589][50/391]\tTime 0.054 (0.051)\tData 0.021 (0.018)\tLoss 1.3733 (1.2421)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [589][100/391]\tTime 0.031 (0.050)\tData 0.000 (0.017)\tLoss 1.1165 (1.2273)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [589][150/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 0.9941 (1.2422)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [589][200/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.1635 (1.2355)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [589][250/391]\tTime 0.034 (0.049)\tData 0.000 (0.017)\tLoss 1.2681 (1.2392)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [589][300/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.1675 (1.2422)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [589][350/391]\tTime 0.034 (0.049)\tData 0.000 (0.016)\tLoss 1.0445 (1.2432)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.101 (0.101)\tLoss 0.7110 (0.7110)\tPrec@1 94.531 (94.531)\n",
            "Test: [50/79]\tTime 0.017 (0.016)\tLoss 0.6961 (0.7286)\tPrec@1 96.875 (95.175)\n",
            " * Prec@1 95.330\n",
            "\n",
            "Epoch: 591/600\n",
            "Learning rate: 0.000168\n",
            "Epoch: [590][0/391]\tTime 0.203 (0.203)\tData 0.155 (0.155)\tLoss 1.1790 (1.1790)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [590][50/391]\tTime 0.064 (0.051)\tData 0.032 (0.018)\tLoss 1.5392 (1.3081)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [590][100/391]\tTime 0.050 (0.049)\tData 0.018 (0.017)\tLoss 1.2267 (1.2655)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [590][150/391]\tTime 0.063 (0.050)\tData 0.032 (0.017)\tLoss 1.4021 (1.2664)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [590][200/391]\tTime 0.063 (0.049)\tData 0.030 (0.017)\tLoss 1.4138 (1.2595)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [590][250/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.4207 (1.2550)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [590][300/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 0.8624 (1.2462)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [590][350/391]\tTime 0.074 (0.049)\tData 0.033 (0.016)\tLoss 1.5205 (1.2453)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.100 (0.100)\tLoss 0.6956 (0.6956)\tPrec@1 96.094 (96.094)\n",
            "Test: [50/79]\tTime 0.010 (0.016)\tLoss 0.6820 (0.7151)\tPrec@1 96.875 (95.542)\n",
            " * Prec@1 95.590\n",
            "\n",
            "Epoch: 592/600\n",
            "Learning rate: 0.000155\n",
            "Epoch: [591][0/391]\tTime 0.209 (0.209)\tData 0.158 (0.158)\tLoss 1.4425 (1.4425)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [591][50/391]\tTime 0.064 (0.051)\tData 0.031 (0.018)\tLoss 1.4094 (1.2682)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [591][100/391]\tTime 0.066 (0.050)\tData 0.034 (0.017)\tLoss 1.1312 (1.2817)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [591][150/391]\tTime 0.065 (0.050)\tData 0.032 (0.017)\tLoss 1.3098 (1.2342)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [591][200/391]\tTime 0.064 (0.049)\tData 0.033 (0.017)\tLoss 1.5269 (1.2259)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [591][250/391]\tTime 0.064 (0.049)\tData 0.031 (0.017)\tLoss 0.8545 (1.2176)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [591][300/391]\tTime 0.065 (0.049)\tData 0.034 (0.016)\tLoss 0.8100 (1.2096)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [591][350/391]\tTime 0.063 (0.049)\tData 0.032 (0.016)\tLoss 1.5215 (1.2033)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.100 (0.100)\tLoss 0.6925 (0.6925)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.010 (0.016)\tLoss 0.6718 (0.7093)\tPrec@1 96.875 (95.466)\n",
            " * Prec@1 95.510\n",
            "\n",
            "Epoch: 593/600\n",
            "Learning rate: 0.000144\n",
            "Epoch: [592][0/391]\tTime 0.203 (0.203)\tData 0.155 (0.155)\tLoss 1.3933 (1.3933)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [592][50/391]\tTime 0.063 (0.050)\tData 0.032 (0.018)\tLoss 1.4754 (1.2559)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [592][100/391]\tTime 0.033 (0.049)\tData 0.001 (0.017)\tLoss 1.3761 (1.2440)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [592][150/391]\tTime 0.035 (0.049)\tData 0.003 (0.017)\tLoss 1.3137 (1.2468)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [592][200/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.2726 (1.2481)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [592][250/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.5176 (1.2488)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [592][300/391]\tTime 0.033 (0.049)\tData 0.000 (0.017)\tLoss 1.4564 (1.2442)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [592][350/391]\tTime 0.033 (0.049)\tData 0.000 (0.016)\tLoss 1.5258 (1.2299)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.101 (0.101)\tLoss 0.7123 (0.7123)\tPrec@1 96.094 (96.094)\n",
            "Test: [50/79]\tTime 0.019 (0.016)\tLoss 0.6987 (0.7314)\tPrec@1 96.875 (95.267)\n",
            " * Prec@1 95.370\n",
            "\n",
            "Epoch: 594/600\n",
            "Learning rate: 0.000134\n",
            "Epoch: [593][0/391]\tTime 0.207 (0.207)\tData 0.160 (0.160)\tLoss 1.1977 (1.1977)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [593][50/391]\tTime 0.064 (0.051)\tData 0.033 (0.019)\tLoss 1.2855 (1.2905)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [593][100/391]\tTime 0.068 (0.050)\tData 0.035 (0.018)\tLoss 1.5405 (1.2571)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [593][150/391]\tTime 0.065 (0.049)\tData 0.033 (0.017)\tLoss 1.4007 (1.2575)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [593][200/391]\tTime 0.063 (0.049)\tData 0.031 (0.017)\tLoss 0.8852 (1.2406)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [593][250/391]\tTime 0.065 (0.049)\tData 0.031 (0.017)\tLoss 0.6992 (1.2369)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [593][300/391]\tTime 0.064 (0.049)\tData 0.032 (0.017)\tLoss 1.5272 (1.2455)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [593][350/391]\tTime 0.064 (0.049)\tData 0.031 (0.016)\tLoss 0.9578 (1.2395)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.100 (0.100)\tLoss 0.7160 (0.7160)\tPrec@1 96.094 (96.094)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.7034 (0.7355)\tPrec@1 96.875 (95.374)\n",
            " * Prec@1 95.390\n",
            "\n",
            "Epoch: 595/600\n",
            "Learning rate: 0.000125\n",
            "Epoch: [594][0/391]\tTime 0.202 (0.202)\tData 0.155 (0.155)\tLoss 1.2837 (1.2837)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [594][50/391]\tTime 0.063 (0.051)\tData 0.030 (0.018)\tLoss 1.4561 (1.1671)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [594][100/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.4248 (1.2114)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [594][150/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 0.7450 (1.1935)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [594][200/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.3158 (1.2052)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [594][250/391]\tTime 0.033 (0.049)\tData 0.000 (0.016)\tLoss 1.5476 (1.2138)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [594][300/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.0083 (1.2149)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [594][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.4923 (1.2300)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.100 (0.100)\tLoss 0.7034 (0.7034)\tPrec@1 96.094 (96.094)\n",
            "Test: [50/79]\tTime 0.015 (0.016)\tLoss 0.6854 (0.7184)\tPrec@1 96.875 (95.450)\n",
            " * Prec@1 95.470\n",
            "\n",
            "Epoch: 596/600\n",
            "Learning rate: 0.000117\n",
            "Epoch: [595][0/391]\tTime 0.203 (0.203)\tData 0.156 (0.156)\tLoss 1.3258 (1.3258)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [595][50/391]\tTime 0.062 (0.051)\tData 0.030 (0.018)\tLoss 0.9832 (1.2139)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [595][100/391]\tTime 0.063 (0.049)\tData 0.030 (0.017)\tLoss 1.5465 (1.2173)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [595][150/391]\tTime 0.064 (0.049)\tData 0.033 (0.017)\tLoss 1.5386 (1.2353)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [595][200/391]\tTime 0.063 (0.049)\tData 0.031 (0.017)\tLoss 0.7195 (1.2282)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [595][250/391]\tTime 0.062 (0.049)\tData 0.031 (0.016)\tLoss 1.1650 (1.2268)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [595][300/391]\tTime 0.063 (0.048)\tData 0.031 (0.016)\tLoss 0.8274 (1.2259)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [595][350/391]\tTime 0.064 (0.048)\tData 0.031 (0.016)\tLoss 1.5436 (1.2310)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.102 (0.102)\tLoss 0.6868 (0.6868)\tPrec@1 94.531 (94.531)\n",
            "Test: [50/79]\tTime 0.018 (0.016)\tLoss 0.6571 (0.6953)\tPrec@1 97.656 (95.604)\n",
            " * Prec@1 95.650\n",
            "\n",
            "Epoch: 597/600\n",
            "Learning rate: 0.000111\n",
            "Epoch: [596][0/391]\tTime 0.221 (0.221)\tData 0.162 (0.162)\tLoss 0.6650 (0.6650)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [596][50/391]\tTime 0.076 (0.062)\tData 0.034 (0.019)\tLoss 1.4157 (1.2458)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [596][100/391]\tTime 0.066 (0.058)\tData 0.034 (0.018)\tLoss 1.2460 (1.2191)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [596][150/391]\tTime 0.067 (0.055)\tData 0.036 (0.017)\tLoss 0.9473 (1.2303)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [596][200/391]\tTime 0.068 (0.054)\tData 0.035 (0.017)\tLoss 1.3326 (1.2345)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [596][250/391]\tTime 0.064 (0.053)\tData 0.032 (0.017)\tLoss 1.2924 (1.2280)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [596][300/391]\tTime 0.065 (0.052)\tData 0.032 (0.017)\tLoss 1.4696 (1.2434)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [596][350/391]\tTime 0.052 (0.052)\tData 0.021 (0.017)\tLoss 1.4067 (1.2473)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.103 (0.103)\tLoss 0.6969 (0.6969)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.022 (0.017)\tLoss 0.6659 (0.7040)\tPrec@1 96.875 (95.619)\n",
            " * Prec@1 95.590\n",
            "\n",
            "Epoch: 598/600\n",
            "Learning rate: 0.000106\n",
            "Epoch: [597][0/391]\tTime 0.199 (0.199)\tData 0.153 (0.153)\tLoss 1.2800 (1.2800)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [597][50/391]\tTime 0.065 (0.051)\tData 0.032 (0.018)\tLoss 1.3213 (1.2449)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [597][100/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 0.9650 (1.2493)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [597][150/391]\tTime 0.033 (0.049)\tData 0.000 (0.016)\tLoss 0.9477 (1.2651)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [597][200/391]\tTime 0.037 (0.049)\tData 0.000 (0.016)\tLoss 0.9182 (1.2487)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [597][250/391]\tTime 0.031 (0.049)\tData 0.000 (0.016)\tLoss 1.0728 (1.2571)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [597][300/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.3328 (1.2602)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [597][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.016)\tLoss 1.0354 (1.2599)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.100 (0.100)\tLoss 0.6958 (0.6958)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.010 (0.016)\tLoss 0.6700 (0.7085)\tPrec@1 97.656 (95.542)\n",
            " * Prec@1 95.540\n",
            "\n",
            "Epoch: 599/600\n",
            "Learning rate: 0.000103\n",
            "Epoch: [598][0/391]\tTime 0.210 (0.210)\tData 0.164 (0.164)\tLoss 1.1892 (1.1892)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [598][50/391]\tTime 0.051 (0.051)\tData 0.019 (0.018)\tLoss 0.6851 (1.2099)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [598][100/391]\tTime 0.032 (0.050)\tData 0.000 (0.017)\tLoss 1.1684 (1.2285)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [598][150/391]\tTime 0.044 (0.049)\tData 0.013 (0.017)\tLoss 0.7483 (1.2174)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [598][200/391]\tTime 0.060 (0.049)\tData 0.028 (0.017)\tLoss 1.5658 (1.2308)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [598][250/391]\tTime 0.034 (0.049)\tData 0.002 (0.016)\tLoss 1.4589 (1.2425)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [598][300/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.5735 (1.2417)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [598][350/391]\tTime 0.032 (0.049)\tData 0.000 (0.017)\tLoss 1.0237 (1.2399)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.102 (0.102)\tLoss 0.6849 (0.6849)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.011 (0.016)\tLoss 0.6564 (0.6971)\tPrec@1 97.656 (95.650)\n",
            " * Prec@1 95.680\n",
            "\n",
            "Epoch: 600/600\n",
            "Learning rate: 0.000101\n",
            "Epoch: [599][0/391]\tTime 0.203 (0.203)\tData 0.158 (0.158)\tLoss 0.7560 (0.7560)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [599][50/391]\tTime 0.068 (0.051)\tData 0.034 (0.018)\tLoss 1.4785 (1.2651)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [599][100/391]\tTime 0.065 (0.050)\tData 0.033 (0.018)\tLoss 1.1933 (1.2537)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [599][150/391]\tTime 0.065 (0.049)\tData 0.032 (0.017)\tLoss 0.9316 (1.2271)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [599][200/391]\tTime 0.054 (0.049)\tData 0.022 (0.017)\tLoss 1.1952 (1.2304)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [599][250/391]\tTime 0.076 (0.050)\tData 0.034 (0.017)\tLoss 0.7850 (1.2434)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [599][300/391]\tTime 0.065 (0.049)\tData 0.032 (0.017)\tLoss 1.0184 (1.2435)\tPrec@1 0.000 (0.000)\n",
            "Epoch: [599][350/391]\tTime 0.067 (0.049)\tData 0.036 (0.017)\tLoss 1.4741 (1.2474)\tPrec@1 0.000 (0.000)\n",
            "Test: [0/79]\tTime 0.100 (0.100)\tLoss 0.7158 (0.7158)\tPrec@1 95.312 (95.312)\n",
            "Test: [50/79]\tTime 0.010 (0.016)\tLoss 0.6945 (0.7331)\tPrec@1 97.656 (95.466)\n",
            " * Prec@1 95.500\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(HybridResNet(\n",
              "   (conv1): Conv2d(3, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "   (bn1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "   (layer1): Sequential(\n",
              "     (0): BasicBlock(\n",
              "       (conv1): Conv2d(22, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(22, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (dropout): Dropout(p=0.15, inplace=False)\n",
              "       (se): SELayer(\n",
              "         (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "         (fc): Sequential(\n",
              "           (0): Linear(in_features=22, out_features=1, bias=False)\n",
              "           (1): ReLU(inplace=True)\n",
              "           (2): Linear(in_features=1, out_features=22, bias=False)\n",
              "           (3): Sigmoid()\n",
              "         )\n",
              "       )\n",
              "       (shortcut): Sequential()\n",
              "     )\n",
              "     (1): BasicBlock(\n",
              "       (conv1): Conv2d(22, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(22, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (dropout): Dropout(p=0.15, inplace=False)\n",
              "       (se): SELayer(\n",
              "         (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "         (fc): Sequential(\n",
              "           (0): Linear(in_features=22, out_features=1, bias=False)\n",
              "           (1): ReLU(inplace=True)\n",
              "           (2): Linear(in_features=1, out_features=22, bias=False)\n",
              "           (3): Sigmoid()\n",
              "         )\n",
              "       )\n",
              "       (shortcut): Sequential()\n",
              "     )\n",
              "     (2): BasicBlock(\n",
              "       (conv1): Conv2d(22, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(22, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (dropout): Dropout(p=0.15, inplace=False)\n",
              "       (se): SELayer(\n",
              "         (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "         (fc): Sequential(\n",
              "           (0): Linear(in_features=22, out_features=1, bias=False)\n",
              "           (1): ReLU(inplace=True)\n",
              "           (2): Linear(in_features=1, out_features=22, bias=False)\n",
              "           (3): Sigmoid()\n",
              "         )\n",
              "       )\n",
              "       (shortcut): Sequential()\n",
              "     )\n",
              "   )\n",
              "   (layer2): Sequential(\n",
              "     (0): BasicBlock(\n",
              "       (conv1): Conv2d(22, 44, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(44, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (dropout): Dropout(p=0.15, inplace=False)\n",
              "       (se): SELayer(\n",
              "         (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "         (fc): Sequential(\n",
              "           (0): Linear(in_features=44, out_features=2, bias=False)\n",
              "           (1): ReLU(inplace=True)\n",
              "           (2): Linear(in_features=2, out_features=44, bias=False)\n",
              "           (3): Sigmoid()\n",
              "         )\n",
              "       )\n",
              "       (shortcut): Sequential(\n",
              "         (0): Conv2d(22, 44, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "         (1): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       )\n",
              "     )\n",
              "     (1): BasicBlock(\n",
              "       (conv1): Conv2d(44, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(44, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (dropout): Dropout(p=0.15, inplace=False)\n",
              "       (se): SELayer(\n",
              "         (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "         (fc): Sequential(\n",
              "           (0): Linear(in_features=44, out_features=2, bias=False)\n",
              "           (1): ReLU(inplace=True)\n",
              "           (2): Linear(in_features=2, out_features=44, bias=False)\n",
              "           (3): Sigmoid()\n",
              "         )\n",
              "       )\n",
              "       (shortcut): Sequential()\n",
              "     )\n",
              "     (2): BasicBlock(\n",
              "       (conv1): Conv2d(44, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(44, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (dropout): Dropout(p=0.15, inplace=False)\n",
              "       (se): SELayer(\n",
              "         (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "         (fc): Sequential(\n",
              "           (0): Linear(in_features=44, out_features=2, bias=False)\n",
              "           (1): ReLU(inplace=True)\n",
              "           (2): Linear(in_features=2, out_features=44, bias=False)\n",
              "           (3): Sigmoid()\n",
              "         )\n",
              "       )\n",
              "       (shortcut): Sequential()\n",
              "     )\n",
              "     (3): BasicBlock(\n",
              "       (conv1): Conv2d(44, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(44, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (dropout): Dropout(p=0.15, inplace=False)\n",
              "       (se): SELayer(\n",
              "         (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "         (fc): Sequential(\n",
              "           (0): Linear(in_features=44, out_features=2, bias=False)\n",
              "           (1): ReLU(inplace=True)\n",
              "           (2): Linear(in_features=2, out_features=44, bias=False)\n",
              "           (3): Sigmoid()\n",
              "         )\n",
              "       )\n",
              "       (shortcut): Sequential()\n",
              "     )\n",
              "   )\n",
              "   (layer3): Sequential(\n",
              "     (0): BasicBlock(\n",
              "       (conv1): Conv2d(44, 89, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(89, 89, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (dropout): Dropout(p=0.15, inplace=False)\n",
              "       (se): SELayer(\n",
              "         (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "         (fc): Sequential(\n",
              "           (0): Linear(in_features=89, out_features=5, bias=False)\n",
              "           (1): ReLU(inplace=True)\n",
              "           (2): Linear(in_features=5, out_features=89, bias=False)\n",
              "           (3): Sigmoid()\n",
              "         )\n",
              "       )\n",
              "       (shortcut): Sequential(\n",
              "         (0): Conv2d(44, 89, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "         (1): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       )\n",
              "     )\n",
              "     (1): BasicBlock(\n",
              "       (conv1): Conv2d(89, 89, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(89, 89, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (dropout): Dropout(p=0.15, inplace=False)\n",
              "       (se): SELayer(\n",
              "         (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "         (fc): Sequential(\n",
              "           (0): Linear(in_features=89, out_features=5, bias=False)\n",
              "           (1): ReLU(inplace=True)\n",
              "           (2): Linear(in_features=5, out_features=89, bias=False)\n",
              "           (3): Sigmoid()\n",
              "         )\n",
              "       )\n",
              "       (shortcut): Sequential()\n",
              "     )\n",
              "     (2): BasicBlock(\n",
              "       (conv1): Conv2d(89, 89, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(89, 89, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (dropout): Dropout(p=0.15, inplace=False)\n",
              "       (se): SELayer(\n",
              "         (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "         (fc): Sequential(\n",
              "           (0): Linear(in_features=89, out_features=5, bias=False)\n",
              "           (1): ReLU(inplace=True)\n",
              "           (2): Linear(in_features=5, out_features=89, bias=False)\n",
              "           (3): Sigmoid()\n",
              "         )\n",
              "       )\n",
              "       (shortcut): Sequential()\n",
              "     )\n",
              "     (3): BasicBlock(\n",
              "       (conv1): Conv2d(89, 89, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(89, 89, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (dropout): Dropout(p=0.15, inplace=False)\n",
              "       (se): SELayer(\n",
              "         (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "         (fc): Sequential(\n",
              "           (0): Linear(in_features=89, out_features=5, bias=False)\n",
              "           (1): ReLU(inplace=True)\n",
              "           (2): Linear(in_features=5, out_features=89, bias=False)\n",
              "           (3): Sigmoid()\n",
              "         )\n",
              "       )\n",
              "       (shortcut): Sequential()\n",
              "     )\n",
              "     (4): BasicBlock(\n",
              "       (conv1): Conv2d(89, 89, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(89, 89, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (dropout): Dropout(p=0.15, inplace=False)\n",
              "       (se): SELayer(\n",
              "         (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "         (fc): Sequential(\n",
              "           (0): Linear(in_features=89, out_features=5, bias=False)\n",
              "           (1): ReLU(inplace=True)\n",
              "           (2): Linear(in_features=5, out_features=89, bias=False)\n",
              "           (3): Sigmoid()\n",
              "         )\n",
              "       )\n",
              "       (shortcut): Sequential()\n",
              "     )\n",
              "     (5): BasicBlock(\n",
              "       (conv1): Conv2d(89, 89, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(89, 89, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (dropout): Dropout(p=0.15, inplace=False)\n",
              "       (se): SELayer(\n",
              "         (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "         (fc): Sequential(\n",
              "           (0): Linear(in_features=89, out_features=5, bias=False)\n",
              "           (1): ReLU(inplace=True)\n",
              "           (2): Linear(in_features=5, out_features=89, bias=False)\n",
              "           (3): Sigmoid()\n",
              "         )\n",
              "       )\n",
              "       (shortcut): Sequential()\n",
              "     )\n",
              "   )\n",
              "   (layer4): Sequential(\n",
              "     (0): BasicBlock(\n",
              "       (conv1): Conv2d(89, 179, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(179, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(179, 179, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(179, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (dropout): Dropout(p=0.15, inplace=False)\n",
              "       (se): SELayer(\n",
              "         (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "         (fc): Sequential(\n",
              "           (0): Linear(in_features=179, out_features=11, bias=False)\n",
              "           (1): ReLU(inplace=True)\n",
              "           (2): Linear(in_features=11, out_features=179, bias=False)\n",
              "           (3): Sigmoid()\n",
              "         )\n",
              "       )\n",
              "       (shortcut): Sequential(\n",
              "         (0): Conv2d(89, 179, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "         (1): BatchNorm2d(179, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       )\n",
              "     )\n",
              "     (1): BasicBlock(\n",
              "       (conv1): Conv2d(179, 179, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(179, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(179, 179, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(179, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (dropout): Dropout(p=0.15, inplace=False)\n",
              "       (se): SELayer(\n",
              "         (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "         (fc): Sequential(\n",
              "           (0): Linear(in_features=179, out_features=11, bias=False)\n",
              "           (1): ReLU(inplace=True)\n",
              "           (2): Linear(in_features=11, out_features=179, bias=False)\n",
              "           (3): Sigmoid()\n",
              "         )\n",
              "       )\n",
              "       (shortcut): Sequential()\n",
              "     )\n",
              "     (2): BasicBlock(\n",
              "       (conv1): Conv2d(179, 179, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn1): BatchNorm2d(179, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (conv2): Conv2d(179, 179, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "       (bn2): BatchNorm2d(179, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "       (dropout): Dropout(p=0.15, inplace=False)\n",
              "       (se): SELayer(\n",
              "         (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "         (fc): Sequential(\n",
              "           (0): Linear(in_features=179, out_features=11, bias=False)\n",
              "           (1): ReLU(inplace=True)\n",
              "           (2): Linear(in_features=11, out_features=179, bias=False)\n",
              "           (3): Sigmoid()\n",
              "         )\n",
              "       )\n",
              "       (shortcut): Sequential()\n",
              "     )\n",
              "   )\n",
              "   (dropout): Dropout(p=0.15, inplace=False)\n",
              "   (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "   (fc): Linear(in_features=179, out_features=10, bias=True)\n",
              " ),\n",
              " 95.68)"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "#---------- MODEL DEFINITION ----------#\n",
        "\n",
        "class SELayer(nn.Module):\n",
        "    \"\"\"Squeeze-and-Excitation block for channel attention\"\"\"\n",
        "    def __init__(self, channel, reduction=16):\n",
        "        super(SELayer, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channel, channel // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channel // reduction, channel, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    \"\"\"Basic ResNet block with two 3x3 convolutions and a skip connection\"\"\"\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, dropout_rate=0.0, use_se=False):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.dropout = nn.Dropout(dropout_rate) if dropout_rate > 0 else None\n",
        "\n",
        "        # Optional SE block\n",
        "        self.use_se = use_se\n",
        "        if use_se:\n",
        "            self.se = SELayer(planes, reduction=16)\n",
        "\n",
        "        # Skip connection handling\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        if self.dropout is not None:\n",
        "            out = self.dropout(out)\n",
        "        out = self.bn2(self.conv2(out))\n",
        "\n",
        "        if self.use_se:\n",
        "            out = self.se(out)\n",
        "\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class BottleneckBlock(nn.Module):\n",
        "    \"\"\"Bottleneck block with 1x1, 3x3, 1x1 convolutions as shown in the tutorial\"\"\"\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, dropout_rate=0.0, use_se=False):\n",
        "        super(BottleneckBlock, self).__init__()\n",
        "        # First 1x1 conv to reduce channels\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        # 3x3 conv with potential stride\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        # Second 1x1 conv to increase channels (expansion)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_rate) if dropout_rate > 0 else None\n",
        "\n",
        "        # Optional SE block\n",
        "        self.use_se = use_se\n",
        "        if use_se:\n",
        "            self.se = SELayer(planes * self.expansion, reduction=16)\n",
        "\n",
        "        # Skip connection handling\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes * self.expansion:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, planes * self.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * self.expansion)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        if self.dropout is not None:\n",
        "            out = self.dropout(out)\n",
        "        out = self.bn3(self.conv3(out))\n",
        "\n",
        "        if self.use_se:\n",
        "            out = self.se(out)\n",
        "\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class HybridResNet(nn.Module):\n",
        "    \"\"\"Hybrid ResNet combining features from both implementations\"\"\"\n",
        "    def __init__(self, block, layers, num_classes=10, dropout_rate=0.0, use_se=False,\n",
        "                 initial_channels=32, width_multiplier=1.0):\n",
        "        super(HybridResNet, self).__init__()\n",
        "        self.in_planes = int(initial_channels * width_multiplier)\n",
        "\n",
        "        # Initial convolution\n",
        "        self.conv1 = nn.Conv2d(3, self.in_planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(self.in_planes)\n",
        "\n",
        "        # Create stages with progressively more channels\n",
        "        # Adjusted to use width_multiplier for controlled scaling\n",
        "        self.layer1 = self._make_layer(block, int(32 * width_multiplier), layers[0], stride=1,\n",
        "                                     dropout_rate=dropout_rate, use_se=use_se)\n",
        "        self.layer2 = self._make_layer(block, int(64 * width_multiplier), layers[1], stride=2,\n",
        "                                     dropout_rate=dropout_rate, use_se=use_se)\n",
        "        self.layer3 = self._make_layer(block, int(128 * width_multiplier), layers[2], stride=2,\n",
        "                                     dropout_rate=dropout_rate, use_se=use_se)\n",
        "        self.layer4 = self._make_layer(block, int(256 * width_multiplier), layers[3], stride=2,\n",
        "                                     dropout_rate=dropout_rate, use_se=use_se)\n",
        "\n",
        "        # Final classifier\n",
        "        self.dropout = nn.Dropout(dropout_rate) if dropout_rate > 0 else None\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        final_channels = int(256 * width_multiplier) * block.expansion\n",
        "        self.fc = nn.Linear(final_channels, num_classes)\n",
        "\n",
        "        # Initialize weights\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride, dropout_rate=0.0, use_se=False):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride, dropout_rate, use_se))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        if self.dropout is not None:\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Model configurations\n",
        "def ResNet18_Hybrid(num_classes=10, use_se=False, dropout_rate=0.0, width_multiplier=1.0):\n",
        "    \"\"\"ResNet-18 with hybrid features\"\"\"\n",
        "    return HybridResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes,\n",
        "                      dropout_rate=dropout_rate, use_se=use_se, width_multiplier=width_multiplier)\n",
        "\n",
        "def ResNet34_Hybrid(num_classes=10, use_se=False, dropout_rate=0.0, width_multiplier=1.0):\n",
        "    \"\"\"ResNet-34 with hybrid features\"\"\"\n",
        "    return HybridResNet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes,\n",
        "                      dropout_rate=dropout_rate, use_se=use_se, width_multiplier=width_multiplier)\n",
        "\n",
        "def ResNet50_Hybrid(num_classes=10, use_se=False, dropout_rate=0.0, width_multiplier=0.5):\n",
        "    \"\"\"ResNet-50 with hybrid features - scaled down for CIFAR-10\"\"\"\n",
        "    return HybridResNet(BottleneckBlock, [3, 4, 6, 3], num_classes=num_classes,\n",
        "                      dropout_rate=dropout_rate, use_se=use_se, width_multiplier=width_multiplier)\n",
        "\n",
        "# Optimized models specifically designed to be under 5M parameters\n",
        "def OptimizedResNet_A(num_classes=10):\n",
        "    \"\"\"Optimized ResNet with basic blocks and SE attention - around 2.5M parameters\"\"\"\n",
        "    return ResNet18_Hybrid(num_classes=num_classes, use_se=True, dropout_rate=0.1, width_multiplier=0.75)\n",
        "\n",
        "def OptimizedResNet_B(num_classes=10):\n",
        "    \"\"\"Optimized ResNet with basic blocks and focused hyperparameters - around 4.2M parameters\"\"\"\n",
        "    return ResNet34_Hybrid(num_classes=num_classes, use_se=True, dropout_rate=0.15, width_multiplier=0.7)\n",
        "\n",
        "def OptimizedResNet_C(num_classes=10):\n",
        "    \"\"\"Optimized ResNet with bottleneck blocks - most parameter efficient\"\"\"\n",
        "    return ResNet50_Hybrid(num_classes=num_classes, use_se=True, dropout_rate=0.2, width_multiplier=0.25)\n",
        "\n",
        "def project1_model():\n",
        "    \"\"\"Drop-in replacement for the original project1_model function\"\"\"\n",
        "    # Replace with one of the optimized models based on preference\n",
        "    return OptimizedResNet_B(num_classes=10)\n",
        "\n",
        "def count_parameters(model):\n",
        "    \"\"\"Count the total number of trainable parameters in a model\"\"\"\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "#---------- TRAINING UTILITIES ----------#\n",
        "\n",
        "class CutoutTransform:\n",
        "    \"\"\"Cutout data augmentation as shown in the tutorial\"\"\"\n",
        "    def __init__(self, n_holes=1, length=16):\n",
        "        self.n_holes = n_holes\n",
        "        self.length = length\n",
        "\n",
        "    def __call__(self, img):\n",
        "        h = img.size(1)\n",
        "        w = img.size(2)\n",
        "\n",
        "        mask = torch.ones((h, w))\n",
        "\n",
        "        for n in range(self.n_holes):\n",
        "            y = np.random.randint(h)\n",
        "            x = np.random.randint(w)\n",
        "\n",
        "            y1 = np.clip(y - self.length // 2, 0, h)\n",
        "            y2 = np.clip(y + self.length // 2, 0, h)\n",
        "            x1 = np.clip(x - self.length // 2, 0, w)\n",
        "            x2 = np.clip(x + self.length // 2, 0, w)\n",
        "\n",
        "            mask[y1:y2, x1:x2] = 0\n",
        "\n",
        "        mask = mask.expand_as(img)\n",
        "        img = img * mask\n",
        "\n",
        "        return img\n",
        "\n",
        "\n",
        "class MixupTransform:\n",
        "    \"\"\"Mixup augmentation as shown in the tutorial\"\"\"\n",
        "    def __init__(self, alpha=1.0):\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def __call__(self, batch, targets):\n",
        "        if self.alpha > 0:\n",
        "            lam = np.random.beta(self.alpha, self.alpha)\n",
        "        else:\n",
        "            lam = 1\n",
        "\n",
        "        batch_size = batch.size(0)\n",
        "        index = torch.randperm(batch_size).to(batch.device)\n",
        "\n",
        "        mixed_x = lam * batch + (1 - lam) * batch[index, :]\n",
        "        y_a, y_b = targets, targets[index]\n",
        "\n",
        "        return mixed_x, y_a, y_b, lam\n",
        "\n",
        "\n",
        "class LabelSmoothingLoss(nn.Module):\n",
        "    \"\"\"Label smoothing loss for better generalization\"\"\"\n",
        "    def __init__(self, classes, smoothing=0.1):\n",
        "        super(LabelSmoothingLoss, self).__init__()\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.classes = classes\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        pred = pred.log_softmax(-1)\n",
        "        with torch.no_grad():\n",
        "            true_dist = torch.zeros_like(pred)\n",
        "            true_dist.fill_(self.smoothing / (self.classes - 1))\n",
        "            true_dist.scatter_(1, target.unsqueeze(1), self.confidence)\n",
        "        return torch.mean(torch.sum(-true_dist * pred, dim=1))\n",
        "\n",
        "\n",
        "class KeepAverages(object):\n",
        "    \"\"\"Computes and stores the average along with the current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "class ResNetParams:\n",
        "    \"\"\"Hyperparameters for training the ResNet model\"\"\"\n",
        "    def __init__(self, arch='Hybrid-Model', epochs=200, start_epoch=0, batch_size=128,\n",
        "                 lr=0.1, min_lr=1e-4, momentum=0.9, weight_decay=5e-4, print_freq=50,\n",
        "                 save_dir='save_checkpoints', save_every=10, use_mixup=True,\n",
        "                 use_cutout=True, label_smoothing=0.1, model_type='B',\n",
        "                 optimizer='sgd', scheduler='cosine'):\n",
        "        self.save_every = save_every  # Saves checkpoints at specified epochs\n",
        "        self.save_dir = save_dir  # Directory for checkpoints\n",
        "        self.print_freq = print_freq  # Print frequency\n",
        "        self.weight_decay = weight_decay  # Weight decay for optimizer\n",
        "        self.momentum = momentum  # Momentum for SGD\n",
        "        self.lr = lr  # Initial learning rate\n",
        "        self.min_lr = min_lr  # Minimum learning rate for cosine annealing\n",
        "        self.batch_size = batch_size  # Batch size\n",
        "        self.start_epoch = start_epoch  # Starting epoch\n",
        "        self.epochs = epochs  # Total epochs\n",
        "        self.arch = arch  # Model name\n",
        "        self.use_mixup = use_mixup  # Whether to use mixup augmentation\n",
        "        self.use_cutout = use_cutout  # Whether to use cutout augmentation\n",
        "        self.label_smoothing = label_smoothing  # Label smoothing factor\n",
        "        self.model_type = model_type  # Model variant (A, B, or C)\n",
        "        self.optimizer = optimizer  # Optimizer type\n",
        "        self.scheduler = scheduler  # Learning rate scheduler\n",
        "\n",
        "\n",
        "def get_data_loaders(args):\n",
        "    \"\"\"Prepare and return data loaders with augmentations\"\"\"\n",
        "    # CIFAR10 mean and std\n",
        "    cifar_mean = [0.4914, 0.4822, 0.4465]\n",
        "    cifar_std = [0.2023, 0.1994, 0.2010]\n",
        "\n",
        "    # Base transforms for both train and test\n",
        "    transform_base = [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(cifar_mean, cifar_std)\n",
        "    ]\n",
        "\n",
        "    # Enhanced training transforms\n",
        "    transform_train = [\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    ]\n",
        "\n",
        "    # Combine transforms\n",
        "    transform_train.extend(transform_base)\n",
        "\n",
        "    # Add cutout augmentation if enabled\n",
        "    if args.use_cutout:\n",
        "        transform_train.append(CutoutTransform(n_holes=1, length=16))\n",
        "\n",
        "    # Create datasets and loaders\n",
        "    trainset = torchvision.datasets.CIFAR10(\n",
        "        root='./data', train=True, download=True,\n",
        "        transform=transforms.Compose(transform_train)\n",
        "    )\n",
        "\n",
        "    trainloader = DataLoader(\n",
        "        trainset, batch_size=args.batch_size, shuffle=True,\n",
        "        num_workers=2, pin_memory=True\n",
        "    )\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR10(\n",
        "        root='./data', train=False, download=True,\n",
        "        transform=transforms.Compose(transform_base)\n",
        "    )\n",
        "\n",
        "    testloader = DataLoader(\n",
        "        testset, batch_size=args.batch_size, shuffle=False,\n",
        "        num_workers=2, pin_memory=True\n",
        "    )\n",
        "\n",
        "    return trainloader, testloader\n",
        "\n",
        "\n",
        "def get_model(args):\n",
        "    \"\"\"Load the appropriate model based on configuration\"\"\"\n",
        "    if args.model_type == 'A':\n",
        "        model = OptimizedResNet_A()\n",
        "    elif args.model_type == 'B':\n",
        "        model = OptimizedResNet_B()\n",
        "    elif args.model_type == 'C':\n",
        "        model = OptimizedResNet_C()\n",
        "    else:\n",
        "        model = project1_model()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_optimizer(args, model):\n",
        "    \"\"\"Configure optimizer based on args\"\"\"\n",
        "    if args.optimizer == 'sgd':\n",
        "        optimizer = torch.optim.SGD(\n",
        "            model.parameters(),\n",
        "            lr=args.lr,\n",
        "            momentum=args.momentum,\n",
        "            weight_decay=args.weight_decay\n",
        "        )\n",
        "    elif args.optimizer == 'adam':\n",
        "        optimizer = torch.optim.Adam(\n",
        "            model.parameters(),\n",
        "            lr=args.lr,\n",
        "            weight_decay=args.weight_decay\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported optimizer: {args.optimizer}\")\n",
        "\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "def get_scheduler(args, optimizer):\n",
        "    \"\"\"Configure learning rate scheduler based on args\"\"\"\n",
        "    if args.scheduler == 'cosine':\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "            optimizer, T_max=args.epochs, eta_min=args.min_lr\n",
        "        )\n",
        "    elif args.scheduler == 'step':\n",
        "        scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
        "            optimizer, milestones=[int(args.epochs*0.5), int(args.epochs*0.75)], gamma=0.1\n",
        "        )\n",
        "    elif args.scheduler == 'onecycle':\n",
        "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "            optimizer, max_lr=args.lr, total_steps=args.epochs\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported scheduler: {args.scheduler}\")\n",
        "\n",
        "    return scheduler\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Compute the precision@k for the specified values of k\"\"\"\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "\n",
        "    return res\n",
        "\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    \"\"\"Mixup loss function\"\"\"\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, epoch, args, mixup_fn=None):\n",
        "    \"\"\"Training function with support for mixup\"\"\"\n",
        "    # Initialize metrics\n",
        "    batch_time = KeepAverages()\n",
        "    data_time = KeepAverages()\n",
        "    losses = KeepAverages()\n",
        "    top1 = KeepAverages()\n",
        "\n",
        "    # Switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "        # Measure data loading time\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        # Move data to device\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "        # Apply mixup if enabled\n",
        "        if args.use_mixup and mixup_fn is not None:\n",
        "            inputs, targets_a, targets_b, lam = mixup_fn(inputs, targets)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
        "        else:\n",
        "            # Regular forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Measure accuracy and record loss\n",
        "        if not args.use_mixup:  # Skip accuracy during mixup as it's not meaningful\n",
        "            prec1 = accuracy(outputs.data, targets)[0]\n",
        "            top1.update(prec1.item(), inputs.size(0))\n",
        "\n",
        "        losses.update(loss.item(), inputs.size(0))\n",
        "\n",
        "        # Measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        # Print progress\n",
        "        if i % args.print_freq == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
        "                      epoch, i, len(train_loader), batch_time=batch_time,\n",
        "                      data_time=data_time, loss=losses, top1=top1))\n",
        "\n",
        "    return losses.avg, top1.avg\n",
        "\n",
        "\n",
        "def validate(val_loader, model, criterion, args):\n",
        "    \"\"\"Validation function\"\"\"\n",
        "    batch_time = KeepAverages()\n",
        "    losses = KeepAverages()\n",
        "    top1 = KeepAverages()\n",
        "\n",
        "    # Switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        end = time.time()\n",
        "        for i, (inputs, targets) in enumerate(val_loader):\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Measure accuracy and record loss\n",
        "            prec1 = accuracy(outputs.data, targets)[0]\n",
        "            losses.update(loss.item(), inputs.size(0))\n",
        "            top1.update(prec1.item(), inputs.size(0))\n",
        "\n",
        "            # Measure elapsed time\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            # Print progress\n",
        "            if i % args.print_freq == 0:\n",
        "                print('Test: [{0}/{1}]\\t'\n",
        "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                      'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
        "                          i, len(val_loader), batch_time=batch_time,\n",
        "                          loss=losses, top1=top1))\n",
        "\n",
        "    print(' * Prec@1 {top1.avg:.3f}'.format(top1=top1))\n",
        "\n",
        "    return losses.avg, top1.avg\n",
        "\n",
        "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
        "    \"\"\"Save model checkpoint\"\"\"\n",
        "    # Create directory if it doesn't exist\n",
        "    directory = os.path.dirname(filename)\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "    # Save checkpoint\n",
        "    torch.save(state, filename)\n",
        "\n",
        "    if is_best:\n",
        "        best_filename = os.path.join(os.path.dirname(filename), 'model_best.pth.tar')\n",
        "        torch.save(state, best_filename)\n",
        "\n",
        "\n",
        "def run_training(args):\n",
        "    \"\"\"Main training loop with all components\"\"\"\n",
        "    # Create save directory if it doesn't exist\n",
        "    if not os.path.exists(args.save_dir):\n",
        "        os.makedirs(args.save_dir)\n",
        "\n",
        "    # Get data loaders\n",
        "    train_loader, val_loader = get_data_loaders(args)\n",
        "\n",
        "    # Get model\n",
        "    model = get_model(args)\n",
        "    model = model.cuda()\n",
        "\n",
        "    # Count parameters\n",
        "    num_params = count_parameters(model)\n",
        "    print(f'Model: {args.arch} - Type: {args.model_type}')\n",
        "    print(f'Total parameters: {num_params:,}')\n",
        "\n",
        "    # Ensure we're under the parameter limit\n",
        "    if num_params > 5000000:\n",
        "        print(f'WARNING: Model exceeds 5M parameter limit with {num_params:,} parameters!')\n",
        "\n",
        "    # Set up loss function\n",
        "    if args.label_smoothing > 0:\n",
        "        criterion = LabelSmoothingLoss(classes=10, smoothing=args.label_smoothing).cuda()\n",
        "    else:\n",
        "        criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "    # Set up optimizer and scheduler\n",
        "    optimizer = get_optimizer(args, model)\n",
        "    scheduler = get_scheduler(args, optimizer)\n",
        "\n",
        "    # Set up mixup function if needed\n",
        "    mixup_fn = MixupTransform(alpha=1.0) if args.use_mixup else None\n",
        "\n",
        "    # Track best accuracy\n",
        "    best_prec1 = 0\n",
        "\n",
        "    # Lists to track metrics\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(args.start_epoch, args.epochs):\n",
        "        # Train and validate\n",
        "        print(f'\\nEpoch: {epoch+1}/{args.epochs}')\n",
        "        print(f'Learning rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
        "\n",
        "        train_loss, train_acc = train(train_loader, model, criterion, optimizer, epoch, args, mixup_fn)\n",
        "        val_loss, val_acc = validate(val_loader, model, criterion, args)\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        # Record metrics\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        # Check if this is the best model\n",
        "        is_best = val_acc > best_prec1\n",
        "        best_prec1 = max(val_acc, best_prec1)\n",
        "\n",
        "        # Save checkpoint\n",
        "        if epoch > 0 and epoch % args.save_every == 0 or is_best:\n",
        "            save_checkpoint({\n",
        "                'epoch': epoch + 1,\n",
        "                'arch': args.arch,\n",
        "                'state_dict': model.state_dict(),\n",
        "                'best_prec1': best_prec1,\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "            }, is_best, filename=f'{args.save_dir}/checkpoint_{epoch+1}.pth.tar')\n",
        "\n",
        "    # Save final model\n",
        "    torch.save(model.state_dict(), f'{args.save_dir}/final_model.pth')\n",
        "\n",
        "    # Plot training curves\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Loss plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Training and Validation Loss')\n",
        "\n",
        "    # Accuracy plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_accs, label='Train Accuracy')\n",
        "    plt.plot(val_accs, label='Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{args.save_dir}/training_curves.png')\n",
        "\n",
        "    return model, best_prec1\n",
        "\n",
        "#---------- MAIN EXECUTION ----------#\n",
        "\n",
        "# Set up training parameters\n",
        "args = ResNetParams(\n",
        "    arch='Hybrid-ResNet',\n",
        "    epochs=600,            # Reduced for Colab\n",
        "    batch_size=128,\n",
        "    lr=0.1,\n",
        "    model_type='B',        # Model B (~4.2M parameters)\n",
        "    use_mixup=True,\n",
        "    use_cutout=True,\n",
        "    scheduler='cosine',\n",
        "    save_dir='checkpoints'\n",
        ")\n",
        "\n",
        "# Run the training\n",
        "run_training(args)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
